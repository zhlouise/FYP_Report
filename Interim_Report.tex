\documentclass[12pt, a4paper]{article}


% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times} % Sets font to Times New Roman
\usepackage[margin=2.5cm]{geometry} % Sets 2.5cm margins
\usepackage{setspace} % Required for double spacing
\usepackage{titlesec}
\usepackage{sectsty}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{color}
\usepackage{amsmath}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amsmath} 
\usepackage{url}
\usepackage{parskip}
\usepackage[hidelinks]{hyperref}


% --- Settings ---
\doublespacing % Applies double spacing to the document
\sectionfont{\normalsize}
\subsectionfont{\normalsize\it\bfseries}
\subsubsectionfont{\normalsize\it}
\setlength{\parindent}{0pt}
\urlstyle{same}


\begin{document}


% --- Title Page ---
\begin{titlepage}
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/PolyU_Logo.png}
    \hfill
    \includegraphics[width=0.25\textwidth]{Included_Images/FENG_Logo.png}
    \vfill
    {\Large 2025/26 Interdepartmental Final Year Project Interim Report\par}
    {\huge \textbf{Intelligent UAV Systems for GNSS-Based Remote Sensing on
    Vegetation} \par}
    \vfill
    {\large [AAE] ZHOU Jiayi (22099961D) \par} {\large [AAE] MAHMUD Md Sahat
    (22097159D) \par} {\large [EEE] GAMAGE Sashenka (22097129D) \par} {\large
    [ME] TAN Qing Lin (22101126D) \par}
    \vfill
    {\large \textbf{Chief Supervisor: } [AAE] Prof. Guohao ZHANG \par} {\large
    \textbf{Co-Supervisor: } [AAE] Prof. Li-Ta HSU \par}
    \vspace{1cm}
    {\large \textbf{Date of Submission: } 2026 January 18 \par}
\end{titlepage}


% --- Table of Contents ---
\pagenumbering{roman}
\tableofcontents
\newpage


% --- Main Content ---
\pagenumbering{arabic}


% --- Abstract ---
\section{Abstract}
Placeholder text


% --- Introduction ---
\section{Introduction}
Vegetation is a critical asset to the environment and the human civilization.
Not only does vegetation produce essential societal resources, but its
distribution and productivity also greatly impacts the terrestrial ecosystems
and the global climate \cite{Richardson2013}. Therefore, the continuous and
accurate monitoring of vegetation is essential for sustainable resource
management \cite{Wallace2006}, ecosystem preservation \cite{Zeng2022}, and
climate change modeling \cite{Richardson2013}. 

Traditionally, methods of vegetation monitoring involved manual field
assessments of site characteristics, extracting vegetation condition indicators
such as species composition, geometrical structure, and biochemical activities
\cite{Gibbons2006}. However, the accuracy and efficiency of such monitoring
methods are highly dependent on the available expertise and resources, as the
site assessments often required the assessor to possess reasonable levels of
field knowledge prior to surveying \cite{Parkes2003}. To address the limitations
of manual assessments and to accommodate for the increasing large-scale
monitoring demands, remote sensing systems emerged as an essential tool for
ecological monitoring \cite{Li2023}. 

Remote sensing refers to the acquisition of an object's information through
measurements obtained without coming into direct contact with said object,
effectively minimizing the need for manual involvement on-site
\cite{campbell2011introduction}. Specifically, remote sensing relies on
information derived from different measurements of energy reflected from the
object of interest \cite{campbell2011introduction}. In the context of surveying
terrestrial vegetation, the remote sensing methods could be classified into two
categories based on the distance between the target object and the measurement
sensor: 1) space-borne and 2) airborne remote sensing. Space-borne remote
sensing involves the use of instruments onboard orbiting satellites, including
spectral and hyperspectral cameras \cite{Qian2022}, synthetic aperture radar
(SAR) \cite{Hu2025}, and space-borne Light Detection And Ranging (LiDAR) sensors
\cite{Bergen2009}. Due to the wide coverage and availability of space-borne
data, large-scale terrestrial changes over time could be captured, enabling the
continuous monitoring of macro-scale ecosystems \cite{Khan2024}. Compared to
space-borne systems, airborne remote sensing can provide significantly improved
spatial resolution and assessment flexibility through the integration of sensors
onboard manned aircraft or unmanned aerial vehicles (UAVs) \cite{Khan2024}.
Although limited by coverage area, airborne remote sensing can provide timely
information for addressing regional emergencies such as pest \cite{Wulder2006}
or wildfire \cite{Arroyo2008} outspreads since the systems could be deployed
on-demand. In UAV-based remote sensing particularly, this temporal flexibility
is further complemented with the benefit of low operational cost, rendering it
an ideal platform for monitoring regional and urban vegetation \cite{Tang2015}. 

Conventionally, UAV remote sensing platforms carry similar instruments to that
of other remote sensing systems, including radar, LiDAR, and multi-spectral or
hyperspectral imagery sensors \cite{Tang2015}. However, while imaging
instruments are susceptible to the influence of lighting and weather
\cite{Wu2024}, LiDAR devices tend to face difficulties penetrating through dense
canopy \cite{Su2007}. Therefore, it is critical to explore a robust sensing
technique that is suitable for UAVs in terms of payload and power consumption to
complement the existing instruments. Recently, the technique of Global
Navigation Satellite System Reflectometry (GNSS-R) is receiving increasing
interest in the field of remote sensing. GNSS-R exploits the L-band signals
transmitted from Global Navigation Satellite Systems (GNSS) that are then
scattered on different terrain surfaces of the Earth \cite{Jin2024}. Then, the
reception of reflected GNSS signals can provide information regarding the
properties of the signal reflector on land \cite{Jin2024}. As signals of
opportunity conventionally dedicated to Positioning, Navigation, and Timing
(PNT) applications, GNSS is capable of providing real-time measurements
regardless of time, location, and weather \cite{Jin2024}. Additionally, as a
bi-static system where signal transmitters are separated from receivers, GNSS-R
is exempt from the need of dedicated transmitter-receiver instruments that are
crucial to mono-static radar systems \cite{Jin2010}. Instead, any consumer-grade
receivers capable of receiving reflected GNSS signals could be used for GNSS-R
remote sensing, further demonstrating GNSS-R's applicability onboard low-cost
remote sensing systems such as an UAV. 
%added by sashenka 

Recent research has demonstrated the potential of integrating GNSS-R with UAVs
by developing a UAV-based GNSS-R system for water body detection and flood
mapping \cite{UAV-water}. This showcases how low-altitude UAV platforms can give
high-resolution and flexible measurements. Their work established UAV-mounted
GNSS-R as a promising tool for monitoring dynamic environmental conditions.
However, due to the challenges in optimal flight trajectories planning and
coverage planning, this research does not provide an autonomous navigation
application. Addressing the above challenges requires autonomous path planning
algorithms, dynamic obstacle avoidance, and environment mapping and
localization. As a result, our research provides a unique opportunity to enhance
the UAV and GNSS-R combination by integrating autonomous navigation. 

\hl{May need some intro for path planning and lidar here. The above is from
Louise's project proposal, so need further editing. (Also need to mention 
the full name of SLAM somewhere)}

In this project, the technique of GNSS-R will be integrated onto an UAV platform
to achieve an intelligent and structured remote sensing system for vegetation
monitoring. The main objectives of this project are as follows:
\begin{enumerate}
\item To introduce an autonomous path planning framework onboard the UAV platform
for optimized GNSS-R remote sensing.
\item To analyze and model the correlations between signal propagation parameters
retrieved through GNSS-R and ground vegetation conditions.
\item To classify signals reflected by vegetation and predict vegetation parameters
based on raw GNSS data using machine learning-based approach.
\item To establish a detailed 3D canopy map using LiDAR-based SLAM, providing a
spatial validation reference for the 2D vegetation features detected by GNSS-R.
\end{enumerate}

This report is structured as follows: Section 3 reviews contemporary research in
UAV path planning, GNSS-R, machine learning, and LiDAR SLAM, while Section 4
outlines the project's methodologies. The subsequent sections cover the
experiments conducted (Section 5), a discussion of current results (Section 6),
and the project conclusions (Section 7). Finally, Section 8 explores future
work, and Section 9 summarizes project management details. 


% --- Literature Review ---
\section{Literature Review}
To achieve the project objectives, this literature review comprehensively examines
the contemporary research in four domains, each associated with one project objective.
\hl{First, existing path planning methodologies are reviewed to \dots}
Subsequently, the existing techniques of GNSS-R are explored, with a focus on its
prior applications in vegetation parameter retrieval. Then machine learning based 
techniques are investigated for the purpose of classification of reflected and direct 
signals. Further investigation is conducted on machine learning based approaches for predicting 
vegetation parameters from raw GNSS data.
\hl{Finally, LiDAR-based SLAM techniques \dots}
Consequently, this review identifies the existing gaps in each domain for the 
objective of vegetation monitoring, which will inform the integrated project 
methodologies.

\subsection{Path Planning}
Path planning is the technique that used in navigating a robot autonomously from
a starting goal to an ending point based on a collision-free route. Path
planning is mainly a geometric problem that generates a path according to a
given sequence of waypoints. It does not consider the variation with time;
however, in trajectory planning the path is defined as a function of time, which
assigns a time function to the geometric path \cite{gasparetto2015path}. At the
current stage, our research is focusing on implementing path planning to
navigate an Unmanned Aerial Vehicle (UAV) autonomously in an outdoor
environment. Consequently, we are focusing on path planning methods and will be
focusing on trajectory planning and optimization in the future based on our
needs.
Earliest sensor-based obstacle avoiding path planning algorithms are
known as bug1 and bug2 algorithms \cite{choset2005principles}. These algorithms
will first go around the obstacles and then determine the shortest path to the
goal based on the shape of the obstacle (Figure \ref{fig:bug_algorithms}).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Included_Images/bug_algorithms.png}
    \caption{Bug1 and Bug2 Path Planning Algorithms}
    \cite{choset2005principles}
    \label{fig:bug_algorithms}  
\end{figure}
However, these algorithms are used in scenarios where the robot has no map
(unknown environment), and these algorithm does not guarantee the optimal path.
With the rise of SLAM (Simultaneous Localization and Mapping) techniques, we are
able to create maps of the navigation environment, which enabled the transition
to use map-based path planning methods. Therefore, in our system we are using
commonly found grid-based path planning algorithms, which enables us to create
an intelligent UAV system that can identify potential vegetation areas using
GNSS-R. 

Most GNSS-R studies remain satellite-based, which limits spatial resolution and
adaptability to localized environments such as fragmented urban forests. To
alleviate this problem, we decided to use UAV-borne GNSS-R systems as a
promising solution. UAVs can fly at lower altitudes, enabling high-resolution,
flexible, and scalable vegetation monitoring missions. However, deploying GNSS-R
on UAVs can be challenging in implementing optimal flight trajectories and
spatial coverage. Therefore, our system introduces autonomous path planning,
dynamic obstacle avoidance, and SLAM for environment mapping and localization.  






\subsection{GNSS-R}
The concept of using GNSS-R for remote sensing was first proposed in 1993, where
the correlation between direct and scattered GNSS signals were proven promising
for the application of ocean altimetry \cite{MartnNeira1993APR}. Today, apart
from the originally proposed application, GNSS-R techniques has been extensively
used for Earth observations and land information retrieval, including ocean
salinity, soil moisture, and ice thickness \cite{RodriguezAlvarez2011}. However,
research on vegetation parameter retrieval remains limited to preliminary
analyses \cite{Jia2018}. 

In a previous study, the capabilities of monitoring vegetation through GNSS-R
were demonstrated through a theoretical simulation of GNSS scattering
characteristics \cite{Ferrazzoli2011}. Furthermore, this study revealed that the
sensitivity of GNSS-R signals can be influenced by incidence angle, soil
parameters, and tree size, while signals with lower elevation angles and RL
polarization (transmission of right-hand circular polarization (RHCP) signal,
reception of left-hand circular polarization (LHCP) signal) appeared to be ideal
for forest monitoring \cite{Ferrazzoli2011}. In another study, the polarization
scattering properties of GNSS-R signals in forest canopies were modeled, where
simulations revealed that tree trunk scattering effects would dominate total
scattering response, while satellite azimuth angles are significantly correlated
to the signal polarization \cite{Wu2014}. 

Aside from simulated approaches, empirical or semi-empirical studies regarding GNSS-R
remote sensing of vegetation were also reviewed. In a study by Yueh et al.,
GNSS-R data onboard the Cyclone GNSS (CYGNSS) satellite were analyzed against
the vegetation water content estimated through the satellite-derived Normalized
Difference Vegetation Index (NDVI) \cite{Yueh2022}. The results demonstrated
near-linear relationship between vegetation water content and GNSS-R signal
attenuation, while the CYGNSS data also suggested the existence of volume
scattering within complex forest components \cite{Yueh2022}. Similarly, another
study analyzing GNSS-R data from TechDemoSat-1 demonstrated reduced sensitivity
to soil moisture retrieval due to vegetation attenuation, which could be
effectively compensated using NDVI data, indicating a correlation between signal
attenuation and vegetation water content \cite{Camps2016}. Additionally, the
interference pattern between direct and reflected GNSS signals from the Earth's
surface was used in estimating vegetation height and land topography
\cite{RodriguezAlvarez2011}. Through using a Soil Moisture Interference-pattern
GNSS Observations at L-band (SMIGOL) reflectometer, the instantaneous power of
the direct and reflected signals were coherently added, in which the resulting
power oscillations present notches that are correlated to vegetation layer
thickness and the reflection geometry \cite{RodriguezAlvarez2011}. As a result,
an RMSE of 3-5 cm in estimating the height of vegetation with simple geometrical
structures (barley and wheat) could be achieved \cite{RodriguezAlvarez2011}.
Yet, despite the effort in prior studies, the use of GNSS-R techniques in
vegetation remote sensing and monitoring is far from developed. Most
importantly, existing research relies heavily on space-borne or static
ground-based platforms, while the use of high spatial resolution and flexibility
platforms - such as an UAV - remains underexplored. Therefore, it is critical to
develop a systematic framework for an UAV-based GNSS-R system, in which
correlations between GNSS-R signal features and vegetation condition parameters
are comprehensively assessed and modeled. Additionally, with adequate dataset size,
the GNSS-R framework could be complemented by machine learning-based modelling
approaches, further enhancing the system robustness. \hl{Connection to ML part}

\subsection{Machine Learning}
A part of this project will be to develop a deep learning based model that can process 
collected GNSS-R data dna address two main topics: (1) Classification of the received signals into 
direct and reflected signals, and (2) Prediction of vegetation parameters from the raw GNSS data.
The goal will be to develop and test tree based and neural network based models that 
take GNSS-R geatures, bith publicly available and collected by our UAV system, as inputs and
output the desired results. The model training will be optimized for highest computational 
efficiency and accurac on python based PyTorch framework. Past research has demonstrated that
GNSS-R can sense vegetation as the reflected signal power and modulation depends on
vegetation cover and moisture. For example, a study by Small et al. (2010) and others found 
that the amplitude of the reflected GNSS signal to Noise Ratio (SNR) oscilations over
crops correlated linearly with Vegetation Water Content (VWC) \cite{small2010sensing}. A study 
by Chen et al. (2024) recently derived a new GNSS-R vegetation index and used Artificial 
Neural Networks (ANN) to retrieve VWX from spaceborne data, achieve a correlation of approximately
0.94 with reference measurements \cite{chen2024new}. Other studies have trained deep
learning models on GNSS-R features. One study trained a ANN on Delay Doppler Maps (DMMs), 
SNR, signal incidence angle and geographic data to predict the Above Ground Biomass (AGB) \cite{pilikos2024biomass}.
Prior GNSS-R vegetation retrieval efforts have focuesed on parameters like biomass, soil moisture,
and VWC. For example. SNR that has been reflected from wheat was used to estimate the height
and water content of wheat, which signifies that strong vegetation attenuates the reflected 
signals \cite{zhang2017use}. Polarimetric GNSS features have also been used in previous studies.
Cross polarization ratios and co polarized reflection coefficients were found to be sensitive
to vegetation biomass and moisture \cite{wu2021recent}. 

Most existing studies have used spaceborne or terestrial platforms, while UAV based GNSS-r
remains underexplored. The few existing UAV based works demonstrate feasibility but do not 
address vegetation mapping \cite{wu2021recent,chen2024new}. Moreover, classfication of difference signals
is not a well studied topic. However, existing study suggests that GNSS-R features can inform vegetation 
parameters \cite{small2010sensing}. But the gap is clear: GNSS-R with UAVs for vegetation 
mappinh and signal classification has not been demonstrated. This project will bridge the gap by developing
ML models tailored for UAV scale GNSS-R data and the specific tasks of vegetation detection and parameters estimation. 

\subsection{LiDAR SLAM}
In recent years, autonomous robots have been widely implemented to carry out
repetitive tasks or take part in site surveys to ensure human safety or even
reduce operational cost. To carry out remote sensing via UAV, mapping is
essential to allow robots to create a spatial understanding of the site in order
to create efficient flight paths as well as avoid obstacles. 




% --- Methodology ---
\section{Methodology}
Placeholder text

\subsection{UAV Platform}
Placeholder text

\subsection{System Architecture}
Placeholder text

\subsection{Path Planning}
As mentioned in the system architecture, path planning in our system focuses on
two different methods: waypoint-based navigation and coverage path planning.
These methods are crucial in observing various aspects of dense forests, such as
hovering over trees and surveying over a dense canopy to obtain dynamic data for
LiDAR SLAM.


\subsubsection{Waypoint-based navigation}

Waypoint-based navigation can be achieved in various approaches such as
search-based algorithms, graph-based algorithms, and Artificial
Intelligence-based algorithms. Since our system is designed to hover over dense
vegetation environments, identifying potential Fresnel zones, we primarily need
to fly outdoor high altitude open spaces. In order to find the optimal algorithm
to build an intelligent UAV system, we conducted a baseline comparison among the
Djikstra algorithm, the A* algorithm, and the RRT algorithm. 

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3} % Adds a little vertical padding for readability
    % 'l' = left align, 'X' = auto-wrap column
    \begin{tabularx}{\textwidth}{@{} l l X X @{}} 
    \toprule
    \textbf{Category} & \textbf{Algorithm} & \textbf{Advantages} & \textbf{Disadvantages} \\ 
    \midrule

    % SEARCH BASED SECTION
    \multirow{10}{*}{\textbf{Search-based}} & \textbf{A*} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Shortest path is guaranteed for a given heuristic map.
        \item Reduces computation power and search space compared to Dijkstra.
    \end{itemize} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item High computational cost in large or continuous spaces.
        \item Poor scalability in dynamic environments.
    \end{itemize} \\ 
    \cmidrule{2-4}
    
    & \textbf{Dijkstra} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Designed to find the optimal path. 
        \item Simple to implement.
    \end{itemize} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Computationally expensive.
        \item Impractical for real-time UAV planning.
    \end{itemize} \\ 
    \midrule

    % SAMPLING BASED SECTION
    \textbf{Sampling-based} & \textbf{RRT} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Fast exploration of high-dimensional spaces.
        \item Perform well in unknown or partially known maps.
    \end{itemize} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Does not guarantee the shortest/optimal path.
        \item Paths are often jagged and require smoothing.
    \end{itemize} \\ 
    \bottomrule
    \end{tabularx}
    \caption{Comparison of Path Planning Algorithms}
    \label{tab:algorithms}
\end{table}

As a result, for a given occupancy grid map, the comparison shows that the A* algorithm is suitable for offline
path planning. Therefore, our system is designed
to perform offline path planning using A* algorithm. On the other hand, RRT
performs well in dynamic unknown environments, which is suitable for online path
planning. 

\paragraph{A* Algorithm Implementation} ~\\
A* algorithm is a heuristic, search based algorithm which calculates the
shortest path from a starting point to an end goal. The algorithm will first
divide the map into nodes, where each node represents a possible position for
the robot to move. These positions depend on the gird architectures such as
4-way connected or 8-way connected grids. In addition to the conventional grid
architecture, novel grid mapping approaches such as NavMeshes are widely used in
3D simulation software. These mapping techniques let the algorithm connects the
locations of the movable agent to the surface (mesh) and stores the surfaces as
convex polygonals. 

In our research, we will be focusing on regular grids which connects nodes
across 26-directions in 3-D space. Each node will be assigned a cost based on the
starting and ending node for a given path. The 2-D path illustration of an A*
algorithm is shown in Figure \ref{fig:astar}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/A-starmap.jpg}
    \caption{Map Highlighting Shortest Path from Node A to Node K}
    \label{fig:astar}
\end{figure}

\paragraph{A* Algorithm Grid Definition} ~\\

In our system the grid map is defined in 3-D spaces having 26 movable
directions, diagonally and straight. However, during the testing phase of
algorithm, we will be using 2-D grids with 8 movable directions for simplicity.
We first convert the coordinates of the given map into gridnode indexes using
\texttt{coord\_to\_index} function. The coordinates can be defined as world coordinates or
image coordinates for a given map image.
An illustration of image coordinates is shown in Figure \ref{fig:image_coordinates}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Included_Images/image_coordinates.png}
    \caption{Image Coordinates System}
    \label{fig:image_coordinates}
\end{figure}
Below is the psuedocode for the \texttt{coord\_to\_index} function:
\begin{algorithm}[H]
\caption{Coordinate to Grid Index Conversion}
\label{alg:coord_to_index}
\begin{algorithmic}[1]
    \Require $pos$: Real-world coordinate vector $(x, y, z)$
    \Ensure $index$: Integer grid indices $(i, j, k)$
    
    \Function{CoordToIndex}{$pos$}
        \State \Comment{Calculate position relative to the map center}
        \State $pos_{relative} \gets pos - pos_{center}$
        
        \State \Comment{Scale by resolution and round to nearest integer}
        \State $index \gets \text{round}(pos_{relative} \cdot step_{inv})$
        
        \State \Comment{Apply the center index offset}
        \State $index \gets index + index_{center}$
        
        \State \Return $index$
    \EndFunction
\end{algorithmic}
\end{algorithm}
\paragraph{A* Algorithm Cost Function Implementation} ~\\
The cost function $f(n)$ for each node $n$ is calculated as follows:
\begin{equation}
    f(n) = g(n) + h(n)  
\end{equation}
where:
\begin{itemize}
    \item $g(n)$ is the cost from the starting node to the current node $n$.
    \item $h(n)$ is the estimated heuristic cost from node $n$ to the goal node.
    \item $f(n)$ is the total estimated cost of the shortest path node $n$.
\end{itemize}
In our implementation the cost $g(n)$ (the cost from starting node to end node) is calculated as the Euclidean distance
between the starting node and the current node $n$.
The Euclidean distance can be calculated as follows:
\begin{equation}
    g(n) = \sqrt{(x_n - x_{start})^2 + (y_n - y_{start})^2 + (z_n - z_{start})^2}  
\end{equation}
where $(x_n, y_n, z_n)$ are the coordinates of the current node $n$ and
$(x_{start}, y_{start}, z_{start})$ are the coordinates of the starting node.

The heuristic cost $h(n)$ (the estimated cost from current node $n$ to goal
node) is implemented using the diagonal heuristic function. The function has five key steps as follows:

\begin{enumerate}
    \item Calculates the distance differences in each dimension ($dx$, $dy$, $dz$) between the current node and the goal node.
    \item Determines the direction with the minimum change in distance.
    \item The heuristic cost starts at zero and will increment by $\sqrt{3} \cdot \min(dx, dy, dz)$ to account for diagonal movement in 3D space.
    \item The function checks which dimensions still have remaining distance to cover. If one dimension has been fully covered (i.e., $dx$, $dy$, or $dz$ equals zero), it calculates the remaining cost using 2D diagonal movement and straight-line movement.
    \item If the three dimensions still have remaining distance, it calculates the cost using a combination of 2D diagonal movement and straight-line movement for the remaining distances.
\end{enumerate}

Below is an example of calculating heuristic cost in 3 dimensional space:

\noindent Go from $(0,0,0)$ to $(5,3,2)$. \\
Initial differences: $dx=5, dy=3, dz=2$.

\begin{itemize}
    \item \textbf{Step 1: 3D diagonal movement} \\
    Calculate the shared 3D distance:
    \[ diag_{3d} = \min(5, 3, 2) = 2 \]
    Cost for 2 steps diagonally in 3D:
    \[ \text{Cost} = \sqrt{3} \times 2 \approx 3.464 \]
    \textit{Remaining:} $dx=3, dy=1, dz=0$.

    \item \textbf{Step 2: Handle remaining} (since $dz=0$) \\
    Calculate 2D diagonal in X-Y ($\min(3,1) = 1$ step):
    \[ \text{Cost} = \sqrt{2} \times 1 \approx 1.414 \]
    Calculate Straight line ($|3-1| = 2$ steps):
    \[ \text{Cost} = 2.0 \]
\end{itemize}

\noindent \textbf{Total estimated cost:}
\[ 3.464 + 1.414 + 2.0 = 6.878 \]

Below is the psuedocode for the diagonal heuristic function:
\begin{center}
    % Change 0.8 to 0.6 or 0.7 to make it even narrower
    \begin{minipage}{0.8\linewidth} 
\begin{algorithm}[H]
\caption{Diagonal Heuristic (3D)}
\label{alg:diagonal_heuristic}
\begin{algorithmic}[1] % The [1] turns on line numbering
    \Require $node_1, node_2$: Current and Goal GridNodes
    \Ensure $h$: Estimated cost to goal
    
    \Function{DiagonalHeuristic}{$node_1, node_2$}
        \State $dx \gets |node_1.x - node_2.x|$
        \State $dy \gets |node_1.y - node_2.y|$
        \State $dz \gets |node_1.z - node_2.z|$
        
        \State \Comment{Find the minimum for 3-D diagonal movement}
        \State $diag_{3d} \gets \min(dx, dy, dz)$
        \State $dx \gets dx - diag_{3d}$
        \State $dy \gets dy - diag_{3d}$
        \State $dz \gets dz - diag_{3d}$
        
        \State $h \gets \sqrt{3} \cdot diag_{3d}$
        
        \State \Comment{Handle remaining 2-D movement}
        \If{$dx = 0$}
            \State $h \gets h + \sqrt{2} \cdot \min(dy, dz) + |dy - dz|$
        \ElsIf{$dy = 0$}
            \State $h \gets h + \sqrt{2} \cdot \min(dx, dz) + |dx - dz|$
        \ElsIf{$dz = 0$}
            \State $h \gets h + \sqrt{2} \cdot \min(dx, dy) + |dx - dy|$
        \Else
            \State \Comment{General case (if dimensions remain)}
            \State $diag_{2d} \gets \min(dx, dy, dz)$
            \State $h \gets h + \sqrt{2} \cdot diag_{2d}$
            \State $remaining \gets (dx - diag_{2d}) + (dy - diag_{2d}) + (dz - diag_{2d})$
            \State $h \gets h + remaining$
        \EndIf
        
        \State \Return $h$
    \EndFunction
\end{algorithmic}
\end{algorithm} 

\end{minipage}
\end{center}

\paragraph{A* Algorithm Core Logic Implementation} ~\\
The core logic of A* algorithm involves determining the neighbouring nodes,
checking the occupancy of the neighbouring nodes, calculating their costs, and
adding the nodes to a separate list that deviates from unexplored nodes. The
algorithm is also responsible to update its list by adding or removing
neighbouring nodes, if a better path is found. The core logic works according to the
following steps:

\begin{enumerate}
    \item Initialize the grid, convert the start and end coordinates to grid
    indices, and adjust the indices if the start and end nodes are inside
    obstacles.
    \item Initialize the heapfile to store the exploring nodes, check the node with lowest $f(n)$, skip if already explored.
    \item Move to the next node, check if the current node is the goal node. If so, backtrack and reconstruct the path.
    \item If not, check the neighbouring nodes, calculate their tentative costs, and add them to the open set if they are not already explored.
    \item If a node is already in the open set but a better path is found,
    update the parent node, adjust the $g(n)$ to the lowest cost, and add to
    the queue again with  new priority.
    \item Repeat steps 2-5 until the goal node is reached or no path is found.
\end{enumerate}

Below is the flow chart illustrating the core logic of A* algorithm:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Included_Images/Astar_flowchart.png}
    \caption{A* Algorithm Core Logic Flowchart}
    \label{fig:astar_flowchart}
\end{figure}

\paragraph{Evaluating the A* Algorithm Performance} ~\\

In order to evaluate the performance of the algorithm, we conducted a series of
test runs in different categories of vegetation density maps. The maps were
obtained by QGround Control software satellite imagery and then imported to an
image processing pipeline. The image processing pipeline can be elaborated in following steps:
\begin{enumerate}
    \item Convert the satellite imagery to grayscale images.
    \item Use HSV (Hue, Saturation, Value) color space conversion and RBG
    Conversion to identify greenery, and NDVI Index to measure vegetation
    health.
    \item The images are categorized into three vegetation types: Dense tree
    canopy, grassland and short vegetation, and cropland and agricultural fields.
    \item Apply edge detection such as canny detection to identify buildings and non vegetation areas.
    \item Plot the series of processed images as occupancy grid maps.
\end{enumerate}

Figure \ref{fig:dense_canopy_map} is an example of processed occupancy grid maps for a dense canopy vegetation:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/densecanopy_map.png}
    \caption{Processed Occupancy Grid Map for Dense Canopy Vegetation}
    \label{fig:dense_canopy_map}
\end{figure}

\paragraph{Normalized Difference Vegetation Index} ~\\
Normalized Difference Vegetation Index (NDVI) is a quantified vegetation index that measures
the difference between near-infrared (which vegetation strongly reflects) and
red light (which vegetation absorbs) \cite{GISGeography_NDVI}. Below is the equation to calculate NDVI:
\begin{equation}
    \text{NDVI} = \frac{\text{NIR} - \text{Red}}{\text{NIR} + \text{Red}}
    \label{eq:ndvi}
\end{equation}
The Normalized Difference Vegetation Index is calculated using Equation
\ref{eq:ndvi}, where \text{NIR} represents the near-infrared band and \text{Red}
represents the red visible light band.

If a vegetation is healthy it reflects more near-infrared and green light
(chlorophyll), while absorbing more red and blue lights.

Test run results of the A* algorithm in different vegetation density maps are
shown in the results and discussion section.

\subsubsection{Coverage Path Planning}
Coverage Path Planning (CPP) is essential for surveying in dense vegetation
areas and obtaining dynamic data for LiDAR SLAM. In this research, we
implemented Boustrophedon Cell Decomposition as the CPP algorithm. The Boustrophedon Cell
Decomposition algorithm works by decomposing the map into cells based on the
polygonal shape of the map and the obstacles present. After decomposing the map
into cells, the algorithm will then generate a sweeping path in each cell based
on the given parameters (refer to Figure \ref{fig:boustrophedon}). Having this sweeping technique
in all the cells will ensure that the entire area is covered. Below is the
detail explanation of now the boustrophedon cell decomposition is implemented,
using the framework developed by Autonomous Systems Lab at ETH Zurich
\cite{B_hnemann_2021}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Included_Images/boustrophedon_path.png}
    \caption{Boustrophedon Cell Decomposition Path Planning Example}
    \cite{Choset-1997-16422}
    \label{fig:boustrophedon}
\end{figure}

\paragraph{Boustrophedon Cell Decomposition Implementation} ~\\

The algorithm decomposite the map (polygonal area) into cells based on three key events:
\begin{itemize} 
    \item \textbf{In Event}: When the sweep line encounters the start of an
    obstacle, the current cell is divided into two new cells. The algorithm is
    defined by an if-else loop that compares the points of the vertexes of the
    obstacle and the sweep line intersection. IN event moves the intersecting
    vertexes as the same direction as the sweep line.

    The if else logic in an IN event can be interpreted as follows:
    \begin{equation}
        \text{If } target\_upper > source\_upper \text{ and } target\_lower > source\_lower
    \end{equation}
Figure \ref{fig:in_event} illustrates an example of an IN event:
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\linewidth]{Included_Images/in_event.png}
        \caption{Boustrophedon Cell Decomposition IN Event Example}
        \label{fig:in_event}
    \end{figure}

    \item \textbf{Middle Event}: When the sweep line passes through the obstacle
    and updates it cell status.
    \item \textbf{Out Event}: The sweep line encounters the end of an obstacle,
    and two cells are merged into one cell. The algorithm is defined by an
    if-else loop that compares the points of the vertexes of the obstacle and the 
    sweep line intersection. OUT event moves the intersecting vertexes in the
    opposite direction of the sweep direction.

    The if else logic in an OUT event can be interpreted as follows:
    \begin{equation}    
        \text{If } target\_upper < source\_upper \text{ and } target\_lower < source\_lower
    \end{equation}
Figure \ref{fig:out_event} illustrates an example of an OUT event:
    \begin{figure}[H]   
        \centering
        \includegraphics[width=0.6\linewidth]{Included_Images/out_event.png}
        \caption{Boustrophedon Cell Decomposition OUT Event Example}    \cite{Choset-1997-16422}
        \label{fig:out_event}
        \end{figure}
\end{itemize}

After decomposition of the map into cells, the algorithm will visit each and
every cell based on depth-first search (DFS) technique. The algorithm will
choose a random cell to visit at first and then proceed the neighboring cells of
the chosen cell. The algorithm will also backpropagate through the visited cells, to ensure all the cells are visited. 

When visiting each cell, the algorithm will generate a sweeping path based on
the given parameters such as sweep direction and sweep distance. The sweeping
path can be performed in horizontal, vertical. The angle of the sweep line can
also be adjusted to fit the obstacle edges. The sweep distance is defined as the
distance between each sweep line. Figure \ref{fig:sweep_parameters} illustrates
the sweeping in each cell. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Included_Images/sweep_parameters.png}
    \caption{Boustrophedon Cell Decomposition Sweeping}
     \cite{Choset-1997-16422}
    \label{fig:sweep_parameters}
\end{figure}

The practical implications of the algorithm usage is shown in the experiment
setup section and the detailed results of the Boustrophedon Cell Decomposition
algorithm is discussed in the discussion section.


\subsection{GNSS-R}
The GNSS-R framework takes in raw GNSS data collected from the onboard
receivers, extracts raw GNSS measurements, and analyzes the characteristics of
such measurements with respect to the ground features that falls within the
Fresnel zones. Ultimately, the correlation between GNSS measurement features and
vegetation properties would be systematically summarized and modeled. The method
used for retrieving GNSS measurements and the Fresnel zone from collected raw data
is briefly outlined in the following two sections, while the MATLAB code for the 
GNSS-R framework is presented in the Appendix.

\subsubsection{GNSS Measurement Extraction}
Two raw data sources are necessary for the extraction of key GNSS measurements
used in this project: the raw binary GNSS data and the Pixhawk controller log.
First, the binary GNSS data is converted into readable observation files using
RTKCONV \cite{takasu2009rtklib}, and together with the satellite ephemeris data
downloaded from \url{https://cddis.nasa.gov/archive/gnss/data/}, measurements
such as the satellite elevation angle, pseudorange, and carrier to noise ratio
could be extracted using the MatRTKLIB library \cite{taroz:matrtklib}. From the
Pixhawk controller log, the UAV position, which is a filtered positioning result
from various onboard sensors, is obtained and regarded as the ground truth
assuming that the positioning error is small. Ultimately, two major GNSS
measurement are taken for further analysis: carrier to noise ratio ($C/N_0$) and
pseudorange error. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/GNSS_Measurement_Flow.png}
    \caption{Flowchart of extracting GNSS measurements from raw data.}
\end{figure}

\paragraph{Carrier to Noise Ratio} ~\\
Carrier to noise ratio refers to the power of received carrier signal relative
to the noise power per unit bandwidth, usually expressed in decibel-Hertz (dB-Hz)
\cite{joseph2010gnss}. $C/N_0$ can be readily obtained from the receiver, and it 
is calculated using the following equation:
\begin{equation}
    C/N_0 = C - (N - BW)
\end{equation}
where $C$ refers to the carrier power in dBm or dBW; $N$ refers to the noise power
in dBm or dBW; and $BW$ refers to the bandwidth of the observation in Hz.

\paragraph{Pseudorange Error} ~\\
In GNSS, pseudorange refers to the "apparent" distance between a satellite and a
receiver, which includes the true geometric range plus various biases and delays.
For the $i$th satellite, its pseudorange $\rho^i$ could be expressed as the following 
\cite{kaplan2017understanding}:
\begin{equation}
    \rho^i = R^i + \Delta t_r + \Delta t_s^i + I^i + T^i + \epsilon^i
\end{equation}
where $R^i$ refers to the true geometric distance between the satellite and the receiver;
$\Delta t_r$ refers to the receiver clock bias; $\Delta t_s^i$ refers to the satellite
clock bias; $I^i$ refers to the ionospheric delay; $T^i$ refers to the tropospheric 
delay; and $\epsilon^i$ refers to the pseudorange error, which is mainly
introduced through signal reflections. Among all the signals received at a given
instant, a satellite $m$ with the highest elevation angle is selected as the
master satellite, where the assumption that the master satellite is free of reflections
is applied \cite{hsu2018analysis}:
\begin{equation}
    \rho^m = R^m + \Delta t_r + \Delta t_s^m + I^m + T^m.
\end{equation}
Assuming $\Delta t_s^i$ and $\Delta t_s^m$ can be effectively removed by 
satellite-broadcasted parameters, $I^i$ and $I^m$ can be effectively removed by the
Klobuchar model, and $T^i$ and $T^m$ can be effectively removed by the Saastamoinen model,
the pseudorange equations can be approximated as \cite{kaplan2017understanding}:
\begin{equation}
    \rho^i = R^i + \Delta t_r + \epsilon^i,
\label{ith satellite pseudorange}
\end{equation}
\begin{equation}
    \rho^m = R^m + \Delta t_r.
\label{master satellite pseudorange}
\end{equation}
Finally, taking the difference between \eqref{ith satellite pseudorange} and 
\eqref{master satellite pseudorange} and rearranging the equation will result in the
pseudorange error:
\begin{equation}
    \epsilon^i = \rho^i - \rho^m - R^i + R^m.
\end{equation}

\subsubsection{Fresnel Zone Analysis}
Following the extraction of GNSS measurements observed onboard the UAV, the GNSS
signal would be particularly analyzed with respect to the 2-dimensional ground region 
that it is reflected from. Within this region, a specular reflection point could be found,
where the angle of incidence is equal to the angle of reflection. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/Fresnel_Zone_Illustration.png}
    \caption{Schematic diagram of GNSS-R signal scattering geometry.}
\end{figure}
The primary scattering area around the specular point in which reflected signals
arrive at the receiver in coherence is called the first Fresnel zone (FFZ). By
definition, it is an ellipse for which the signal phase change across is within
$\frac{\lambda}{2}$ radians, where $\lambda$ refers to the signal wavelength
\cite{Jia2018}. Given the incidence angle $\theta$ and the height of the
receiver $H$, the semi-major axis $a$ and the semi-minor axis $b$ of the FFZ can
be calculated as the following \cite{yu2021theory}: 
\begin{equation}
    a = \frac{\sqrt{\lambda H sin(\theta)+\frac{\lambda^2}{4}}}{(sin(\theta))^2},
\end{equation}
\begin{equation}
    b = \frac{\sqrt{\lambda H sin(\theta)+\frac{\lambda^2}{4}}}{sin(\theta)}.
\end{equation}
To calculate the location of the FFZ center, a local coordinate system is
defined. The origin is the receiver's ground projection, while the y-axis aligns
with the ground projection of the receiver-to-satellite vector. The z-axis
follows the local East-North-Up (ENU) "up" direction, and the x-axis completes
the right-handed system by remaining orthogonal to both x and y axes. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Included_Images/Fresnel_Zone_Coordinates.png}
    \caption{Schematic diagram of the local coordinate system defined.}
\end{figure}
In this coordinate system, the location of the FFZ center could be expressed as
$(x_c, y_c, z_c)$, where \cite{yu2021theory}:
\begin{equation}
    x_c = 0
\end{equation}
\begin{equation}
    y_c = (\frac{\lambda}{2}+H sin(\theta))\frac{cos(\theta)}{(sin(\theta))^2}
\end{equation}
\begin{equation}
    z_c = 0
\end{equation}
The flowchart for calculating the FFZ shape and location is outlined in the
figure below. Here, since the UAV true position is given in the global
coordinate of latitude, longitude, and height (LLH) in the Pixhawk log data, the
terrain height shall be subtracted to obtain the UAV height above ground, $H$.
This terrain height is obtained through the Hong Kong Digital Terrain Model
(\url{https://data.gov.hk/en-data/dataset/hk-landsd-openmap-5m-grid-dtm}), which
shows the topographical information in 5 meter by 5 meter grids with an accuracy of
$\pm$ 5 meters \cite{dataDigitalTerrain}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Included_Images/FFZ_Flow.png}
    \caption{Flowchart of calculating FFZ shape and location from collected data.}
\end{figure}


\subsection{Machine Learning}
The projects aims to utilize machine learning algorithms for two purposes:
(1) to classify the received signals into three classes: reflected from ground,
direct to down antenna and reflected from trees, and (2) to predict vegetation 
properties based on the features extracted from GNSS measurements.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Included_Images/Classes.png}
    \caption{Classes of signals to be classified.}
\end{figure}
So far, only the first part has been attempted. A supervised learning approach was takes, where a dataset was 
constructed from collected GNSS data with labels assigned based on the collection scenarios. 
The collected data was analyzed and features were then selected to train a classification model. 
The details of the feature analysis, dataset construction, feature selection and model training 
are outlined in the following sections.

\subsubsection{Feature Analysis}
Before constructing and training a classification model, the collected GNSS 
measurements were analyzed to identify potential features that could help
distinguish between reflected and direct signals. Four major features were
found to have different distributions between the two scenarios, which are UAV hovering 
over trees and UAV hovering over bare ground. These features are outlined below:
\begin{itemize}
    \item\textbf{Carrier to Noise Ratio (C/N0)}: From the figures below, It can 
    be observed that the $C/N_0$ values for signals reflected from trees are
    generally lower than those for signals getting reflected from the bare ground. 
    This indicated that vegetation attenuated the GNS signal quality. The figures also 
    illustrate a wider spread of $C/N_0$ values for signals reflected from trees compared
    to those from bare ground, in which case the signal distribution is more 
    concentrated. 
    \begin{figure}[H]
        \centering
        \includegraphics[width= 1\textwidth]{Included_Images/CNR.png}
        \caption{Distribution of $C/N_0$ for signals reflected from trees and bare ground.}
    \end{figure}
    
    \item\textbf{Pseudorange}: Similar to $C/N_0$, the pseudorange values for signals
    reflected from trees display an increased pseudorange bias and variance, which produces a
    ckear decision boundary for the classifier than that of bare ground signals.
    Some of the pseudorange values for signals reflected from trees are extremely high, which 
    might be due to the multipath effect caused by dense vegetation. The figures below
    illustrate the distribution of pseudorange values for signals reflected from trees
    and bare ground.
    \begin{figure}[H]
        \centering
        \includegraphics[width= 1\textwidth]{Included_Images/Pseudorange.png}
        \caption{Distribution of pseudorange for signals reflected from trees and bare ground.}
    \end{figure}
    
    \item\textbf{Doppler measurement}: Dopple measurement refers to the instantaneous
    shift in frequency of the received GNSS carrier signal due to the relative 
    velocity between the satellite and the receiver. As can be seen from the 
    figures below, the doppler values for signal reflected from trees are clearly
    distinguishable from those reflected from bare ground. For the case of bare ground, 
    the doppler values for up and down antenna are almost identical. However, for 
    the case of UAV hovering over vegetation, the doppler distribution is more
    spread out, 
    
    \item\textbf{Carrier Phase}:
\end{itemize}

\subsection{LiDAR SLAM}
\subsubsection{LiDAR-Inertial Odometry}
For the front end of the modular SLAM system, LiDAR-Inertial Odometry (Fast-LIO)
is used to build a map effectively within a short amount of time. To do this,
Iterative Extended Kalman Filter (IEKF) is used to fuse the data between IMU and LiDAR
point clouds to create an odometry of the environment. 

Iterative Extended Kalman Filter (IEKF) follows the core of Bayesian Recursion
to estimate the state of the robot through the prediction and measurement update
step. In the prediction step, IEKF assumes that the state of the system at time
$k$ evolved from the prior state at time $k-1$ is shown as follows:
\begin{equation}
    x_k = f(x_{k-1},u_k) + w_{k} 
\end{equation}

Where $x_k$ is the state vector containing the terms of interest for the system
at time $k$. The $f(.)$ represents a non-linear state function that is used to
forecast current state data from prior state data $x_{k-1}$ and $u_k$ is a control vector. We can approximate $w_{k-1}$ as
$N(0,Q_k)$ where it has a zero-mean Gaussian distribution with covariance matrix
$Q_k$.

To know the uncertainty of the prediction model, error covariance matrix $P^f_k$
is defined as shown:
\begin{align}
    P_k^f &= E\left(e_k^f(e_k^f)^T\right) \nonumber \\
    &\approx F_k E(e_{k-1}e_{k-1}^T)F_k^T + E(w_{k-1}w_{k-1}^T) \nonumber \\
    &= F_kP_{k-1}F_k^T + Q_{k-1}
\end{align}

Where $e$ is the prediction error, $F$ is the Jacobian matrix of the linearized
state transition function $f(.)$ evaluated at the previous estimate
$x_{k-1}$, and $Q$ is the process noise covariance matrix representing the
uncertainty of the physical model.

Even so, the predicted state alone is prone to errors due to various
factors such as inaccurate prediction function and further source of noise. This
issue can be solved by taking sensor measurements to improve the predicted state
accuracy. Hence the observation model is defined as shown:
\begin{equation}
    z_k = h(x_k) + v_k
\end{equation} 

Where $z_k$ represents the expected sensor measurements based on the predicted
state $x_k$ expressed by a non-linear function $h(.)$, and the observation noise
$v_k$ is assumed as $N(0,R_k)$ where it has zero mean with covariance matrix $R_k$.

To further improve the accuracy for the nonlinear system, an iteration index
$K$ is added to repeat the measurement update at time step $k$ as shown:
\begin{equation}
    x^a_{k,K=0} = x^f_k
\end{equation}

Where $x^a$ is the state estimate after corrected by sensor measurement and
$x^f$ is the predictive distribution before looking at the lidar data. In this
case, the iteration $K$ is repeated until the change in the state estimate
$|x^a_{k,K+1} - x^a_{k,K}|$ falls below threshold $\epsilon$. 

As actual measurements from sensors will typically differ from this prediction
due to sensor noise and model inaccuracies, it is incorporated into the state
estimate by computing the difference between the actual measurements and the
predicted measurement based on the current iterative estimate $K$:
\begin{equation}
    z_{k, K} = z_k - h(x^a_{k, K})
\end{equation}
Where $h(x^a_{k, K})$ is the non-linear observation function evaluated at the
most recent estimate. To balance the uncertainty of the prediction against the
new sensor data, the Kalman Gain $K_{k,K}$ is calculated using the state-space
dimension formula \cite{xu2021fast}.
\begin{equation} 
    K_{k, K} = (H_{k, K}^T R_k^{-1} H_{k, K} + (P_k^f)^{-1})^{-1} H_{k, K}^T R_k^{-1} 
\end{equation}

Where $H_{k, K}$ is the Jacobian of the observation function re-evaluated
at the current iterative estimate $x^a_{k, K}$. This gain is then used
to compute the next refined state estimate:
\begin{equation}
    x^a_{k,K+1} = x^a_{k, K} + K_{k, K} (z_{k, K}) - (I - K_{k, K} H_{k, K})(x^a_{k, K} - x_k^f)
\end{equation}

After updating the state estimate, the updated covariance of the state
distribution is given as:
\begin{equation} 
    P_k = (I - K_{k, K} H_{k, K}) P_k^f 
\end{equation}

While the IEKF provides a high-frequency estimate of the current state, the
overall SLAM problem can be generalized as a Maximum A Posteriori (MAP)
estimation. Bayes Filter is then used to recursively perform the state
prediction and measurement update steps to minimizes linearization errors for
robust localization. In this case, MAP can be expressed as:
\begin{equation}
    P(X|Z, U) \propto P(x_0) \prod_{k} P(z_k|x_k) \prod_{k} P(x_k|x_{k-1}, u_{k-1})
\end{equation}

Where $P(X|Z, U)$ is the refined state estimate, $P(x_0)$ is the prior,
$P(z_k|x_k)$ is the sensor measurement and $P(x_k|x_{k-1}, u_{k-1})$ is the
predicted state.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/Kalman Filter.png}
    \caption{The red distribution in the figure represents the predicted step $P(x_k|x_{k-1}, u_{k-1})$  which results in predictive distribution  $x^f_k$ .The blue distribution represents likelihood $P(z_k|x_k)$ derived from the noisy sensor measurement $z_k$. The green distribution illustrates the refined state estimate $x^a_k$, showing the reduction in the uncertainty covariance $P_k$ after fusing both data sources \cite{faragher2012understanding}.}
\end{figure}

\subsubsection{Graph-Based SLAM}
As LiDAR odometry is used in the front end to provide high-rate local pose
estimates without global constrains, iSAM2 pose graph optimization architecture
is used for the back end to further optimize the point cloud map for higher
accuracy. This technique is rooted from the factor graph optimization algorithm
where Maximum A Posteriori (MAP) is also used to factorize the joint posterior
probability of the robot trajectory X as shown  \cite{wen2021factor}:

\begin{equation} 
    P(X|Z, U) \propto P(x_0) \prod_{k} P(z_k|x_k) \prod_{k} P(x_k|x_{k-1}, u_{k-1}) 
\end{equation}

Where $P(x_0)$ is the prior, $P(z_k|x_k)$ represents the Update Factors from
LiDAR measurements, and $P(x_k|x_{k-1}, u_{k-1})$ represents the Propagation
Factors derived from the IEKF prediction. The purpose of this is to decompose
the complex problem of estimating a full trajectory into smaller, simpler pieces
called 'factors'. By breaking down the trajectory into these discrete
components, this allows iSAM2 to optimize specific parts of the trajectory
affected by new information or loop closures instead of resolving the entire
global map every time the robot moves. This ensures that global consistency is
maintained without the computational burden of a full-batch optimization.

To do this, the Gaussian noise is factorized into exponential functions. Unlike
a standard filter that only estimates the current state $x_k$, the factorized
graph maintains the entire trajectory history $X$ = ($x_0, x_1, \dots, x_k$) to
minimize the sum of squared errors. The Gaussian noise from the IEKF motion
prediction is expressed as:
\begin{equation}
    P(x_k|x_{k-1},u_{k-1}) = \exp(-||f_k(x_{k-1}, u_k)-x_k||^2_{\Sigma_{u,k}})
\end{equation}

Similarly, the Gaussian noise from lidar measurements is factorized into the following equation:
\begin{equation}
    P(z_k | x_k) = \exp(-||h_k(x_k) - z_k||^2_{\Sigma_{z,k}})
\end{equation}

The factorization is crucial to transform MAP estimation from a product
of probability distributions into a Non-linear Least Squares (NLS) optimization
problem. By taking the negative logarithm of the posterior, we convert the task
of finding the maximum probability into finding the minimum of an error function
$e(\mathcal{X})$ as shown:
\begin{equation}
    \hat{\chi} = \arg\max_{\chi} \prod_{k} F_k(x_k) = \arg\min_{\chi} e(\chi)
\end{equation}
\begin{equation}
    e(\chi) \doteq \sum_{k} ||h_k(x_k) - z_k||^2_{\Sigma_k} + \sum_{k} ||f_k(x_{k-1}, u_k) - x_k||^2_{\Sigma_k}
\end{equation}

To solve the non-linear least squares problem defined by $e(\mathcal{X})$, the
system iteratively refines the trajectory estimate to find the optimal solution
$\hat{\mathcal{X}}$. The error function is linearized using a first-order Taylor
expansion around the current estimate $\mathcal{X}^{(i)}$ where the state is
then uploaded incrementally:
\begin{equation}
    \mathcal{X}^{(i+1)} = \mathcal{X}^{(i)} + \Delta \mathcal{X}
\end{equation}

\hl{Where $\Delta \mathcal{X}$ is the optimal step calculated to move the current estimate toward the minimum of the "error
valley".This optimization process relies on the Jacobian matrix $\mathbf{J}_R$,
which represents the first-order partial derivatives of the residuals with
respect to the state variables. In the context of LiDAR-Inertial integration,
the Jacobian is composed of two primary components: the partial derivatives of
the LiDAR measurement residuals and the propagation model
residuals: (EQUATIONS)

By setting the gradient of the error function to zero, the system solves for
$\Delta \mathcal{X}$ through the normal equations, typically using the
Gauss-Newton method. Unlike the IEKF, which only iterates on the current state
at time $k$, this graph-based optimization allows the system to re-linearize the
entire trajectory historyor a sliding window of itensuring that local sensor
noise does not lead to permanent global map distortion }

\begin{equation}
    \mathbf{J}_R = \frac{\partial(\mathbf{H}(\mathcal{X}) - \mathbf{Z})}{\partial \mathcal{X}} + \frac{\partial(\mathbf{F}(\mathcal{X}, \mathbf{U}) - \mathcal{X})}{\partial \mathcal{X}}
\end{equation}

\subsubsection{System Architecture}
In this project, a modular SLAM system is used with the front end having a
Fast-LIO lidar odometry to carry out mapping in real time, while Pose Graph
Optimization (PGO) coupled with GPS is used for the back end. During
implementation, both Fast-Lio and PGO are initialized. The point cloud data will
first be processed by Fast-LIO to create an odometry, and the odometry will then
be passed to the PGO to further optimize the point clouds to increase accuracy.
In the meantime, GPS data is passed into PGO to provide constraints to the SLAM
if large drifts occur. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Included_Images/SLAM flowchart.png}
    \caption{Flowchart of the modular SLAM system}
\end{figure}

This project builds upon an open-source implementation
that integrates Fast-LIO with PGO for GPS-aided SLAM:
\url{https://github.com/yxw027/FAST_LIO_GPS} 

The pseudocode of the front-end and back-end of the modular SLAM can be shown below:

{
    \singlespacing
    \begin{algorithm}[H]
    \caption{Front-End: FAST-LIO with MAVLink TimeSync}
    \begin{algorithmic}[1]
    \State \textbf{Initialize:} IEKF state $\mathbf{x}$ and covariance $\mathbf{P}$
    \State \textbf{Initialize:} ikd-Tree for incremental mapping
    \While{ROS is active}
        \State Sync LiDAR and IMU buffers based on GPS-aligned timestamps
        \If{IMU not initialized}
            \State Perform \textit{IMU\_init} (Gravity/Bias estimation)
            \State \textbf{continue}
        \EndIf
        \State \textbf{State Prediction:} $\hat{\mathbf{x}} \leftarrow$ IMU forward propagation
        \State \textbf{Undistortion:} Compensate point motion using $\hat{\mathbf{x}}$
        \For{$iter = 1$ \textbf{to} $NUM\_MAX\_ITERATIONS$}
            \State Find point-to-plane correspondences in ikd-Tree
            \State Compute measurement residual $\mathbf{z}$ and Jacobian $\mathbf{H}$
            \State \textbf{Update:} Correct state $\mathbf{x}$ using IEKF update rule
            \If{converged} \State \textbf{break} \EndIf
        \EndFor
        \State \textbf{ikd-Tree Update:} Add new features to the spatial map
        \State Publish Odometry to Back-End
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
}

{
    \singlespacing
    \begin{algorithm}[H]
    \caption{Back-End Pose Graph Optimization (PGO)}
    \begin{algorithmic}[1]
    \State \textbf{Initialize:} GTSAM Factor Graph and ISAM2 optimizer
    \State \textbf{Initialize:} Noise models (Odometry, GPS, Loop Closure)
    \While{ROS is active}
        \If{New Front-End Odometry received}
            \State Calculate delta movement since last Keyframe
            \If{movement $>$ $KeyframeThreshold$}
                \State Create new Node $n_t$ in Pose Graph
                \State \textbf{Propagation Factor:} Add \textit{BetweenFactor}($x_{k-1}, x_k, \Delta Odom$)
                \If{GPS matched for $n_t$}
                    \State Transform GPS Lat/Lon to UTM coordinates
                    \State \textbf{GPS Factor:} Add \textit{GPSFactor}($x_k, UTM_{xyz}$)
                \EndIf
                \State \textbf{Loop Closure:} Search for matches using Scan Context
                \If{Loop detected with $n_{\text{old}}$}
                    \State Calculate relative transform via ICP
                    \State \textbf{Loop Factor:} Add \textit{BetweenFactor}($x_{\text{old}}, x_t, T_{\text{ICP}}$)
                \EndIf
                \State Perform ISAM2 global optimization
                \State Update trajectory with optimized poses
            \EndIf
        \EndIf
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/rqt_graph.png}
    \caption{Interactions of different ROS topics in the SLAM system}
\end{figure}




% --- Experiments ---
\section{Experiments}
Placeholder text

\subsection{Experiment Workflow}
Placeholder text

\subsection{Summary of Experiments Conducted}
Placeholder text

\subsection{Key Experiment 1}
Placeholder text

\subsection{Key Experiment 2}
Placeholder text


% --- Results and Discussion ---
\section{Results and Discussion}
Placeholder text

\subsection{Path Planning}
Placeholder text

\subsection{GNSS-R}
Placeholder text

\subsection{Machine Learning}
Placeholder text

\subsection{LiDAR SLAM}
\subsubsection{Stage 1: Modular SLAM Implementation without GPS}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg1_fastlio.png}
    \caption{Fast-LIO LiDAR Odometry}
    \label{fig:stg1_fastlio}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg1_pgo.png}
    \caption{PGO with FPFH and ICP}
    \label{fig:stg1_pgo}
\end{figure}
The implementation of modular SLAM is first done without the integration of GPS
to make sure the architecture works. When comparing \autoref{fig:stg1_fastlio}
and \autoref{fig:stg1_pgo}, it is observed that there is less drifting in the
point clouds in the PGO SLAM. This is because the PGO SLAM uses Fast Point
Feature Histograms (FPFH) method to have an initial guess to align the point
clouds and uses Iterative Closest Point (ICP) for further refinement. FPFH can
effectively represent the local geometry around key points in a point cloud which
makes it robust against changes in viewpoints. Since FPFH is better at aligning
point clouds that are far away for global robustness, it is then passed to ICP
for further precise refinement with millimeter-level precision
\cite{song2025robotic}. 

On the other hand, Fast-LIO only uses Iterated Extended Kalman Filter (IEKF) to
align the point clouds. In this case, we can see that the dual layer of point
cloud alignment in PGO further increase the accuracy in SLAM.

\subsubsection{Stage 2: Modular SLAM Implementation with GPS}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg2_pgomis.png}
    \caption{PGO SLAM after integration of GPS. Green trajectory indicates LiDAR odometry, 3 axis trajectory indicates GPS trajectory}
    \label{fig:stg2_pgo}
\end{figure}
Once gps is being integrated to the PGO SLAM, we can see in \autoref{fig:stg2_pgo} that there
are two different trajectories and the point clouds are very misaligned. First
of all, both trajectories are not aligned as the position of the LiDAR and the
GPS on the drone itself are different, hence having two different frames. Second of all, the point clouds are misaligned as GPS
has a high gain in the SLAM and some issues related to the transformation of the
GPS to LiDAR odometry. 

To solve the misalignments of both trajectories, extrinsic calibration is
carried out between IMU from Pixhawk that is connected to the GPS and the LiDAR
point clouds to align both frames together. In this case, HKU Mars calibration
system is used for the extrinsic calibration:
\url{https://github.com/hku-mars/LiDAR_IMU_Init/issues} 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg3_fastlio.png}
    \caption{Fast-LIO LiDAR Odometry}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg3_pgo.png}
    \caption{PGO SLAM with GPS and odometry trajectory aligned. Green trajectory indicates LiDAR odometry, 3 axis trajectory indicates GPS trajectory}
    \label{fig:stg3_pgo}
\end{figure}

Based on \autoref{fig:stg3_pgo}, despite both odometry and GPS trajectory and aligned, we can
still observe that the point clouds in PGO are still misaligned. Aside from having high
gain for GPS in the SLAM, the misalignment is due to the issues regarding the coordinate
frame rotation between GPS and LiDAR Odometry, and GPS transformation logic which
needs to be solved in later stage. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/withgps.png}
    \caption{PGO with GPS integration}
    \label{fig:withgps}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/nogps.png}
    \caption{PGO without GPS integration}
    \label{fig:nogps}
\end{figure}

\autoref{fig:withgps} and \autoref{fig:nogps} shows the difference between
the PGO SLAM with and without GPS integration when using the same architecture. In
this case we can confirm that GPS is the main issue that causes the point cloud
distortion.

% --- Conclusion ---
\section{Conclusion}
Placeholder text


% --- Future Works ---
\section{Future Works}
Placeholder text

\subsection{Path Planning}
Placeholder text

\subsection{GNSS-R}
Placeholder text

\subsection{Machine Learning}
Placeholder text

\subsection{LiDAR SLAM}
Placeholder text


% --- Project Management ---
\section{Project Management}
Placeholder text

\subsection{Gantt Chart}
Placeholder text

\subsection{Project Difficulties and Solutions}
Placeholder text

\subsubsection{Path Planning}
Placeholder text

\subsubsection{GNSS-R}
Placeholder text

\subsubsection{Machine Learning}
Placeholder text

\subsubsection{LiDAR SLAM}
Placeholder text


% --- Appendix ---
\newpage
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}

\subsubsection*{GitHub Repositories}
The code scripts of this project are organized in various GitHub repositories, which
aligns with different frameworks of this project. Specifically: 
\begin{itemize}
    \item GNSS-R: \url{https://github.com/zhlouise/UAV_GNSS-R}
\end{itemize}


% --- References ---
\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{IEEEtran}
\bibliography{References.bib} 


\end{document}