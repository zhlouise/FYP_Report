\documentclass[12pt, a4paper]{article}


% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times} % Sets font to Times New Roman
\usepackage[margin=2.5cm]{geometry} % Sets 2.5cm margins
\usepackage{setspace} % Required for double spacing
\usepackage{titlesec}
\usepackage{sectsty}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{color}
\usepackage{amsmath}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{parskip}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{ragged2e}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{pgfgantt}
\usepackage{rotating}
\usepackage[dvipsnames]{xcolor}

\definecolor{overallcolor}{RGB}{114,158,206}
\definecolor{pathcolor}{RGB}{230,145,56}
\definecolor{gnsscolor}{RGB}{102,194,164}
\definecolor{mlcolor}{RGB}{194,165,207}
\definecolor{slamcolor}{RGB}{158,202,225}
\usepackage{longtable}


% --- Settings ---
\doublespacing % Applies double spacing to the document
\sectionfont{\normalsize}
\subsectionfont{\normalsize\it\bfseries}
\subsubsectionfont{\normalsize\it}
\setlength{\parindent}{0pt}
\urlstyle{same}


\begin{document}


% --- Title Page ---
\begin{titlepage}
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/PolyU_Logo.png}
    \hfill
    \includegraphics[width=0.25\textwidth]{Included_Images/FENG_Logo.png}
    \vfill
    {\Large 2025/26 Interdepartmental Final Year Project Interim Report\par}
    {\huge \textbf{Intelligent UAV Systems for GNSS-Based Remote Sensing on
    Vegetation} \par}
    \vfill
    {\large [AAE] ZHOU Jiayi (22099961D) \par} {\large [AAE] MAHMUD Md Sahat
    (22097159D) \par} {\large [EEE] GAMAGE Sashenka (22097129D) \par} {\large
    [ME] TAN Qing Lin (22101126D) \par}
    \vfill
    {\large \textbf{Chief Supervisor: } [AAE] Prof. Guohao ZHANG \par} {\large
    \textbf{Co-Supervisor: } [AAE] Prof. Li-Ta HSU \par}
    \vspace{1cm}
    {\large \textbf{Date of Submission: } 2026 January 18 \par}
\end{titlepage}


% --- Table of Contents ---
\pagenumbering{roman}
\tableofcontents
\newpage


% --- Main Content ---
\pagenumbering{arabic}


% --- Abstract ---
\section{Abstract}
\begin{itshape}
Vegetation monitoring is essential for environmental sustainability, yet
conventional sensing methods are often limited by high costs, weather
dependency, and low spatial resolution. This project explores an alternative
solution through developing an intelligent Unmanned Aerial Vehicle (UAV) system
that integrates Global Navigation Satellite System Reflectometry (GNSS-R) remote
sensing, autonomous path planning algorithms, machine learning techniques, and
Light Detection and Ranging (LiDAR) based mapping for robust vegetation
monitoring. Currently, the system design implements A* and Boustrophedon
algorithms for path planning, a dual antenna system with opposite polarization
for GNSS signal reception, a LightGBM model for GNSS signal classification, and
a modular LiDAR Simultaneous Localization and Mapping (SLAM) framework for
generating canopy maps. In this interim report, preliminary experiments
demonstrating successful data collection, GNSS signal feature extraction,
analysis, and classification, as well as functional path planning and SLAM
modules will be detailed. With the foundational platform developed, the ongoing
works will focus on system integration, correlation modelling, and vegetation
parameter prediction to establish a robust and intelligent vegetation monitoring
technology.
\end{itshape}


% --- Introduction ---
\section{Introduction}
Vegetation is a critical asset to the environment and the human civilization.
Not only does vegetation produce essential societal resources, but its
distribution and productivity also greatly impacts the terrestrial ecosystems
and the global climate \cite{Richardson2013}. Therefore, the continuous and
accurate monitoring of vegetation is essential for sustainable resource
management \cite{Wallace2006}, ecosystem preservation \cite{Zeng2022}, and
climate change modeling \cite{Richardson2013}. 

Traditionally, methods of vegetation monitoring involved manual field
assessments of site characteristics, extracting vegetation condition indicators
such as species composition, geometrical structure, and biochemical activities
\cite{Gibbons2006}. However, the accuracy and efficiency of such monitoring
methods are highly dependent on the available expertise and resources, as the
site assessments often required the assessor to possess reasonable levels of
field knowledge prior to surveying \cite{Parkes2003}. To address the limitations
of manual assessments and to accommodate for the increasing large-scale
monitoring demands, remote sensing systems emerged as an essential tool for
ecological monitoring \cite{Li2023}. 

Remote sensing refers to the acquisition of an object's information through
measurements obtained without coming into direct contact with said object,
effectively minimizing the need for manual involvement on-site
\cite{campbell2011introduction}. Specifically, remote sensing relies on
information derived from different measurements of energy reflected from the
object of interest \cite{campbell2011introduction}. In the context of surveying
terrestrial vegetation, the remote sensing methods could be classified into two
categories based on the distance between the target object and the measurement
sensor: 1) space-borne and 2) airborne remote sensing. Space-borne remote
sensing involves the use of instruments onboard orbiting satellites, including
spectral and hyperspectral cameras \cite{Qian2022}, synthetic aperture radar
(SAR) \cite{Hu2025}, and space-borne LiDAR sensors \cite{Bergen2009}. Due to the
wide coverage and availability of space-borne data, large-scale terrestrial
changes over time could be captured, enabling the continuous monitoring of
macro-scale ecosystems \cite{Khan2024}. Compared to space-borne systems,
airborne remote sensing can provide significantly improved spatial resolution
and assessment flexibility through the integration of sensors onboard manned
aircraft or UAVs \cite{Khan2024}. Although limited by coverage area, airborne
remote sensing can provide timely information for addressing regional
emergencies such as pest \cite{Wulder2006} or wildfire \cite{Arroyo2008}
outspreads since the systems could be deployed on-demand. In UAV-based remote
sensing particularly, this temporal flexibility is further complemented with the
benefit of low operational cost, rendering it an ideal platform for monitoring
regional and urban vegetation \cite{Tang2015}. 

Conventionally, UAV remote sensing platforms carry similar instruments to that
of other remote sensing systems, including radar, LiDAR, and multi-spectral or
hyperspectral imagery sensors \cite{Tang2015}. However, while imaging
instruments are susceptible to the influence of lighting and weather
\cite{Wu2024}, LiDAR devices tend to face difficulties penetrating through dense
canopy \cite{Su2007}. Therefore, it is critical to explore a robust sensing
technique that is suitable for UAVs in terms of payload and power consumption to
complement the existing instruments. Recently, the technique of GNSS-R is
receiving increasing interest in the field of remote sensing. GNSS-R exploits
the L-band signals transmitted from Global Navigation Satellite Systems (GNSS)
that are then scattered on different terrain surfaces of the Earth
\cite{Jin2024}. Then, the reception of reflected GNSS signals can provide
information regarding the properties of the signal reflector on land
\cite{Jin2024}. As signals of opportunity conventionally dedicated to
Positioning, Navigation, and Timing (PNT) applications, GNSS is capable of
providing real-time measurements regardless of time, location, and weather
\cite{Jin2024}. Additionally, as a bi-static system where signal transmitters
are separated from receivers, GNSS-R is exempt from the need of dedicated
transmitter-receiver instruments that are crucial to mono-static radar systems
\cite{Jin2010}. Instead, any consumer-grade receivers capable of receiving
reflected GNSS signals could be used for GNSS-R remote sensing, further
demonstrating GNSS-R's applicability onboard low-cost remote sensing systems
such as an UAV. 

Recent research has demonstrated the potential of integrating GNSS-R techniques
with UAVs through an UAV-based GNSS-R system for water body detection and flood
mapping \cite{UAV-water}. Not only does this application demonstrate the
capability of obtaining high resolution and flexibility measurements onboard
low-altitude UAV platforms, but it also established UAV-mounted GNSS-R as a
promising tool for monitoring dynamic environmental conditions. However, while
existing systems are yet to be developed specifically for vegetation monitoring
purposes, they also often lack fully autonomous navigation capabilities.
Therefore, this project aims to address this gap through both the development of
a robust GNSS-R framework for vegetation remote sensing and the integration of
optimal trajectory planning, dynamic obstacle avoidance, and environment
mapping. An additional LiDAR-based 3D canopy map will also provide the critical
spatial reference to validate any vegetation parameters obtained through the
GNSS-R framework.

In summary, this project will integrate the technologies of autonomous
navigation, GNSS-R, machine learning, and LiDAR SLAM to achieve an intelligent
and structured remote sensing system for vegetation monitoring. The main
objectives of this project are as follows:
\begin{enumerate}
\item To introduce an autonomous path planning framework onboard the UAV platform
for optimized GNSS-R remote sensing.
\item To analyze and model the correlations between signal propagation parameters
retrieved through GNSS-R and ground vegetation conditions.
\item To classify signals reflected by vegetation and predict vegetation parameters
based on raw GNSS data using machine learning-based approach.
\item To establish a detailed 3D canopy map using LiDAR-based SLAM, providing a
spatial validation reference for the 2D vegetation features detected by GNSS-R.
\end{enumerate}

This report is structured as follows: Section 3 reviews contemporary research in
UAV path planning, GNSS-R, machine learning, and LiDAR SLAM, while Section 4
outlines the project's methodologies. The subsequent sections cover the
experiments conducted (Section 5), a discussion of current results (Section 6),
and the project conclusions (Section 7). Finally, Section 8 explores future
works, and Section 9 summarizes project management details. 


% --- Literature Review ---
\section{Literature Review}
To achieve the project objectives, this literature review comprehensively examines
the contemporary research in four domains, each associated with one project
objective. First, existing path planning methodologies are reviewed to identify
suitable algorithms for autonomous UAV navigation in vegetated environments.
Subsequently, the existing techniques of GNSS-R are explored, with a focus on
its prior applications in vegetation parameter retrieval. Then, machine learning
based techniques are investigated for the purpose of classification of reflected
and direct signals. Further investigation is conducted on machine learning based
approaches for predicting vegetation parameters from raw GNSS data. Finally,
LiDAR-based SLAM techniques are assessed to identify robust mapping and
localization frameworks suitable for generating 3D canopy models to provide
spatial references. Consequently, this review identifies the existing gaps in
each domain for the objective of vegetation monitoring, which will inform the
integrated project methodologies.


\subsection{Path Planning}
Path planning is a term affiliated with fields such as robotics, artificial
intelligence, and control systems. In robotics, path planning is a technique
that is used in navigating a robot autonomously from an origin to a destination
via a collision-free route. Therefore, path planning is primarily a geometric
problem that generates a path according to a given sequence of waypoints,
without accounting for variations over time. In contrast, trajectory planning
defines the path as a function of time, assigning specific timing to the
geometric route \cite{gasparetto2015path}. Currently, our research focuses on
implementing path planning for the autonomous navigation of an Unmanned Aerial
Vehicle (UAV) in outdoor environments. Consequently, we prioritize path planning
methodologies in this work, reserving trajectory planning and optimization for
future investigation. 

The earliest sensor-based, obstacle-avoiding path planning algorithms are known
as Bug1 and Bug2 algorithms \cite{choset2005principles}, which are designed to
first navigate around an obstacle and then determine the shortest path around
the obstacle to reach the goal (Figure \ref{fig:bug_algorithms}).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Included_Images/bug_algorithms.png}
    \caption{Bug1 and Bug2 path planning algorithms \cite{choset2005principles}.}
    \label{fig:bug_algorithms}  
\end{figure}
However, both Bug1 and Bug2 algorithms are used in scenarios where the robot has
no map (i.e., an unknown environment). Additionally, these algorithms do not
guarantee the optimal path. With the rise of SLAM techniques, maps of the navigation
environment can be generated, which enabled the transition to map-based path planning methods. Therefore, the conventional grid-based path planning algorithms will be 
integrated in this project, which enables the identification of potential vegetated
areas for further GNSS-R surveying.

One of the primary reasons for utilizing autonomous navigation and path planning
in our UAV system is that it is scalable across vegetation monitoring, providing
accurate and precise navigation in fragmented urban forests. To develop path
planning mechanisms, we conducted our literature review across four different
path planning approaches. 

\begin{enumerate} 
    \item \textbf{Search-based algorithms}: Dijkstra algorithm and A* algorithm.
    \item \textbf{Sampling-based algorithms}: RRT (Rapidly-exploring Random Tree)
    algorithm, RRT*, and PRM (Probabilistic Roadmap) algorithm.
    \item \textbf{Artificial Intelligence-based algorithms}: Soft-Actor-Critic.
    \item \textbf{Cell Decomposition Algorithms}: Boustrophedon Cell Decomposition.
\end{enumerate}

\paragraph{Search-based algorithms} ~\\
Dijkstra's algorithm solves the problem of finding the shortest path from a
given starting point to a given ending point. The algorithm explores this path
by simultaneously finding the shortest path from a given starting node to all
points in the graph \cite{javaid2013understanding}. As a result, the algorithm is known to perform the
guaranteed shortest path across two points. However, finding the shortest path
across all the points in the map consumes high computational cost, and
therefore, it is impractical to use for real-time UAV path planning. The update rule \ref{alg:dijkstra} illustrates the
pathfinding process of Dijkstra's algorithm.
\begin{algorithm}[H]
\caption{Dijkstra's Algorithm (Relaxation Step)}
\label{alg:dijkstra}

\begin{algorithmic}[1]
  \State $d[u] \gets$ current known shortest distance from start to node $u$
  \State $w(u,v) \gets$ weight (cost) of edge from $u$ to $v$
  \State \textbf{The core update rule is:}
  \State $d[v] \gets \min(d[v], d[u] + w(u,v))$
\end{algorithmic}
\end{algorithm}
To address the limitations of Dijkstra’s algorithm, scientists developed another
search-based algorithm called the A* (A-star) algorithm. A* algorithm is one of
the most common path planning algorithms, which can be applied to grid-based or
topological configuration spaces \cite{Astar_simulation}. This algorithm
combines the shortest path calculation in Dijkstra’s algorithm and heuristic
searching. The heuristic cost is known as the estimated cost from the current
node to the end node. A* algorithm is better in terms of reducing the search
space compared to the Dijkstra algorithm. However, it requires accurate
heuristics to determine the shortest path and has a large computation time
depending on the number of cells in the map.   \cite{DUCHON201459}.  

\paragraph{Sampling-based algorithms} ~\\
As previously discussed, search-based algorithms consume an excessive amount of
computational power in high-dimensional spaces and with a large number of
obstacles \cite{RRT_star}.  To avoid such burnout, sampling-based algorithms have been proposed
and have performed effectively in high-dimensional spaces, attracting
significant attention across several decades \cite{Kavraki},\cite{kavraki_etal}. One of the most common
sampling-based algorithms is RRTs and PRMs, which sample points randomly from
the given map. 

The RRT algorithm works by selecting a random available cell and randomly
checking its neighboring nodes. As a result, there is no need to assign nodes at
every point in the path. The algorithm spreads like a tree from the random
neighboring nodes and eventually will reach the endpoint. Additionally, RRT
effectively handles systems with differential constraints \cite{anytimeRRT},
making it a suitable algorithm for state-of-the-art robotics. Although RRT
quickly computes the path between the starting and ending points, it does not
guarantee the shortest path. This was also proven by Karaman and Frazzoli
\cite{karaman2010incrementalsamplingbasedalgorithmsoptimal}, showing that the
probability of the RRT algorithm converging to an optimal solution is zero.
Therefore, the RRT*, an optimal version of the RRT algorithm, was proposed. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Included_Images/rrt_vs_rrtstar.png}
    \caption{RRT vs RRT* Path Planning Algorithms}
    \label{fig:rrt_vs_rrtstar}
\end{figure}
In RRT*, the algorithm considers neighboring nodes within a window of radius r
and examines the shortest path by comparing nodes in the window, shown in Figure
\ref{fig:rrt_vs_rrtstar}. RRT* has the same properties as RRT, but with
near-neighbor search and tree rewriting operations
\cite{karaman2011samplingbasedalgorithmsoptimalmotion}. As the number of
iterations increases, RRT* performance improves, while the path becomes more
jagged with each iteration \cite{noreen2016comparison}. Moreover, PRM is another
type of sampling-based algorithm that has a network of possible paths in the
freely available space. By creating a network among the available spaces, the
PRM increases the path-finding speed by converting the map into a graph.
\cite{PRM}.

\paragraph{AI-based algorithms} ~\\
In recent years, deep reinforcement learning methods have been employed to
address task allocation and path planning problems in autonomous vehicles
\cite{RL-path-planning}. Multi-UAV path planning systems have been modeled as
multi-agent reinforcement learning problems \cite{SAC-path-planning}. Recent studies have merged the
Soft-Actor-Critic (SAC) reinforcement learning algorithm with a sampling-based
path planning algorithm to design adaptively informed trees (AIT*) \cite{ait}. The network
structure of the hybrid path planner is shown in the Figure \ref{fig:sac_ait}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/sac_ait.png}
    \caption{SAC Network Structure}
    \cite{SAC-path-planning}
    \label{fig:sac_ait}
\end{figure}

SAC learns a policy that specifies a given state and the corresponding action to
be taken. In other words, “given my current state (position, velocity, sensor
readings), what action (thrust, roll, yaw, pitch) should I take?” With the
correct reward design (e.g., penalties for collisions, rewards for reaching the
goal), SAC can learn to generate collision-free trajectories. The data needed
for each state should be stored in a replay buffer, including the state at a
given time, the action taken at that time, the reward received after taking that
action, and the next state after the action.

Table \ref{tab:algorithms_comparison} summarizes the advantages and disadvantages of the search-based,
sampling-based, and AI-based path planning algorithms. 

\begin{table}[ht]
    \newcolumntype{L}{>{\RaggedRight\arraybackslash}X}
    \caption{Comparison of Path Planning Algorithms} 
    \label{tab:algorithms_comparison}
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{@{} l l L L @{}} 
    \toprule
    \textbf{Category} & \textbf{Algorithm} & \textbf{Advantages} & \textbf{Disadvantages} \\ 
    \midrule

    % --- SEARCH BASED ---
 
    \multirow{10}{*}{\textbf{Search-based}} & \textbf{A*} & 
    % [nosep] removes all vertical space, [leftmargin=*] aligns bullets to text
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet] 
        \item Guarantees shortest path if heuristic is admissible.
        \item Reduces the search space compared to Dijkstra.
    \end{itemize} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item High computational cost in large/continuous spaces.
        \item Needs accurate heuristics.
        \item It may not be scalable for dynamic environments.
    \end{itemize} \\ 
    \addlinespace[4pt] % Adds a tiny gap between rows instead of a line
    
    & \textbf{Dijkstra} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Finds optimal path.
        \item Works with any weighted graph.
        \item Simple to implement.
    \end{itemize} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Computationally expensive.
        \item Impractical for real-time UAVs.
    \end{itemize} \\ 
    \midrule

    % --- SAMPLING BASED ---
    \multirow{10}{*}{\textbf{Sampling-based}} & \textbf{RRT} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Fast exploration of high-dim spaces.
        \item Good for unknown maps.
    \end{itemize} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item No guarantee of optimal path.
        \item Paths are often jagged and require smoothing.
    \end{itemize} \\ 
    \addlinespace[4pt]
    
    & \textbf{RRT*} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Paths improve over time.
        \item Handles complex spaces.
        \item Retains the exploration ability of RRT.
    \end{itemize} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Computational cost is higher than RRT.
        \item Slower convergence.
    \end{itemize} \\ 
    \midrule

    % --- AI BASED ---
    \multirow{10}{*}{\textbf{AI-based}} & \textbf{SAC} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Learns adaptive policies.
        \item Handles continuous control.
        \item Robust in dynamic, uncertain environments.
        \item No explicit map needed.
    \end{itemize} & 
    \begin{itemize}[nosep, leftmargin=*, label=\textbullet]
        \item Requires large training data and simulation
        \item Reward design is non-trivial.
        \item No hard optimality guarantee.
        \item Complex to implement
    \end{itemize} \\ 
    \bottomrule
    \end{tabularx}
\end{table}

\paragraph{Decomposition-based algorithms} ~\\
One of the primary reasons for using cell decomposition methods in our project
is to cover fragmented areas in the vegetation environment. In cell
decomposition methods, the robot's free space is subdivided into several
regions, called cells \cite{gasparetto2015path}. The decomposition process can
be explained as follows: subdivision of space into numbered polygons, creation
of a connectivity graph, identification of regions to be crossed, and generation
of the path.  The figure \ref{fig:cell_decomposition} shows the detailed steps
of the exact cell decomposition process. In our system, we utilize exact cell
decomposition, specifically boustrophedon decomposition, for coverage path
planning. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/cell_decomposition.png}
    \caption{Exact Cell Decomposition Process}
    \cite{choset2005principles}
    \label{fig:cell_decomposition}
\end{figure}
The boustrophedon cellular decomposition is a common algorithm that breaks down
the robot’s free space into cells and covers each cell with a motion of back and
forth \cite{choset2000coverage}. Compared to spiral patterns, which cause the UAV to constantly learn,
resulting in an unstable antenna, using boustrophedon keeps the drone stable
while covering the area. Similarly, in wavefront algorithms, having zigzag
patterns often provides a jagged path, whereas boustrophedon provides the
kinematic feasibility. The detailed implementation of the boustrophedon
algorithm is discussed in the methodology section.


\subsection{GNSS-R}
The concept of using GNSS-R for remote sensing was first proposed in 1993, where
the correlation between direct and scattered GNSS signals were proven promising
for the application of ocean altimetry \cite{MartnNeira1993APR}. Today, apart
from the originally proposed application, GNSS-R techniques has been extensively
used for Earth observations and land information retrieval, including ocean
salinity, soil moisture, and ice thickness \cite{RodriguezAlvarez2011}. However,
research on vegetation parameter retrieval remains limited to preliminary
analyses \cite{Jia2018}. 

In a previous study, the capabilities of monitoring vegetation through GNSS-R
were demonstrated through a theoretical simulation of GNSS scattering
characteristics \cite{Ferrazzoli2011}. Furthermore, this study revealed that the
sensitivity of GNSS-R signals can be influenced by incidence angle, soil
parameters, and tree size, while signals with lower elevation angles and RL
polarization (transmission of right-hand circular polarization (RHCP) signal,
reception of left-hand circular polarization (LHCP) signal) appeared to be ideal
for forest monitoring \cite{Ferrazzoli2011}. In another study, the polarization
scattering properties of GNSS-R signals in forest canopies were modeled, where
simulations revealed that tree trunk scattering effects would dominate total
scattering response, while satellite azimuth angles are significantly correlated
to the signal polarization \cite{Wu2014}. 

Aside from simulated approaches, empirical or semi-empirical studies regarding
GNSS-R remote sensing of vegetation were also reviewed. In a study by Yueh et
al., GNSS-R data onboard the Cyclone GNSS (CYGNSS) satellite were analyzed
against the vegetation water content estimated through the satellite-derived
Normalized Difference Vegetation Index (NDVI) \cite{Yueh2022}. The results
demonstrated near-linear relationship between vegetation water content and
GNSS-R signal attenuation, while the CYGNSS data also suggested the existence of
volume scattering within complex forest components \cite{Yueh2022}. Similarly,
another study analyzing GNSS-R data from TechDemoSat-1 demonstrated reduced
sensitivity to soil moisture retrieval due to vegetation attenuation, which
could be effectively compensated using NDVI data, indicating a correlation
between signal attenuation and vegetation water content \cite{Camps2016}.
Additionally, the interference pattern between direct and reflected GNSS signals
from the Earth's surface was used in estimating vegetation height and land
topography \cite{RodriguezAlvarez2011}. Through using a Soil Moisture
Interference-pattern GNSS Observations at L-band (SMIGOL) reflectometer, the
instantaneous power of the direct and reflected signals were coherently added,
in which the resulting power oscillations present notches that are correlated to
vegetation layer thickness and the reflection geometry (Figure \ref{SMIGOL})
\cite{RodriguezAlvarez2011}. As a result, an RMSE of 3-5 cm in estimating the
height of vegetation with simple geometrical structures (barley and wheat) could
be achieved \cite{RodriguezAlvarez2011}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/SMIGOL_reflectometer.png}
        \caption{SMIGOL Reflectometer Setup}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/SMIGOL_interference.png}
        \caption{Interference Notch}
    \end{subfigure}
    \caption{Setup and interference notch examples of the SMIGOL reflectometer
    \cite{RodriguezAlvarez2011}.}
    \label{SMIGOL}
\end{figure}

Yet, despite the effort in prior studies, the use of GNSS-R techniques in
vegetation remote sensing and monitoring is far from developed. Most
importantly, existing research relies heavily on space-borne or static
ground-based platforms, while the use of high spatial resolution and flexibility
platforms - such as an UAV - remains underexplored. Therefore, it is critical to
develop a systematic framework for an UAV-based GNSS-R system, in which
correlations between GNSS-R signal features and vegetation condition parameters
are comprehensively assessed and modeled. Additionally, with adequate dataset size,
the GNSS-R framework could be complemented by machine learning-based modelling
approaches, further enhancing the system robustness.


\subsection{Machine Learning}
A part of this project will be to develop a deep learning based model that can
process collected GNSS-R data and address two main topics: (1) Classification of
the received signals into direct and reflected signals, and (2) Prediction of
vegetation parameters from the raw GNSS data. The goal will be to develop and
test tree based and neural network based models that take GNSS-R features, both
publicly available and collected by our UAV system, as inputs and output the
desired results. The model training will be optimized for highest computational
efficiency and accuracy on python based PyTorch framework. Past research has
demonstrated that GNSS-R can sense vegetation as the reflected signal power and
modulation depends on vegetation cover and moisture. For example, a study by
Small et al. (2010) and others found that the amplitude of the reflected GNSS
signal to Noise Ratio (SNR) oscillations over crops correlated linearly with
Vegetation Water Content (VWC) \cite{small2010sensing}. A study by Chen et al.
(2024) recently derived a new GNSS-R vegetation index and used Artificial Neural
Networks (ANN) to retrieve VWX from spaceborne data, achieve a correlation of
approximately 0.94 with reference measurements \cite{chen2024new}. Other studies
have trained deep learning models on GNSS-R features. One study trained an ANN on
Delay Doppler Maps (DDMs), SNR, signal incidence angle and geographic data to
predict the Above Ground Biomass (AGB) \cite{pilikos2024biomass}. Prior GNSS-R
vegetation retrieval efforts have focused on parameters like biomass, soil
moisture, and VWC. For example. SNR that has been reflected from wheat was used
to estimate the height and water content of wheat, which signifies that strong
vegetation attenuates the reflected signals \cite{zhang2017use}. Polarimetric
GNSS features have also been used in previous studies. Cross polarization ratios
and co polarized reflection coefficients were found to be sensitive to
vegetation biomass and moisture \cite{wu2021recent}. 

Most existing studies have used spaceborne or terrestrial platforms, while UAV
based GNSS-r remains underexplored. The few existing UAV based works demonstrate
feasibility but do not address vegetation mapping
\cite{wu2021recent,chen2024new}. Moreover, classification of difference signals
is not a well studied topic. However, existing study suggests that GNSS-R
features can inform vegetation parameters \cite{small2010sensing}. But the gap
is clear: GNSS-R with UAVs for vegetation mapping and signal classification has
not been demonstrated. This project will bridge the gap by developing ML models
tailored for UAV scale GNSS-R data and the specific tasks of vegetation
detection and parameters estimation. 

\subsection{LiDAR SLAM}
In recent years, autonomous robots have been widely implemented to carry out
repetitive tasks or take part in site surveys to ensure human safety or even
reduce operational cost. To carry out remote sensing via UAV, mapping is
essential to allow robots to create a spatial understanding of the site in order
to create efficient flight paths as well as avoid obstacles. 

The common way for remote sensing robots to carry out mapping is by using
sensors such as camera or LiDAR to create an odometry. This allows the robots to
estimate their position and movement over time by measuring the changes in each
frame of the sensors. In practise, the common approaches to frame-to-frame
odometry are visual odometry or LiDAR odometry. Visual odometry estimates motion
by tracking distinct features in each image frame through gradients and textures
while LiDAR odometry uses laser range measurements to scan the environment.
Since our project focuses on investigating forests and dense vegetation, LiDAR
odometry is preferred as it is not affected by visually repetitive scenes and
varying lighting conditions that cause instability in camera-based methods. 

\subsubsection{LiDAR Odometry} 
The accuracy of LiDAR odometry largely depends on how well point clouds are
aligned and it generally categorized into point-based and feature-based
approaches. In terms of point-based point cloud registration, iterative closest
point (ICP) is considered one of the most well-known pointwise LiDAR odometry
methods. It is a simple, modular algorithm that aligns two consecutive point
clouds by establishing point-to-point correspondences and estimating the
transformation that minimizes their distance error \cite{besl1992method}.
However, a key limitation of ICP is its high computational cost when registering
dense point clouds. Its accuracy relies strongly on the quality of the initial
alignment due to the nonconvex nature of the optimization, and the presence of
outliers from moving objects can further complicate the process by adding extra
nonconvexities \cite{maron2016point}. To counter the limitation of ICP, variants
such as Generalized ICP (G-ICP), FASTG-ICP, Voxelized G-ICP (VGICP) have been
developed to improve its computational efficiency \cite{segal2009generalized,
huang2022point}. 

On the other hand, feature-based LiDAR odometry methods utilize a set of
representative features from the point clouds to estimate the transformation
\cite{rusu2009fast, salti2014shot}. Fast Point Feature Histograms (FPFH) is a
feature-based point cloud matching method that looks at the distinct shape in
point clouds (surface normals) and uses multidimensional histogram to label the
geometric properties of each point \cite{rusu2009fast}. As FPFH is able to
remain consistent even if the sensor is rotated or moved, it creates a robust
tool for finding matches between different scans and can be used as an initial
alignment of point clouds before handling it to other types of processors
\cite{wu2020iterative}. However, creating complex descriptors for every point is
computationally demanding which is not favorable for real time requirements
\cite{atik2021lokal}. 

In the meantime, LOAM uses more efficient way to carry out feature-based LiDAR
odometry. It is done by extracting features and edges within the point clouds
based on the smoothness of a small region near a given feature point
\cite{huang2022point}. Unlike conventional ICP methods that compare consecutive
scans directly, LOAM matches these extracted features against a continuously
updated map of dense edge features, creating a scan-to-map correspondence
system. Since feature-based methods process only a small fraction of the
original point cloud data rather than all raw points, they require substantially
less computational power for matching compared to G-ICP and NDT optimization
processes. Feature-based methods rely less on the accuracy of the
initial pose estimate than point-wise methods, but this reduced dependence often
results in noticeable drift along the altitude axis. \cite{koide2021voxelized,
shan2018lego}. In this case, LOAM variants such as Lightweight and ground
optimized (LeGO)-LOAM \cite{shan2018lego} are proposed to counter the drifting
in altitude direction.

\subsubsection{Simultaneous Localization and Mapping (SLAM)}
While odometry provides the foundation for estimating local motion through
consecutive point cloud alignment, it lacks mechanisms such as loop closure and
mapping, making it prone to drift over time \cite{liu2019optimized,
thrun2000probabilistic}. Simultaneous Localization and Mapping (SLAM) extends
odometry by incorporating global optimization techniques, enabling both drift
correction and the construction of consistent long-term maps. Moreover, SLAM
frameworks naturally support sensor fusion, allowing data from multiple
modalities such as GNSS and LiDAR to be combined, which further enhances
localization robustness and mapping accuracy \cite{xu2022review, chang2019gnss}.

In terms of optimization strategies, SLAM methods can generally be divided into
two categories: filtering-based approaches and pose graph optimization
approaches. Filtering methods, such as the Extended Kalman Filter (EKF) or
particle filters, update the robot’s state incrementally as new sensor data
arrives, making them suitable for real-time applications but often less accurate
over long trajectories. In contrast, pose graph optimization such as GraphSLAM
treats past poses and sensor measurements as nodes and constraints in a graph,
allowing global corrections through nonlinear optimization. 

EKF SLAM is one of the fundamental SLAM algorithms within the filtering-based
approach. It estimates the system state through an iterative cycle of
prediction, observation, and correction, effectively functioning as a
specialized form of Bayesian filtering \cite{paz2007ekf}. Unlike pure
odometry-based methods, EKF SLAM incorporates implicit loop closure, where it
matches current observations to known landmarks. The filter then automatically
updates the estimated position and all correlated landmarks in the state vector
to reduce accumulated drift. While this mechanism improves consistency, it is
less flexible than the explicit loop closure in graph-based SLAM, which performs
global optimization \cite{khan2021comparative}. 

Although EKF SLAM is known to be used in real-time SLAM applications due to its
low-latency estimates, it faces quadratic complexity when updating the
covariance matrix with new landmarks, making it less scalable for large
environments but still efficient for small to medium-scale maps
\cite{gil2015occupancy}. Additionally, EKF SLAM integrates data from multiple
sensors by weighting each input according to its uncertainty, which helps manage
cross-sensor correlations \cite{oh2010symmetrical}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/EKF SLAM.png}
    \caption{The figure depicts a spring network analogy to visualize how EKF
SLAM works. The landmarks are connected by springs describing correlations
between landmarks. As the robot pass through the landmark more than once, this
correlations increase which makes the red spring thicker
\cite{stachniss2016simultaneous}.}
\end{figure}

GraphSLAM on the other hand is an algorithm designed for the offline SLAM
problem, where pose graph optimization is employed to refine the robot’s
trajectory by minimizing errors across a graph of poses and constraints. Similar
to EKF SLAM, GraphSLAM uses Bayes’ rule for probabilistic inference and similar
methods for building motion and measurement models [23]. The key difference
between GraphSLAM and EKF SLAM is that GraphSLAM requires to accumulates all
data first before process everything together. This makes it more
computationally heavy as solving the resulting large optimization problem often
involves costly operations such as matrix inversion. To address this, GraphSLAM
applies iterative optimization methods like stochastic gradient descent [24],
which adaptively solve the non-linear optimization problem more efficiently,
while alternatives such as conjugate gradient methods can further improve
scalability [25]. 

Moreover, GraphSLAM explicitly incorporates loop closure.
Which means that when the robot revisits a previously mapped location, it
recognizes the place and adds new constraints linking the current pose to the
past one. This additional relation corrects accumulated drift and improves
global consistency [26]. Based on [25, 27], GraphSLAM can refer to more than 100
million environments features, as each robot pose only connects to adjacent
poses and each feature only connects to poses where it was observed without
connecting to each other. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/pgo.png}
    \caption{Visualization of GraphSLAM with loop closure}
\end{figure}

Although GraphSLAM is more robust than EKF-SLAM and both approaches have
demonstrated strong performance in urban and structured environments [4, 17],
single-sensor SLAM systems (e.g. LiDAR-only) remain vulnerable to drift because
their measurements are purely relative, allowing errors to accumulate over time.
While recent work such as \cite{mailka2024efficient}has integrated additional
sensors to improve SLAM accuracy, their applicability to UAV-based forest
monitoring remains limited due to canopy interference, sparse LiDAR features,
and computational trade-offs. This project investigates how such SLAM methods
can be adapted and deployed for real-time forestry applications.


% --- Methodology ---
\section{Methodology}
In this section, a detailed analysis of the system architecture is presented,
with a focus on the UAV platform for data collection. The system is divided into
four key workflows: path planning, GNSS-R data processing, Machine Learning, and
LiDAR SLAM. A detailed overview of the processes, mechanisms, and derivations of
each workflow is presented in the methodology section. 


\subsection{UAV Platform}
In this project, one of the primary objectives is to utilize a UAV to collect
GNSS-R data and conduct subsequent post-processing. This data collection system
is built on a Holybro X650 drone frame, integrated with a Pixhawk 6C flight
controller for navigation and a SiK telemetry radio for ground station
communication. For GNSS-R measurements, the setup features a dual-antenna
configuration consisting of zenith and nadir antennas. An onboard Jetson Orin
Nano serves as the primary computer for processing data from the Livox LiDAR and
GNSS receivers. The system is further supported by a GPS module and a
receiver for the remote controller, enabling both manual and autonomous flight
operations. The figure \ref{UAV Platform Diagram} demonstrates the onboard setup of the UAV and
sensors. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/UAV_Setup.png}
    \caption{UAV Platform and Onboard Setup}
    \label{UAV Platform Diagram}
\end{figure}


\subsection{System Architecture}

Our system architecture comprises four key components: path planning, GNSS-R,
machine learning, and LiDAR SLAM. First, we will conduct path planning using
waypoint-based navigation and coverage path planning to navigate the UAV in
desired vegetation environments. Then, we will send the planned trajectory to
the ground control station for execution of the mission. Finally, during mission
execution, we will collect raw GNSS data and LiDAR data for SLAM. 

In the GNSS pipeline, we will conduct GNSS measurement extraction and Fresnel
Zone analysis to obtain the GNSS measurement level model. Simultaneously, we
will send the raw data to the machine learning pipeline to perform data
preprocessing and signal classification for open sky, UAV hovering over bare
ground, and UAV hovering over trees. Once the classification is complete, we
will proceed with vegetation feature regression. Ultimately, the system
integrates the GNSS-R modeling with vegetation feature regression, enabling the
precise retrieval of the desired vegetation parameters. 

In the LIDAR SLAM pipeline, we will first synchronize the LiDAR data with the
timestamps from the flight controller (Pixhawk). Once synchronized, we obtain
the LiDAR data and corresponding GPS data. We then send these to our front-end
LiDAR Odometry and subsequently to the back-end pose graph optimization. This
data is used to cross-validate the vegetation parameters retrieved by the GNSS-R
and Machine Learning pipelines. 
A comprehensive flow of our system architecture
is shown in Figure \ref{System Architecture}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/System_Architecture.png}
    \caption{System Architecture}
    \label{System Architecture}
\end{figure}


\subsection{Path Planning}
As mentioned in the system architecture, path planning focuses on
two methods: Waypoint-Based Navigation and Coverage Path Planning. These methods
are crucial for observing dense forests, such as hovering over trees and
surveying the dense canopy to obtain dynamic data for LiDAR SLAM. Below are the
detailed methodologies for each path planning method.

\subsubsection{Waypoint-based navigation}

Waypoint-based navigation can be achieved through various approaches, such as
search-based algorithms, graph-based algorithms, and Artificial
Intelligence-based algorithms. Since the UAV is designed to hover over dense
vegetation environments and identify potential Fresnel Zones, it primarily needs
to fly outdoors, at high-altitude and open spaces. To determine the optimal
algorithm for building an intelligent UAV system, a baseline
comparison was conducted among the Dijkstra algorithm, the A* algorithm, and the RRT
algorithm.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3} % Adds a little vertical padding for readability
    % 'l' = left align, 'X' = auto-wrap column
    \begin{tabularx}{\textwidth}{@{} l l X X @{}} 
    \toprule
    \textbf{Category} & \textbf{Algorithm} & \textbf{Advantages} & \textbf{Disadvantages} \\ 
    \midrule

    % SEARCH BASED SECTION
    \multirow{10}{*}{\textbf{Search-based}} & \textbf{A*} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Shortest path is guaranteed for a given heuristic map.
        \item Reduces computation power and search space compared to Dijkstra.
    \end{itemize} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item High computational cost in large or continuous spaces.
        \item Poor scalability in dynamic environments.
    \end{itemize} \\ 
    \cmidrule{2-4}
    
    & \textbf{Dijkstra} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Designed to find the optimal path. 
        \item Simple to implement.
    \end{itemize} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Computationally expensive.
        \item Impractical for real-time UAV planning.
    \end{itemize} \\ 
    \midrule

    % SAMPLING BASED SECTION
    \textbf{Sampling-based} & \textbf{RRT} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Fast exploration of high-dimensional spaces.
        \item Perform well in unknown or partially known maps.
    \end{itemize} & 
    \begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
        \item Does not guarantee the shortest/optimal path.
        \item Paths are often jagged and require smoothing.
    \end{itemize} \\ 
    \bottomrule
    \end{tabularx}
    \caption{Comparison of Path Planning Algorithms}
    \label{tab:algorithms}
\end{table}

As a result, for a given occupancy grid map, the comparison shows that the A*
algorithm is suitable for offline path planning. Therefore, the system is
designed to perform offline path planning using the A* algorithm. On the other
hand, RRT performs well in dynamic, unknown environments and is suitable for
online path planning. 

\paragraph{A* Algorithm Implementation} ~\\
The A* algorithm is a heuristic, search-based algorithm that calculates the
shortest path from a starting point to a goal. The algorithm first divides the
map into nodes, where each node represents a possible position for the robot to
move. These positions depend on the grid architecture, such as 4-way or 8-way
connected grids. In addition to the conventional grid architecture, novel grid
mapping approaches such as NavMeshes \cite{putz20163d} are widely used in 3D simulation software.
These mapping techniques connect the locations of the movable agent to the
surface (mesh) and store the surfaces as convex polygons.  

In this project, the focus will be on regular grids that connect nodes across 26
directions in 3-D space. Each node will be assigned a cost based on the starting
and ending nodes of a given path. A 2-D path illustration of an A* algorithm is
shown in Figure \ref{fig:astar}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/A-starmap.jpg}
    \caption{Map Highlighting Shortest Path from Node A to Node K}
    \label{fig:astar}
\end{figure}

\paragraph{A* Algorithm Grid Definition} ~\\


This system utilizes a grid map defined in 3D space, featuring 26-connectivity,
which allows for movement in all straight and diagonal directions. However, to
simplify the initial validation, the algorithm is currently evaluated using a 2D
grid with 8-connectivity. The coordinates of the given map were first converted into
grid node indexes using the \texttt{coord\_to\_index} function. The coordinates
can be defined as world coordinates or image coordinates for a given map image.
An illustration of image coordinates is shown in Figure \ref{fig:image_coordinates}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Included_Images/image_coordinates.png}
    \caption{Image Coordinates System}
    \label{fig:image_coordinates}
\end{figure}
Below is the psuedocode for the \texttt{coord\_to\_index} function:
\begin{singlespace}
\begin{algorithm}[H]
\caption{Coordinate to Grid Index Conversion}
\label{alg:coord_to_index}
\begin{algorithmic}[1]
    \Require $pos$: Real-world coordinate vector $(x, y, z)$
    \Ensure $index$: Integer grid indices $(i, j, k)$
    
    \Function{CoordToIndex}{$pos$}
        \State \Comment{Calculate position relative to the map center}
        \State $pos_{relative} \gets pos - pos_{center}$
        
        \State \Comment{Scale by resolution and round to nearest integer}
        \State $index \gets \text{round}(pos_{relative} \cdot step_{inv})$
        
        \State \Comment{Apply the center index offset}
        \State $index \gets index + index_{center}$
        
        \State \Return $index$
    \EndFunction
\end{algorithmic}
\end{algorithm}
\end{singlespace}
\paragraph{A* Algorithm Cost Function Implementation} ~\\
The cost function $f(n)$ for each node $n$ is calculated as follows:
\begin{equation}
    f(n) = g(n) + h(n)  
\end{equation}
where:
\begin{itemize}
    \item $g(n)$ is the cost from the starting node to the current node $n$.
    \item $h(n)$ is the estimated heuristic cost from node $n$ to the goal node.
    \item $f(n)$ is the total estimated cost of the shortest path node $n$.
\end{itemize}
In this project, the cost $g(n)$ (the cost from starting node to end node) is calculated as the Euclidean distance
between the starting node and the current node $n$.
The Euclidean distance can be calculated as follows:
\begin{equation}
    g(n) = \sqrt{(x_n - x_{start})^2 + (y_n - y_{start})^2 + (z_n - z_{start})^2}  
\end{equation}
where $(x_n, y_n, z_n)$ are the coordinates of the current node $n$ and
$(x_{start}, y_{start}, z_{start})$ are the coordinates of the starting node.

The heuristic cost $h(n)$ (the estimated cost from current node $n$ to goal
node) is implemented using the diagonal heuristic function. The function has five key steps as follows:

\begin{enumerate}
    \item Calculates the distance differences in each dimension ($dx$, $dy$, $dz$) between the current node and the goal node.
    \item Determines the direction with the minimum change in distance.
    \item The heuristic cost starts at zero and will increment by $\sqrt{3} \cdot \min(dx, dy, dz)$ to account for diagonal movement in 3D space.
    \item The function checks which dimensions still have remaining distance to cover. If one dimension has been fully covered (i.e., $dx$, $dy$, or $dz$ equals zero), it calculates the remaining cost using 2D diagonal movement and straight-line movement.
    \item If the three dimensions still have remaining distance, it calculates the cost using a combination of 2D diagonal movement and straight-line movement for the remaining distances.
\end{enumerate}

Below is an example of calculating heuristic cost in 3-dimensional space:

\noindent Go from $(0,0,0)$ to $(5,3,2)$. \\
Initial differences: $dx=5, dy=3, dz=2$.

\begin{itemize}
    \item \textbf{Step 1: 3D diagonal movement} \\
    Calculate the shared 3D distance:
    \[ diag_{3d} = \min(5, 3, 2) = 2 \]
    Cost for 2 steps diagonally in 3D:
    \[ \text{Cost} = \sqrt{3} \times 2 \approx 3.464 \]
    \textit{Remaining:} $dx=3, dy=1, dz=0$.

    \item \textbf{Step 2: Handle remaining} (since $dz=0$) \\
    Calculate 2D diagonal in X-Y ($\min(3,1) = 1$ step):
    \[ \text{Cost} = \sqrt{2} \times 1 \approx 1.414 \]
    Calculate Straight line ($|3-1| = 2$ steps):
    \[ \text{Cost} = 2.0 \]
\end{itemize}

\noindent \textbf{Total estimated cost:}
\[ 3.464 + 1.414 + 2.0 = 6.878 \]

Below is the psuedocode for the diagonal heuristic function:
\begin{singlespace}
\begin{algorithm}[H]
\caption{Diagonal Heuristic (3D)}
\label{alg:diagonal_heuristic}
\begin{algorithmic}[1] % The [1] turns on line numbering
    \Require $node_1, node_2$: Current and Goal GridNodes
    \Ensure $h$: Estimated cost to goal
    
    \Function{DiagonalHeuristic}{$node_1, node_2$}
        \State $dx \gets |node_1.x - node_2.x|$
        \State $dy \gets |node_1.y - node_2.y|$
        \State $dz \gets |node_1.z - node_2.z|$
        
        \State \Comment{Find the minimum for 3-D diagonal movement}
        \State $diag_{3d} \gets \min(dx, dy, dz)$
        \State $dx \gets dx - diag_{3d}$
        \State $dy \gets dy - diag_{3d}$
        \State $dz \gets dz - diag_{3d}$
        
        \State $h \gets \sqrt{3} \cdot diag_{3d}$
        
        \State \Comment{Handle remaining 2-D movement}
        \If{$dx = 0$}
            \State $h \gets h + \sqrt{2} \cdot \min(dy, dz) + |dy - dz|$
        \ElsIf{$dy = 0$}
            \State $h \gets h + \sqrt{2} \cdot \min(dx, dz) + |dx - dz|$
        \ElsIf{$dz = 0$}
            \State $h \gets h + \sqrt{2} \cdot \min(dx, dy) + |dx - dy|$
        \Else
            \State \Comment{General case (if dimensions remain)}
            \State $diag_{2d} \gets \min(dx, dy, dz)$
            \State $h \gets h + \sqrt{2} \cdot diag_{2d}$
            \State $remaining \gets (dx - diag_{2d}) + (dy - diag_{2d}) + (dz - diag_{2d})$
            \State $h \gets h + remaining$
        \EndIf
        
        \State \Return $h$
    \EndFunction
\end{algorithmic}
\end{algorithm} 
\end{singlespace}

\paragraph{A* Algorithm Core Logic Implementation} ~\\
The core logic of the A* algorithm involves determining the neighboring nodes,
checking the occupancy of the neighboring nodes, calculating their costs, and
adding the nodes to a separate list that deviates from the list of unexplored
nodes. The algorithm is also responsible for updating its list by adding or
removing neighboring nodes if a better path is found. The core logic works
according to the following steps:

\begin{enumerate}
    \item Initialize the grid, convert the start and end coordinates to grid
    indices, and adjust the indices if the start and end nodes are inside
    obstacles.
    \item Initialize the heapfile to store the exploring nodes, check the node with lowest $f(n)$, skip if already explored.
    \item Move to the next node, check if the current node is the goal node. If so, backtrack and reconstruct the path.
    \item If not, check the neighboring nodes, calculate their tentative costs, and add them to the open set if they are not already explored.
    \item If a node is already in the open set but a better path is found,
    update the parent node, adjust the $g(n)$ to the lowest cost, and add to
    the queue again with  new priority.
    \item Repeat steps 2-5 until the goal node is reached or no path is found.
\end{enumerate}

Below is the flow chart illustrating the core logic of A* algorithm:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Included_Images/Astar_flowchart.png}
    \caption{A* Algorithm Core Logic Flowchart}
    \label{fig:astar_flowchart}
\end{figure}

\paragraph{Evaluating the A* Algorithm Performance} ~\\
To evaluate the algorithm's performance, a series of test runs conducted
using vegetation density maps in different categories. The maps were obtained
using QGround Control \cite{QGroundControl_PlanView} software from satellite imagery and then imported into an
image processing pipeline. The pipeline can be described in the following steps:
\begin{enumerate}
    \item Convert the satellite imagery to grayscale images.
    \item Use HSV (Hue, Saturation, Value) color space conversion and RGB
    Conversion to identify greenery, and NDVI Index to measure vegetation
    health.
    \item The images are categorized into three vegetation types: Dense tree
    canopy, grassland and short vegetation, and cropland and agricultural fields.
    \item Apply edge detection such as canny detection \cite{ding2001canny} to identify buildings and non vegetation areas.
    \item Plot the series of processed images as occupancy grid maps.
\end{enumerate}

Figure \ref{fig:dense_canopy_map} is an example of processed occupancy grid maps for a dense canopy vegetation:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/densecanopy_map.png}
    \caption{Processed Occupancy Grid Map for Dense Canopy Vegetation}
    \label{fig:dense_canopy_map}
\end{figure}

\paragraph{Normalized Difference Vegetation Index} ~\\
Normalized Difference Vegetation Index (NDVI) is a quantified vegetation index that measures
the difference between near-infrared (which vegetation strongly reflects) and
red light (which vegetation absorbs) \cite{GISGeography_NDVI}. Below is the equation to calculate NDVI:
\begin{equation}
    \text{NDVI} = \frac{\text{NIR} - \text{Red}}{\text{NIR} + \text{Red}}
    \label{eq:ndvi}
\end{equation}
The Normalized Difference Vegetation Index is calculated using Equation
\ref{eq:ndvi}, where \text{NIR} represents the near-infrared band and \text{Red}
represents the red visible light band.

If a vegetation is healthy it reflects more near-infrared and green light
(chlorophyll), while absorbing more red and blue lights.

A detailed overview of the A* algorithm performance evaluation is presented in
the results and discussion section. 

\subsubsection{Coverage Path Planning}
Coverage Path Planning (CPP) is essential for surveying in dense vegetation
areas and for obtaining dynamic data for LiDAR SLAM. In this research,
Boustrophedon Cell Decomposition was implemented as the CPP algorithm. The
decomposition algorithm works by dividing the map into cells based on the
polygonal shape of the map and the obstacles present. After decomposing the map
into cells, the algorithm generates a sweeping path in each cell based on the
given parameters (refer to Figure \ref{fig:boustrophedon}). This sweeping
technique in all the cells ensures that the entire area is covered. Below is a
detailed explanation of how the Boustrophedon Cell Decomposition is implemented
using the framework developed by the Autonomous Systems Lab at ETH Zurich
\cite{B_hnemann_2021}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Included_Images/boustrophedon_path.png}
    \caption{Boustrophedon Cell Decomposition Path Planning Example}
    \cite{Choset-1997-16422}
    \label{fig:boustrophedon}
\end{figure}

\paragraph{Boustrophedon Cell Decomposition Implementation} ~\\
The algorithm decomposite the map (polygonal area) into cells based on three key events:
\begin{itemize} 
    \item \textbf{In Event}: When the sweep line encounters the start of an
    obstacle, the current cell is divided into two new cells. The algorithm is
    defined by an if-else loop that compares the vertices of the obstacle and
    the sweep line intersection. In the IN event, the intersecting vertices move
    in the same direction as the sweep line.

    The if else logic in an IN event can be interpreted as follows:
\[
    \text{If } \texttt{target\_upper} > \texttt{source\_upper} \text{ and } \texttt{target\_lower} < \texttt{source\_lower}
\]
Figure \ref{fig:in_event} illustrates an example of an IN event:
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\linewidth]{Included_Images/in_event.png}
        \caption{Boustrophedon Cell Decomposition IN Event Example}
        \label{fig:in_event}
          \cite{Choset-1997-16422}
    \end{figure}

    \item \textbf{Middle Event}: When the sweep line passes through the obstacle
    and updates it cell status.
    \item \textbf{Out Event}: The sweep line encounters the end of an obstacle,
    and two cells merge into one cell. The algorithm is defined by an if-else
    loop that compares the points of the obstacle's vertices and the
    intersection of the sweep line. In the OUT event, intersecting vertices move
    in the opposite direction of the sweep direction. The if else logic in an
    OUT event can be interpreted as follows:
\[
    \text{If } \texttt{target\_upper} < \texttt{source\_upper} \text{ and } \texttt{target\_lower} > \texttt{source\_lower}
\]
Figure \ref{fig:out_event} illustrates an example of an OUT event:
    \begin{figure}[H]   
        \centering
        \includegraphics[width=0.6\linewidth]{Included_Images/out_event.png}
        \caption{Boustrophedon Cell Decomposition OUT Event Example}    \cite{Choset-1997-16422}
        \label{fig:out_event}
        \end{figure}
\end{itemize}

After decomposing the map into cells, the algorithm will visit each cell using a
depth-first search (DFS). It will choose a random cell to visit first and then
visit the neighboring cells of that cell. It will also backpropagate through the
visited cells to ensure all cells are visited. When visiting each cell, the
algorithm generates a sweeping path based on the given parameters, such as the
sweeping direction and sweep distance. The sweeping path can be performed
horizontally or vertically. The angle of the sweep line can also be adjusted to
fit the edges of the obstacles. The sweep distance is defined as the distance
between each sweep line. Figure \ref{fig:sweep_parameters} illustrates sweeping
in each cell. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Included_Images/sweep_parameters.png}
    \caption{Boustrophedon Cell Decomposition Sweeping}
     \cite{Choset-1997-16422}
    \label{fig:sweep_parameters}
\end{figure}

The practical implications of the algorithm's use are shown in the experiment
setup section, and the detailed results of the Boustrophedon Cell Decomposition
algorithm are discussed in the results and discussion section.


\subsection{GNSS-R}
The GNSS-R framework takes in raw GNSS data collected from the onboard
receivers, extracts raw GNSS measurements, and analyzes the characteristics of
such measurements with respect to the ground features that falls within the
Fresnel zones. Ultimately, the correlation between GNSS measurement features and
vegetation properties would be systematically summarized and modeled. The method
used for retrieving GNSS measurements and the Fresnel zone from collected raw data
is briefly outlined in the following two sections, while the MATLAB code for the 
GNSS-R framework is presented in the Appendix.

\subsubsection{GNSS Measurement Extraction}
Two raw data sources are necessary for the extraction of key GNSS measurements
used in this project: the raw binary GNSS data and the Pixhawk controller log.
First, the binary GNSS data is converted into readable observation files using
RTKCONV \cite{takasu2009rtklib}, and together with the satellite ephemeris data
downloaded from \url{https://cddis.nasa.gov/archive/gnss/data/}, measurements
such as the satellite elevation angle, pseudorange, and carrier to noise ratio
could be extracted using the MatRTKLIB library \cite{taroz:matrtklib}. From the
Pixhawk controller log, the UAV position, which is a filtered positioning result
from various onboard sensors, is obtained and regarded as the ground truth
assuming that the positioning error is small. Ultimately, two major GNSS
measurement are taken for further analysis: carrier to noise ratio ($C/N_0$) and
pseudorange error. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/GNSS_Measurement_Flow.png}
    \caption{Flowchart of extracting GNSS measurements from raw data.}
\end{figure}

\paragraph{Carrier to Noise Ratio} ~\\
Carrier to noise ratio refers to the power of received carrier signal relative
to the noise power per unit bandwidth, usually expressed in decibel-Hertz (dB-Hz)
\cite{joseph2010gnss}. $C/N_0$ can be readily obtained from the receiver, and it 
is calculated using the following equation:
\begin{equation}
    C/N_0 = C - (N - BW)
\end{equation}
where $C$ refers to the carrier power in dBm or dBW; $N$ refers to the noise power
in dBm or dBW; and $BW$ refers to the bandwidth of the observation in Hz.

\paragraph{Pseudorange Error} ~\\
In GNSS, pseudorange refers to the "apparent" distance between a satellite and a
receiver, which includes the true geometric range plus various biases and delays.
For the $i$th satellite, its pseudorange $\rho^i$ could be expressed as the following 
\cite{kaplan2017understanding}:
\begin{equation}
    \rho^i = R^i + \Delta t_r + \Delta t_s^i + I^i + T^i + \epsilon^i
\end{equation}
where $R^i$ refers to the true geometric distance between the satellite and the receiver;
$\Delta t_r$ refers to the receiver clock bias; $\Delta t_s^i$ refers to the satellite
clock bias; $I^i$ refers to the ionospheric delay; $T^i$ refers to the tropospheric 
delay; and $\epsilon^i$ refers to the pseudorange error, which is mainly
introduced through signal reflections. Among all the signals received at a given
instant, a satellite $m$ with the highest elevation angle is selected as the
master satellite, where the assumption that the master satellite is free of reflections
is applied \cite{hsu2018analysis}:
\begin{equation}
    \rho^m = R^m + \Delta t_r + \Delta t_s^m + I^m + T^m.
\end{equation}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/Pr_Error_Illustration.png}
    \caption{Schematic diagram illustrating satellite $i$ and satellite $m$.}
\end{figure}
Assuming $\Delta t_s^i$ and $\Delta t_s^m$ can be effectively removed by 
satellite-broadcasted parameters, $I^i$ and $I^m$ can be effectively removed by the
Klobuchar model, and $T^i$ and $T^m$ can be effectively removed by the Saastamoinen model,
the pseudorange equations can be approximated as \cite{kaplan2017understanding}:
\begin{equation}
    \rho^i = R^i + \Delta t_r + \epsilon^i,
\label{ith satellite pseudorange}
\end{equation}
\begin{equation}
    \rho^m = R^m + \Delta t_r.
\label{master satellite pseudorange}
\end{equation}
Finally, taking the difference between \eqref{ith satellite pseudorange} and 
\eqref{master satellite pseudorange} and rearranging the equation will result in the
pseudorange error:
\begin{equation}
    \epsilon^i = \rho^i - \rho^m - R^i + R^m.
\end{equation}

\subsubsection{Fresnel Zone Analysis}
Following the extraction of GNSS measurements observed onboard the UAV, the GNSS
signal would be particularly analyzed with respect to the 2-dimensional ground region 
that it is reflected from. Within this region, a specular reflection point could be found,
where the angle of incidence is equal to the angle of reflection. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/Fresnel_Zone_Illustration.png}
    \caption{Schematic diagram of GNSS-R signal scattering geometry.}
\end{figure}
The primary scattering area around the specular point in which reflected signals
arrive at the receiver in coherence is called the first Fresnel zone (FFZ). By
definition, it is an ellipse for which the signal phase change across is within
$\frac{\lambda}{2}$ radians, where $\lambda$ refers to the signal wavelength
\cite{Jia2018}. Given the incidence angle $\theta$ and the height of the
receiver $H$, the semi-major axis $a$ and the semi-minor axis $b$ of the FFZ can
be calculated as the following \cite{yu2021theory}: 
\begin{equation}
    a = \frac{\sqrt{\lambda H sin(\theta)+\frac{\lambda^2}{4}}}{(sin(\theta))^2},
\end{equation}
\begin{equation}
    b = \frac{\sqrt{\lambda H sin(\theta)+\frac{\lambda^2}{4}}}{sin(\theta)}.
\end{equation}
To calculate the location of the FFZ center, a local coordinate system is
defined. The origin is the receiver's ground projection, while the y-axis aligns
with the ground projection of the receiver-to-satellite vector. The z-axis
follows the local East-North-Up (ENU) "up" direction, and the x-axis completes
the right-handed system by remaining orthogonal to both x and y axes. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Included_Images/Fresnel_Zone_Coordinates.png}
    \caption{Schematic diagram of the local coordinate system defined.}
\end{figure}
In this coordinate system, the location of the FFZ center could be expressed as
$(x_c, y_c, z_c)$, where \cite{yu2021theory}:
\begin{equation}
    x_c = 0
\end{equation}
\begin{equation}
    y_c = (\frac{\lambda}{2}+H sin(\theta))\frac{cos(\theta)}{(sin(\theta))^2}
\end{equation}
\begin{equation}
    z_c = 0
\end{equation}
The flowchart for calculating the FFZ shape and location is outlined in the
figure below. Here, since the UAV true position is given in the global
coordinate of latitude, longitude, and height (LLH) in the Pixhawk log data, the
terrain height shall be subtracted to obtain the UAV height above ground, $H$.
This terrain height is obtained through the Hong Kong Digital Terrain Model
(\url{https://data.gov.hk/en-data/dataset/hk-landsd-openmap-5m-grid-dtm}), which
shows the topographical information in 5 meter by 5 meter grids with an accuracy of
$\pm$ 5 meters \cite{dataDigitalTerrain}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Included_Images/FFZ_Flow.png}
    \caption{Flowchart of calculating FFZ shape and location from collected data.}
\end{figure}

\subsection{Machine Learning}
The projects aim to utilize machine learning algorithms for two purposes:
(1) to classify the received signals into three classes: reflected from ground,
direct to down antenna and reflected from trees, and (2) to predict vegetation 
properties based on the features extracted from GNSS measurements.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Included_Images/Classes.png}
    \caption{Classes of signals to be classified.}
\end{figure}
So far, only the first part has been attempted. A supervised learning approach was takes, where a dataset was 
constructed from collected GNSS data with labels assigned based on the collection scenarios. 
The collected data was analyzed and features were then selected to train a classification model. 
The details of the feature analysis, dataset construction, feature selection and model training 
are outlined in the following sections.

\subsubsection{Feature Analysis}
Before constructing and training a classification model, the collected GNSS 
measurements were analyzed to identify potential features that could help
distinguish between reflected and direct signals. Four major features were
found to have different distributions between the two scenarios, which are UAV hovering 
over trees and UAV hovering over bare ground. These features are outlined below:
\begin{itemize}
    \item\textbf{Carrier to Noise Ratio (C/N0)}: From the figures below, It can 
    be observed that the $C/N_0$ values for signals reflected from trees are
    generally lower than those for signals getting reflected from the bare ground. 
    This indicated that vegetation attenuated the GNSS signal quality. The figures also 
    illustrate a wider spread of $C/N_0$ values for signals reflected from trees compared
    to those from bare ground, in which case the signal distribution is more 
    concentrated. 
    \begin{figure}[H]
        \centering
        \includegraphics[width= 1\textwidth]{Included_Images/CNR.png}
        \caption{Distribution of $C/N_0$ for signals reflected from trees and bare ground.}
    \end{figure}
    
    \item\textbf{Pseudorange}: Similar to $C/N_0$, the pseudorange values for signals
    reflected from trees display an increased pseudorange bias and variance, which produces a
    clear decision boundary for the classifier than that of bare ground signals.
    Some of the pseudorange values for signals reflected from trees are extremely high, which 
    might be due to the multipath effect caused by dense vegetation. The figures below
    illustrate the distribution of pseudorange values for signals reflected from trees
    and bare ground.
    \begin{figure}[H]
        \centering
        \includegraphics[width= 1\textwidth]{Included_Images/Pseudorange.png}
        \caption{Distribution of pseudorange for signals reflected from trees and bare ground.}
    \end{figure}
    
    \item\textbf{Doppler measurement}: Doppler measurement refers to the instantaneous
    shift in frequency of the received GNSS carrier signal due to the relative 
    velocity between the satellite and the receiver. As can be seen from the 
    figures below, the Doppler values for signal reflected from trees are clearly
    distinguishable from those reflected from bare ground. For the case of bare ground, 
    the Doppler values for up and down antenna are almost identical. However, for 
    the case of UAV hovering over vegetation, the Doppler distribution is more
    spread out, ranging from approximately -5000 dbHz to 4000 dbHz. This indicates
    that reflections caused by trees introduce additional frequency shifts in the received 
    signals. The figure below illustrates the distribution of Doppler values for signals
    reflected from trees and bare ground.
    \begin{figure}[H]
        \centering
        \includegraphics[width= 1\textwidth]{Included_Images/doppler.png}
        \caption{Distribution of Doppler for signals reflected from trees and bare ground.}
    \end{figure}
    
    \item\textbf{Carrier Phase}: Carrier phase refers to the precise range observables that are derived 
     from the accumulated phase cycles of the GNSS carrier wave. From the figure below, 
     it can be observed that the carrier phase values for signals reflected from trees
     exhibit 2.5 times broader carrier phase spread compared to those reflected from bare 
     ground, reflecting multipath induced phase delays versus the concentrated phase coherence of bare 
     ground reflections. This indicated that vegetation introduces additional phase 
     shifts in the received signals. The figure below illustrates the distribution of
     carrier phase values for signals reflected from trees and bare ground.
    \begin{figure}[H]
        \centering
        \includegraphics[width= 1\textwidth]{Included_Images/Carrier Phase.png}
        \caption{Distribution of carrier phase for signals reflected from trees and bare ground.}
    \end{figure}
\end{itemize}
\subsubsection{Dataset Construction}
Data was collected from multiple locations with different environmental settings, in order to have 
a diverse dataset that captures the three classes of signals. The scenarios include:
(1) Open Sky, (2) UAV hovering over bare ground, and (3) UAV hovering over trees.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/opensky.png}
        \caption{Open Sky}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.43\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/bareground.png}
        \caption{UAV hovering over bare ground}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/overtrees.png}
        \caption{UAV hovering over trees}
    \end{subfigure}
    \caption{Data collection scenarios.}
\end{figure}
The data was then labeled based on the collection scenarios, where data collected 
from open sky mostly contains direct signals, data collected from UAV hovering over bare ground 
mostly contains signals reflected from ground, and data collected from UAV hovering over trees
mostly contains signals reflected from trees.
\subsubsection{Model Selection}
For the classification task, a LightGBM model was selected due to its advantages
with native support for NaN values. This feature of the model is particularly
important for our case as raw GNSS data often contains missing values due to
signal blockages or receiver limitations. Cleaning the data by removing the NaN
values would lead to significant loss of data, as cleaning process would have to
be carried out either row wise of column wise. Replacing the NaN values with
zeros or mean values would introduce bias to the dataset, which would
potentially affect the performance of the model. Therefore, using a model with
native support for NaN values was the most suitable approach for our case. Below
is a brief introduction to LightGBM model. \paragraph{LightGBM Model} ~\\
LightGBM (Light Gradient Boosting Machine) is a highly efficient framework 
for gradient boosting decision trees (GBDT) which was developed by Microsoft
\cite{ke2017lightgbm}. The model it similar to popular GBDT such as XGBoost, 
however it addresses some of the limitations of these models in terms of efficiency 
and scalability for large datasets with high dimensional features \cite{ke2017lightgbm}.

Unlike level wise tree growth that can be seen in traditional GBDT models, 
LightGBM uses a leaf wise growth strategy as can be seen in the figure below \cite{ke2017lightgbm}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Included_Images/LightGBM.png}
    \caption{Leaf wise tree growth strategy.}
\end{figure}
It expands the leaf with the maximum variance gain at each step, as shown in the formula below:
\begin{equation}
V_{j|o}(d) = \frac{1}{n_o} \left( \frac{ \left( \sum_{ {x_i \in O: x_{ij} \leq d} } g_i \right)^2 }{ n_l^o(d) } + \frac{ \left( \sum_{ {x_i \in O: x_{ij} > d} } g_i \right)^2 }{ n_r^o(d) } \right),
\end{equation}
where $V_{j|o}(d)$ is the variance gain, $g_i$ are the gradients, $n_o$ is the number of samples in the parent node. This approach 
optimizes boosting and leads to faster convergence when combined with gradient based sampling. 

LightGBM also provides native support for NaN values through optimal 
directional assignment during splits. This helps to avoid manual imputation 
of data that could introduce bias. 
\subsubsection{Model Training}
In order to train the LightGBM model, the dataset was first split into training
testing and validation sets with a ration of 60:20:20 respectively. A total of 70,125 
labeled samples were used for training the model. The training features included pseudorange, 
Carrier to Noise Ratio, Doppler measurement and Carrier Phase.
The tuning of hyperparameters was then performed using a grid search approach and the used 
hyperparameters are shown in the table below. 
\begin{table}[htbp]
\centering
\label{tab:lgbm-hyperparams}
\renewcommand{\arraystretch}{1.2} % Increase row height for better readability
\begin{tabular}{ll}
\toprule
\rowcolor{lightgray}
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
objective & multiclass \\
n\_estimators & 2000 \\
\rowcolor{gray!10}
num\_leaves & 96 \\
learning\_rate & 0.02 \\
\rowcolor{gray!10}
boosting\_type & gbdt \\
max\_depth & -1 \\
\rowcolor{gray!10}
min\_child\_samples & 20 \\
min\_child\_weight & 1e-3 \\
\rowcolor{gray!10}
reg\_alpha & 0.5 \\
reg\_lambda & 1.0 \\
\rowcolor{gray!10}
subsample & 0.8 \\
subsample\_freq & 1 \\
\rowcolor{gray!10}
colsample\_bytree & 0.8 \\
class\_weight & balanced \\
\rowcolor{gray!10}
random\_state & 42 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for LGBMClassifier}
\end{table}

The model converged well and the loss curves for training and validation sets are shown in the figure below.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Included_Images/LossCurve.png}
    \caption{Loss curves for training and validation sets.}
\end{figure}
From the figure above, it is clear that the model did not overfit as the training and validation 
loss curves are close to each other and decreases steadily. Further performance assessment results 
of the model are presented in the results and discussion section. 
\subsection{LiDAR SLAM}
\subsubsection{LiDAR-Inertial Odometry}
For the front end of the modular SLAM system, LiDAR-Inertial Odometry (Fast-LIO)
is used to build a map effectively within a short amount of time. To do this,
Iterative Extended Kalman Filter (IEKF) is used to fuse the data between IMU and LiDAR
point clouds to create an odometry of the environment. 

Iterative Extended Kalman Filter (IEKF) follows the core of Bayesian Recursion
to estimate the state of the robot through the prediction and measurement update
step. In the prediction step, IEKF assumes that the state of the system at time
$k$ evolved from the prior state at time $k-1$ is shown as follows:
\begin{equation}
    x_k = f(x_{k-1},u_k) + w_{k} 
\end{equation}

Where $x_k$ is the state vector containing the terms of interest for the system
at time $k$. The $f(.)$ represents a non-linear state function that is used to
forecast current state data from prior state data $x_{k-1}$ and $u_k$ is a control vector. We can approximate $w_{k-1}$ as
$N(0,Q_k)$ where it has a zero-mean Gaussian distribution with covariance matrix
$Q_k$.

To know the uncertainty of the prediction model, error covariance matrix $P^f_k$
is defined as shown:
\begin{align}
    P_k^f &= E\left(e_k^f(e_k^f)^T\right) \nonumber \\
    &\approx F_k E(e_{k-1}e_{k-1}^T)F_k^T + E(w_{k-1}w_{k-1}^T) \nonumber \\
    &= F_kP_{k-1}F_k^T + Q_{k-1}
\end{align}

Where $e$ is the prediction error, $F$ is the Jacobian matrix of the linearized
state transition function $f(.)$ evaluated at the previous estimate
$x_{k-1}$, and $Q$ is the process noise covariance matrix representing the
uncertainty of the physical model.

Even so, the predicted state alone is prone to errors due to various
factors such as inaccurate prediction function and further source of noise. This
issue can be solved by taking sensor measurements to improve the predicted state
accuracy. Hence the observation model is defined as shown:
\begin{equation}
    z_k = h(x_k) + v_k
\end{equation} 

Where $z_k$ represents the expected sensor measurements based on the predicted
state $x_k$ expressed by a non-linear function $h(.)$, and the observation noise
$v_k$ is assumed as $N(0,R_k)$ where it has zero mean with covariance matrix $R_k$.

To further improve the accuracy for the nonlinear system, an iteration index
$K$ is added to repeat the measurement update at time step $k$ as shown:
\begin{equation}
    x^a_{k,K=0} = x^f_k
\end{equation}

Where $x^a$ is the state estimate after corrected by sensor measurement and
$x^f$ is the predictive distribution before looking at the LiDAR data. In this
case, the iteration $K$ is repeated until the change in the state estimate
$|x^a_{k,K+1} - x^a_{k,K}|$ falls below threshold $\epsilon$. 

As actual measurements from sensors will typically differ from this prediction
due to sensor noise and model inaccuracies, it is incorporated into the state
estimate by computing the difference between the actual measurements and the
predicted measurement based on the current iterative estimate $K$:
\begin{equation}
    z_{k, K} = z_k - h(x^a_{k, K})
\end{equation}
Where $h(x^a_{k, K})$ is the non-linear observation function evaluated at the
most recent estimate. To balance the uncertainty of the prediction against the
new sensor data, the Kalman Gain $K_{k,K}$ is calculated using the state-space
dimension formula \cite{xu2021fast}.
\begin{equation} 
    K_{k, K} = (H_{k, K}^T R_k^{-1} H_{k, K} + (P_k^f)^{-1})^{-1} H_{k, K}^T R_k^{-1} 
\end{equation}

Where $H_{k, K}$ is the Jacobian of the observation function re-evaluated
at the current iterative estimate $x^a_{k, K}$. This gain is then used
to compute the next refined state estimate:
\begin{equation}
    x^a_{k,K+1} = x^a_{k, K} + K_{k, K} (z_{k, K}) - (I - K_{k, K} H_{k, K})(x^a_{k, K} - x_k^f)
\end{equation}

After updating the state estimate, the updated covariance of the state
distribution is given as:
\begin{equation} 
    P_k = (I - K_{k, K} H_{k, K}) P_k^f 
\end{equation}

While the IEKF provides a high-frequency estimate of the current state, the
overall SLAM problem can be generalized as a Maximum A Posteriori (MAP)
estimation. Bayes Filter is then used to recursively perform the state
prediction and measurement update steps to minimizes linearization errors for
robust localization. In this case, MAP can be expressed as:
\begin{equation}
    P(X|Z, U) \propto P(x_0) \prod_{k} P(z_k|x_k) \prod_{k} P(x_k|x_{k-1}, u_{k-1})
\end{equation}

Where $P(X|Z, U)$ is the refined state estimate, $P(x_0)$ is the prior,
$P(z_k|x_k)$ is the sensor measurement and $P(x_k|x_{k-1}, u_{k-1})$ is the
predicted state.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/Kalman Filter.png}
    \caption{The red distribution in the figure represents the predicted step $P(x_k|x_{k-1}, u_{k-1})$  which results in predictive distribution  $x^f_k$.The blue distribution represents likelihood $P(z_k|x_k)$ derived from the noisy sensor measurement $z_k$. The green distribution illustrates the refined state estimate $x^a_k$, showing the reduction in the uncertainty covariance $P_k$ after fusing both data sources \cite{faragher2012understanding}.}
\end{figure}

\subsubsection{Graph-Based SLAM}
As LiDAR odometry is used in the front end to provide high-rate local pose
estimates without global constrains, iSAM2 pose graph optimization architecture
is used for the back end to further optimize the point cloud map for higher
accuracy. This technique is rooted in factor graph optimization algorithm
where Maximum A Posteriori (MAP) is also used to factorize the joint posterior
probability of the robot trajectory X as shown  \cite{wen2021factor}:

\begin{equation} 
    P(X|Z, U) \propto P(x_0) \prod_{k} P(z_k|x_k) \prod_{k} P(x_k|x_{k-1}, u_{k-1}) 
\end{equation}

Where $P(x_0)$ is the prior, $P(z_k|x_k)$ represents the Update Factors from
LiDAR measurements, and $P(x_k|x_{k-1}, u_{k-1})$ represents the Propagation
Factors derived from the IEKF prediction. The purpose of this is to decompose
the complex problem of estimating a full trajectory into smaller, simpler pieces
called 'factors'. By breaking down the trajectory into these discrete
components, this allows iSAM2 to optimize specific parts of the trajectory
affected by new information or loop closures instead of resolving the entire
global map every time the robot moves. This ensures that global consistency is
maintained without the computational burden of a full-batch optimization.

To do this, the Gaussian noise is factorized into exponential functions. Unlike
a standard filter that only estimates the current state $x_k$, the factorized
graph maintains the entire trajectory history $X$ = ($x_0, x_1, \dots, x_k$) to
minimize the sum of squared errors. The Gaussian noise from the IEKF motion
prediction is expressed as:
\begin{equation}
    P(x_k|x_{k-1},u_{k-1}) = \exp(-||f_k(x_{k-1}, u_k)-x_k||^2_{\Sigma_{u,k}})
\end{equation}

Similarly, the Gaussian noise from LiDAR measurements is factorized into the following equation:
\begin{equation}
    P(z_k | x_k) = \exp(-||h_k(x_k) - z_k||^2_{\Sigma_{z,k}})
\end{equation}

The factorization is crucial to transform MAP estimation from a product
of probability distributions into a Non-linear Least Squares (NLS) optimization
problem. By taking the negative logarithm of the posterior, we convert the task
of finding the maximum probability into finding the minimum of an error function
$e(\mathcal{X})$ as shown:
\begin{equation}
    \hat{\chi} = \arg\max_{\chi} \prod_{k} F_k(x_k) = \arg\min_{\chi} e(\chi)
\end{equation}
\begin{equation}
    e(\chi) \doteq \sum_{k} ||h_k(x_k) - z_k||^2_{\Sigma_k} + \sum_{k} ||f_k(x_{k-1}, u_k) - x_k||^2_{\Sigma_k}
\end{equation}

To solve the non-linear least squares problem defined by $e(\mathcal{X})$, the
system iteratively refines the trajectory estimate to find the optimal solution
$\hat{\mathcal{X}}$. The error function is linearized using a first-order Taylor
expansion around the current estimate $\mathcal{X}^{(i)}$ where the state is
then uploaded incrementally:
\begin{equation}
    \mathcal{X}^{(i+1)} = \mathcal{X}^{(i)} + \Delta \mathcal{X}
\end{equation}

Where $\Delta \mathcal{X}$ is the calculation for minimizing the error. This
optimization process relies on the Jacobian matrix $\mathbf{J}_R$ to linearize
the state variables as shown:

\begin{equation}
    \mathbf{J}_R = \frac{\partial(\mathbf{H}(\mathcal{X}) - \mathbf{Z})}{\partial \mathcal{X}} + \frac{\partial(\mathbf{F}(\mathcal{X}, \mathbf{U}) - \mathcal{X})}{\partial \mathcal{X}}
\end{equation}

Where $\mathbf{H}(\mathcal{X}) - \mathbf{Z}$ is the measurement residuals and
$\mathbf{F}(\mathcal{X}, \mathbf{U}) - \mathcal{X}$ is the predictive propagation
model. By setting the gradient of the error function to zero, the system solves
for $\Delta \mathcal{X}$ through the normal equations using the Gauss-Newton
method. Unlike the IEKF, which only iterates on the current state at time $k$,
this graph-based optimization allows the system to re-linearize the entire
trajectory history to ensure local sensor noise does not lead to permanent
global map distortion 



\subsubsection{System Architecture}
In this project, a modular SLAM system is used with the front end having a
Fast-LIO LiDAR odometry to carry out mapping in real time, while Pose Graph
Optimization (PGO) coupled with GPS is used for the back end. During
implementation, both Fast-LIO and PGO are initialized. The point cloud data will
first be processed by Fast-LIO to create an odometry, and the odometry will then
be passed to the PGO to further optimize the point clouds to increase accuracy.
In the meantime, GPS data is passed into PGO to provide constraints to the SLAM
if large drifts occur. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Included_Images/SLAM flowchart.png}
    \caption{Flowchart of the modular SLAM system}
\end{figure}

This project builds upon an open-source implementation
that integrates Fast-LIO with PGO for GPS-aided SLAM:
\url{https://github.com/yxw027/FAST_LIO_GPS} 

The pseudocode of the front-end and back-end of the modular SLAM can be shown below:

{
    \singlespacing
    \begin{algorithm}[H]
    \caption{Front-End: FAST-LIO with MAVLink TimeSync}
    \begin{algorithmic}[1]
    \State \textbf{Initialize:} IEKF state $\mathbf{x}$ and covariance $\mathbf{P}$
    \State \textbf{Initialize:} ikd-Tree for incremental mapping
    \While{ROS is active}
        \State Sync LiDAR and IMU buffers based on GPS-aligned timestamps
        \If{IMU not initialized}
            \State Perform \textit{IMU\_init} (Gravity/Bias estimation)
            \State \textbf{continue}
        \EndIf
        \State \textbf{State Prediction:} $\hat{\mathbf{x}} \leftarrow$ IMU forward propagation
        \State \textbf{Undistortion:} Compensate point motion using $\hat{\mathbf{x}}$
        \For{$iter = 1$ \textbf{to} $NUM\_MAX\_ITERATIONS$}
            \State Find point-to-plane correspondences in ikd-Tree
            \State Compute measurement residual $\mathbf{z}$ and Jacobian $\mathbf{H}$
            \State \textbf{Update:} Correct state $\mathbf{x}$ using IEKF update rule
            \If{converged} \State \textbf{break} \EndIf
        \EndFor
        \State \textbf{ikd-Tree Update:} Add new features to the spatial map
        \State Publish Odometry to Back-End
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
}

{
    \singlespacing
    \begin{algorithm}[H]
    \caption{Back-End Pose Graph Optimization (PGO)}
    \begin{algorithmic}[1]
    \State \textbf{Initialize:} GTSAM Factor Graph and ISAM2 optimizer
    \State \textbf{Initialize:} Noise models (Odometry, GPS, Loop Closure)
    \While{ROS is active}
        \If{New Front-End Odometry received}
            \State Calculate delta movement since last Keyframe
            \If{movement $>$ $KeyframeThreshold$}
                \State Create new Node $n_t$ in Pose Graph
                \State \textbf{Propagation Factor:} Add \textit{BetweenFactor}($x_{k-1}, x_k, \Delta Odom$)
                \If{GPS matched for $n_t$}
                    \State Transform GPS Lat/Lon to UTM coordinates
                    \State \textbf{GPS Factor:} Add \textit{GPSFactor}($x_k, UTM_{xyz}$)
                \EndIf
                \State \textbf{Loop Closure:} Search for matches using Scan Context
                \If{Loop detected with $n_{\text{old}}$}
                    \State Calculate relative transform via ICP
                    \State \textbf{Loop Factor:} Add \textit{BetweenFactor}($x_{\text{old}}, x_t, T_{\text{ICP}}$)
                \EndIf
                \State Perform ISAM2 global optimization
                \State Update trajectory with optimized poses
            \EndIf
        \EndIf
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/rqt_graph.png}
    \caption{Interactions of different ROS topics in the SLAM system}
\end{figure}


% --- Experiments ---
\section{Experiments}
To apply the methodologies outlined in the previous section, a series of field
experiments were conducted, aiming to demonstrate the functionality of the UAV
platform and collect data necessary for further analysis. Over ten unique
flight missions were executed across three different locations in Hong Kong and
diverse flight scenarios, including static hovering, dynamic surveying, and manual
controlled flight. The following subsections present the experimental setups, 
summarize experiments conducted, and detail one key experiment that highlight 
the current system capabilities and challenges for further development.


\subsection{Experimental Setup}
Before the initiation of a flight mission, the ground control station must be
configurated and remote connections with the onboard computer should be established.

\subsubsection{Ground Control Setup}
To set up the ground control station, we first paired the telemetry in the UAV
and in the ground controller. For this experiment, QGround Control
\cite{QGroundControl_PlanView} was used as the ground controlling software and use 433MHz
Radio Telemetry for communication between Pixhawk and QGround Control. 

QGround Control (QGC) is a full-featured flight control and mission planning
software for MAVLink-enabled drones.  The experiments were conducted as
missions, which enables the user to select a set of waypoints in the QGC map and
upload it to the UAV’s flight controller. 

The plan view in QGC helps to assign particular waypoints, including takeoff and
landing. Figure \ref{fig:qgc_plan_view} shows a screenshot of the QGC plan view,
which specifies the different toolbars, including plan tools, the command
editor, and the mission list.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/qgc_planview.jpg}
    \caption{QGround Control Plan View}
    \label{fig:qgc_plan_view}
\end{figure}
Additionally, users can specify the altitude and speed at each waypoint by
editing the mission command list. For instance, if the UAV is hovering
over a tree, a speed of 2 m/s will be assigned, to hold for 300 seconds, and the
altitude will be roughly around 30m. Figure \ref{fig:qgc_mission_list} shows an
example of a mission command list in QGC.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Included_Images/qgc_mission_list.jpg}
    \caption{QGround Control Mission Command List}
    \label{fig:qgc_mission_list}
\end{figure}

\subsubsection{Establishing Remote Connection with Onboard Computer}
While the ground controller is being set up, the onboard Jetson Orin is being connected
through both the SSH protocol and remote desktop on two separate computers. 

Through Wi-Fi, both the SSH computer and the Jetson Orin is connected into a common internal
network that was pre-established using ZeroTier. Then, the Terminal of the Jetson Orin could
be easily accessed through Virtual Studio Code, where the command to log GNSS data from the
two onboard receivers can be executed. 

On the other hand, SLAM data including LiDAR point clouds, IMU, GPS data are
collected into a .bag file from Robot Operating System (ROS). To do so,
Nomachine is used to create remote access from Jetson Orin with the
pre-established ZeroTier connection. Such method enable access of multiple
terminals from Jetson Orin so that all the required sensor drivers can be
launched to carry out data collection.


\subsection{Summary of Experiments Conducted}
A total of 10 successful flights were conducted during the reporting period. They are conducted in the following three locations: Homantin Mountain, Tai Hang Tun Kite Flying Area, and Nam Sang Wai. Photos showing the experiment locations are arranged in Figure \ref{Experiment Environment} and the experiments are summarized in Table \ref{Experiment Summary}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Included_Images/HomantinMountain.jpg}
        \subcaption{Homantin Mountain}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Included_Images/TaiHangTun.jpg}
        \subcaption{Tai Hang Tun}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{Included_Images/NamSangWai.jpg}
        \subcaption{Nam Sang Wai}
    \end{subfigure}
    \caption{Experiment environment photos at experiment locations.}
    \label{Experiment Environment}
\end{figure}


\begin{longtable}{|m{0.03\textwidth}|m{0.05\textwidth}|m{0.12\textwidth}|m{0.25\textwidth}|m{0.4\textwidth}|}
    \caption{Summary of Experiments Conducted}
    \label{Experiment Summary}\\
    \hline
    & \textbf{Date} & \textbf{Location} & \textbf{Descriptions} & \textbf{Flight Trajectory}\\ 
    \hline
    \endfirsthead
    \hline
    & \textbf{Date} & \textbf{Location} & \textbf{Descriptions} & \textbf{Flight Trajectory}\\ 
    \hline
    \endhead
    \hline
    \endfoot
    (a) & 2025 Oct. 01 & Homantin Mountain & This experiment is carried out via manual flight control where it hovers above bare ground with grass for 3 minutes & \includegraphics[width=\linewidth]{Included_Images/HomantinMountainTrack.png}\\ 
    \hline
    (b) & 2025 Oct. 12 & Tai Hang Tun & This experiment indicates the first successful implementation of path planning where the UAV hovers above bare ground with grass in two different points for 2 minutes respectively. & \includegraphics[width=\linewidth]{Included_Images/TaiHangTunGrass.png}\\ 
    \hline
    (c) & 2025 Oct. 12 & Tai Hang Tun & This experiment is carried out via path planning where the UAV is hovering over bare ground with grass for 2 minutes and above trees for 3 minutes & \includegraphics[width=\linewidth]{Included_Images/TaiHangTunTree.png}\\ 
    \hline
    (d) & 2025 Oct. 12 & Tai Hang Tun & This experiment is carried out via manual flight control. However, we face the problem of controlling the UAV mid-way due to large wind velocity. Hence resulting in irregular flight path. & \includegraphics[width=\linewidth]{Included_Images/TaiHangTunDynamic.png}\\ 
    \hline
    (e) & 2025 Nov. 09 & Nam Sang Wai & This experiment is carried out via path planning where the UAV is hovering over bare ground with grass in two different points for 3 minutes respectively. & \includegraphics[width=\linewidth]{Included_Images/NamSangWai094324.png}\\ 
    \hline
    (f) & 2025 Nov. 09 & Nam Sang Wai & This experiment is carried out via path planning where the UAV is hovering over bare ground with grass for 3 minutes and above trees for 5 minutes. & \includegraphics[width=\linewidth]{Included_Images/NamSangWai101211.png}\\ 
    \hline
    (g) & 2025 Nov. 09 & Nam Sang Wai & This experiment is conducted by manually assigning multiple waypoints in QGroundControl to perform path planning. The UAV hovers above each waypoint for one minute, with a total of six waypoints assigned in the mission. & \includegraphics[width=\linewidth]{Included_Images/NamSangWai103755.png}\\    
    \hline
    (h) & 2025 Nov. 09 & Nam Sang Wai & This experiment indicates the first implementation of using surveying function from QGroundControl to carry out coverage path planning. The UAV is going back and forth between bare ground with grass and tree clusters. & \includegraphics[width=\linewidth]{Included_Images/NamSangWai113425.png}\\ 
    \hline
    (i) & 2025 Nov. 09 & Nam Sang Wai & This experiment demonstrates wider-area coverage through autonomous path planning, similar to (h) where it allows the UAV to systematically fly over clusters of trees. & \includegraphics[width=\linewidth]{Included_Images/NamSangWai113830.png}\\ 
    \hline
    (j) & 2025 Nov. 09 & Nam Sang Wai & This experiment was also carried out via autonomous coverage path planning, but in the opposite direction compared to experiment (h) and (i) when scanning tree clusters. & \includegraphics[width=\linewidth]{Included_Images/NamSangWai115109.png}\\ 
    \hline
\end{longtable} 



\subsection{Key Experiment: Experiment (i)}
In this subsection, the details of Experiment (i) in Table \ref{Experiment Summary}
are discussed. Here, a survey flight mission was conducted to collect 
feature-extensive LiDAR and GNSS-R datasets above a canopy area consisted of mixed
dense vegetation. 

\subsubsection{Setup and Procedure}
The experiment was conducted in Nam Sang Wai on November 09, 2025, where the UAV
followed a pre-planned path surveying an area consisting of dense, mixed-species 
tree. First, the UAV sensors were calibrated before the experiment session. After
the UAV is being connected to the battery, radio telemetry connection and remote 
connections to the onboard computer were established. With the trajectory plan
uploaded onto the onboard Pixhawk controller and the onboard computer configured to
initialize data collection, the mission was executed. Lastly, the collected data could
be downloaded and post-processed for further analysis.

\subsubsection{Path Planning and Execution}
In this experiment, the trajectory is planned to ascend and maintain at an altitude of
35 meters during the survey mission. The paths in the survey area are spaced at 2 meter
intervals, with the turnaround buffer distance set at 5 meter. The result of the flight
plan is shown in the figure below. 
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=0.75\textwidth]{Included_Images/KeyExperiment_Pathplanninglist.png}
        \caption{Mission Planning List in QGroundControl}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.65\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/KeyExperiment_Plannedpath.png}
        \caption{Planned Path Result}
    \end{subfigure}
    \caption{Path planning and execution results.}
\end{figure}

\subsubsection{GNSS-R Analysis}
After the successful collection of GNSS data during the flight mission, the raw data
was post processed into measurement information. With the FFZs of the reflected GNSS 
signals calculated, they were plotted onto the satellite image map. Here, color
of the FFZs indicate the $C/N_0$ ratio between the bottom antenna and the top antenna,
and strong reflections over water and bare ground could be observed compared to those
reflected from vegetation.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Included_Images/KeyExperiment_GNSSR.png}
    \caption{GNSS-R Fresnel zone analysis result.}
\end{figure}

\subsubsection{Machine Learning Signal Classification}
For this dataset, a trained LGBM classification model was used to classify the
GNSS signals into the categories of tree-reflected, ground-reflected, and direct
signals, with the results shown in the figure below. Here, the majority of the
signals are classified as tree-reflected since the survey was conducted over 
vegetation. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Included_Images/KeyExperiment_ML.png}
    \caption{LGBM-based signal classification result.}
\end{figure}

\subsubsection{LiDAR SLAM}
For this dataset, LiDAR data was processed using two back-end techniques: the
standard Fast-LIO odometry and SCPGO with GPS fusion (SCPGO+GPS). As shown in
the figures below, the SCPGO+GPS result demonstrates significant drift compared
to the Fast-LIO output, which will be further discussed in the coming section.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=0.95\textwidth]{Included_Images/KeyExperiment_FAST-LIO.png}
        \caption{Fast-LIO}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/KeyExperiment_SCPGO_GPS.png}
        \caption{SCPGO+GPS}
    \end{subfigure}
    \caption{LiDAR SLAM results using two different techniques.}
\end{figure}

% --- Results and Discussion ---
\section{Results and Discussion}
Placeholder text

\subsection{Path Planning}
\subsubsection{Waypoint-based Navigation Results}
The results of the waypoint-based navigation can be divided into two platforms,
as test runs were conducted using both a locally developed A* algorithm and
a waypoint navigation in QGroundControl. 

\paragraph{Locally Developed A* Algorithm Results} ~\\
A* algorithm results evaluation The results are obtained in various types of
vegetation environments, including dense forests, crop fields, and grasslands.
The images are obtained from the QGround Control satellite imagery and sent to
our image processing pipeline to identify the potential flying areas. 

The figure \ref{fig:forest_path} shows two different start and end points, and how the A* algorithm
finds the shortest path between them in a dense forest canopy. The image
processing filter is designed to identify darker shades of green in this image.
Therefore, the algorithm fits precisely to predict the paths in the dense area. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Included_Images/forest_path.png}
    \caption{A* path planning in dense forest canopy}
    \label{fig:forest_path}
\end{figure}
The output of the figure \ref{fig:forest_path} waypoints is shown in the below output:
\begin{lstlisting}[frame=single, basicstyle=\ttfamily\small, breaklines=true, caption={Path Planning Execution Output}, label={lst:path_output}]
Planning UAV path...
Search stats: {'iterations': 156, 'nodes_explored': 146, 'path_length': 57, 'time': 0.05894970893859863}
Path found! Length: 57 waypoints
\end{lstlisting}
Similarly, figures \ref{fig:grassland_path} and \ref{fig:mixed_path} show how
the A* algorithm determines the path to a grassland and to a combination of land
and dense canopy. Since the filter is designed to filter out light shades of
green color, the dense vegetation is considered occupied. Therefore, the
algorithm will avoid converging on paths that lead to dense vegetation. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Included_Images/grassland_path.png}
    \caption{A* path planning in grassland area}
    \label{fig:grassland_path}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Included_Images/mixed_path.png}
    \caption{A* path planning in mixed land and dense canopy area}
    \label{fig:mixed_path}
\end{figure}

\paragraph{QGround Control Waypoint-based Navigation} ~\\
In QGround Control, we defined the important parameters, such as flying
waypoints, flying speed, and flying altitude, using the Graphical User Interface
in QGround Control. Then, the coordinates were uploaded and the data to the Pixhawk
flight controller using telemetry. The figure shows the settings and the
successful execution of a mission. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/gqc_mission.png}
    \caption{Waypoint-based navigation mission planning in QGround Control}
    \label{fig:qgc_mission}
\end{figure}

\subsubsection{Coverage Path Planning Results}
Similar to waypoint-based navigation, the coverage-based path planning results
are divided into two segments: a locally implemented boustrophedon algorithm and
coverage mission planning in QGroundControl. 
\paragraph{Boustrophedon Implementation Results} ~\\
The algorithm results were obtained by running the ROS (Robot Operating System) \cite{ROS_website}
package developed by the Autonomous Systems Lab at ETH Zurich \cite{B_hnemann_2021}. The experiments
were conducted in an Ubuntu 22.2 Linux environment. The figure \ref{fig:cpp_gui}
shows the graphical user interface of the coverage path planner. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/cpp_gui.png}
    \caption{Coverage path planning GUI}
    \label{fig:cpp_gui}
\end{figure}

At first, the map was specified as a polygon in the GUI. Once giving the input, 
the algorithm will determine the coordinates of the polygon, the sweep
distance, and the altitude.  An example of result interpretation for a polygon
shown in Figure \ref{fig:cpp_result} is as follows: 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/cpp_result.png}
    \caption{Coverage path planning result interpretation}
    \label{fig:cpp_result}  
\end{figure}
\begin{lstlisting}[
    frame=single, 
    basicstyle=\ttfamily\scriptsize, % Used scriptsize because logs are wide
    breaklines=true, 
    caption={Coverage Planner (Boustrophedon) Execution Log}, 
    label={lst:ros_log}
]
[ INFO] [1762353101.697507684]: Successfully loaded polygon.
[ INFO] [1762353101.697518253]: Altitude: 3 m
[ INFO] [1762353101.697524727]: Global frame: world
[ INFO] [1762353101.697541062]: Polygon: 6 48.7642 63.1467 69.111 63.589 73.7554 75.7529 61.3704 85.9262 39.4755 84.8204 33.9465 74.8682 0
[ INFO] [1762353101.698284610]: Sensor model: frustum
[ INFO] [1762353101.698308756]: Lateral FOV: 0.84
[ INFO] [1762353101.698322562]: Altitude: 3
[ INFO] [1762353101.698376220]: Lateral overlap: 0.6
[ INFO] [1762353101.698389550]: Sweep distance: 1.07177
[ INFO] [1762353101.699355168]: Start creating sweep plan graph.
[ INFO] [1762353101.706477515]: Successfully created boustrophedon decomposition with 1 polygon(s).
[ INFO] [1762353101.770296130]: Created sweep plan graph with 6 nodes and 0 edges.
[ INFO] [1762353101.770459306]: Initially created 24 nodes.
[ INFO] [1762353101.770476268]: Pruned 18 nodes.
[ INFO] [1762353101.770542355]: Finished creating the sweep planner.
[ INFO] [1762353101.770556488]: Sending visualization messages.
\end{lstlisting}
As shown in the command-line output, the polygon is considered to be 3 meters
high in altitude. This simulates a camera or a sensor flying at 3m height with a
field of view. The sweeps are spaced 1.07177m apart to ensure 60 percent overlap
between consecutive passes. The Boustrophedon algorithm divides the polygon
into one cell that can be swept in a back-and-forth pattern. The algorithm
generated 24 possible sweep patterns across the cell. 

Following the initial polygon path planning tests, the system was further
evaluated in an environment containing polygonal obstacles. The simulation
environment is illustrated in Figure \ref{fig:env_obstacles}, while the
corresponding execution log is presented in Listing \ref{lst:output_log}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/env_obstacles.png}
    \caption{Coverage path planning environment with obstacles}
    \label{fig:env_obstacles}   

\end{figure}
\begin{lstlisting}[
    frame=single, 
    basicstyle=\ttfamily\scriptsize, % Keeps the long lines from wrapping too much
    breaklines=true, 
    caption={Path Planning with Polygonal Obstacles Execution Log}, 
    label={lst:output_log}
]
[ INFO] [1762354993.019431606]: Reset planner.
[ INFO] [1762354993.016887499]: Sensor model: frustum
[ INFO] [1762354993.017337497]: Lateral FOV: 0.84
[ INFO] [1762354993.017813480]: Altitude: 4
[ INFO] [1762354993.017922272]: Lateral overlap: 0.6
[ INFO] [1762354993.017982320]: Sweep distance: 1.42903
[ INFO] [1762354993.019739225]: Start creating sweep plan graph.
[ INFO] [1762354993.048630096]: Successfully created boustrophedon decomposition with 7 polygon(s).
[ INFO] [1762354993.192612591]: Created sweep plan graph with 30 nodes and 768 edges.
[ INFO] [1762354993.192705705]: Initially created 72 nodes.
[ INFO] [1762354993.192714347]: Pruned 42 nodes.
[ INFO] [1762354993.192781320]: Finished creating the sweep planner.
[ INFO] [1762355001.110867717]: Selecting START from RVIZ PublishPoint tool.
[ INFO] [1762355008.224192732]: Selecting GOAL from RVIZ PublishPoint tool.
[ INFO] [1762355008.224279349]: Start solving.
[ INFO] [1762355008.224302797]: Start solving GTSP using GK MA.
[ INFO] [1762355008.225939482]: The adjacency matrix scale is: 1901.533171685
[ INFO] [1762355008.247250514]: Start solving GTSP.
[ INFO] [1762355008.291611215]: Finished solving GTSP.
[ INFO] [1762355008.292001969]: Finished plan.
\end{lstlisting}
As shown in the output log, seven polygons were initially created with 72 nodes.
The algorithm then pruned the number of nodes to 42. In summary, the
boustrophedon decomposition was tested locally on polygonal maps with and
without obstacles. 

\paragraph{Coverage Path Planning in QGround Control} ~\\
First we defined the important parameters, such as waypoints, speed, trigger
distance and altitude, using the Graphical User Interface in QGround Control.
The trigger distance is a parameter that specifies the space between each sweep
line. For coverage path planning, we also defined the geo-fence and the rally
point (an alternative landing point). We then uploaded the mission to the
Pixhawk flight controller via telemetry. The figure shows the settings and the
successful execution of the mission. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/gqc_coverage.png}
    \caption{Coverage path planning mission in QGround Control}
    \label{fig:gqc_coverage}
\end{figure}

In summary, waypoint-based navigation experiments were conducted primarily to
hover the UAV above a tree or vegetation and above bare ground without trees.
This is mainly to assist in obtaining the differences  between the GNSS-R signals
in two scenarios and conduct an analysis on the reflected signals from the
trees. 
Additionally, coverage path planning was implemented to obtain dynamic UAV
movement data, which enables the development of LiDAR SLAM in our system. LiDAR
SLAM is crucial in the system to cross-validate the data obtained through GNSS-R
analysis and the Machine Learning pipeline. 

\subsection{GNSS-R}
After successful execution of the path planning algorithms onboard the UAV, GNSS
measurements from both the zenith and nadir antenna are recorded and
post-processed. Specifically, the ratio of the nadir $C/N_0$ and the zenith
$C/N_0$ is analyzed with respect to the FFZ locations. In the figure below, the
FFZs from experiment (j) and (i) from Table \ref{Experiment Summary} are plotted
onto a satellite image map, with colors representing the ratio of $\frac{C/N_{0,
reflected signal}}{C/N_{0,direct signal}}$.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.75\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/CNR_Ratio_1.png}
        \caption{Experiment (j)}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.75\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/CNR_Ratio_2.png}
        \caption{Experiment (i)}
    \end{subfigure}
    \caption{Plot of FFZs on satellite imagery, colored by $C/N_0$ ratios.}
\end{figure}
From the above, it is evident that when the FFZ is over water or bare ground,
the $C/N_0$ ratio between the reflected and direct signals tends to be higher.
This indicates that stronger reflections occur over water and bare ground
compared to vegetated areas.

Conversely, when the UAV hovers over trees, more reflected signals are observed,
as shown in Figure \ref{Pr_Error Distribution}. By definition, since pseudorange
errors are primarily caused by signal reflections, signals received in an
unobstructed open sky environment should exhibit near-zero mean pseudorange
errors. However, the pseudorange error distribution of the zenith antenna
reveals a small bump centered at large positive values, indicating that
ground-reflected signals are being received. This bump is more pronounced in
plot (b), suggesting more ground reflections when the UAV is hovering above 
vegetation.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.75\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/Bare_Ground_Pr_Error.png}
        \caption{UAV hovering over bare ground.}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.75\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/Tree_Pr_Error.png}
        \caption{UAV hovering over trees.}
    \end{subfigure}
    \caption{Normalized distributions of psuedorange errors for both antennas in 
    two different flight scenarios.}
    \label{Pr_Error Distribution}
\end{figure}

While the above analysis reveals key differences in GNSS measurements that could
potentially distinguish tree-reflected signals for identifying vegetated areas,
it also highlights a critical assumption that must be validated. Up to this
point, it was assumed that the zenith antenna receives only direct signals while
the nadir antenna receives only reflected signals. However, Figure \ref{Pr_Error
Distribution} has already demonstrated that this assumption is invalid. To
investigate further, the distribution of the difference between the absolute
pseudorange errors of the nadir and zenith antennas is plotted in Figure
\ref{Pr_Error Difference}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Included_Images/Pseudorange_Error_Difference.png}
    \caption{Normalized distribution of difference between the absolute pseudorange errors of the 
    nadir antenna and the zenith antenna.}
    \label{Pr_Error Difference}
\end{figure}
First, since many observations exhibit near-zero differences between nadir and
zenith pseudorange errors, both nadir and zenith antenna may receive signals
with similar pseudorange errors. Two possible scenarios explain this phenomenon:
\begin{enumerate}
    \item The zenith antenna receives ground-reflected signals, exhibiting
    large pseudorange errors comparable to those of the nadir antenna. This
    finding further validates the previous observations in Figure \ref{Pr_Error
    Distribution}.
    \item The nadir antenna receives direct propagated signals, exhibiting
    small pseudorange errors comparable to those of the zenith antenna. Figure
    \ref{Pr_Error Distribution} also supports this possibility, showing near-zero
    pseudorange errors in the nadir antenna distribution plot.
\end{enumerate}
Furthermore, Figure \ref{Pr_Error Difference} shows observations where the
zenith pseudorange error significantly exceeds the nadir pseudorange error,
indicating that the zenith antenna receives reflected signals. Since the zenith
antenna is RHCP and the nadir antenna is LHCP, this also indicates that a GNSS
RHCP antenna is able to receive LHCP signals and vice versa. To further
investigate this observation, both antenna are tested in an open sky environment
and their respective received $C/N_0$ values are plotted in Figure \ref{Antenna
Testing Open Sky}.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/Open_Sky_Setup.jpg}
        \caption{Set up of antenna testing experiment.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{Included_Images/Open_Sky_Fisheye.png}
        \caption{Fisheye view of the experiment environment.}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.75\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/CNR_Distribution.png}
        \caption{Normalized $C/N_0$ distributions of both antenna.}
    \end{subfigure}
    \caption{Antenna testing in open sky environment.}
    \label{Antenna Testing Open Sky}
\end{figure}
Assuming all signals received in the open sky environment are non-reflected and
thus RHCP, the LHCP antenna (shown in orange) can indeed receive RHCP signals,
just at lower $C/N_0$ values compared to the RHCP antenna due to lower antenna
gains. This phenomenon is more pronounced in a subsequent experiment, where both
antennas were elevated midair. During the first five minutes, both antennas
faced upward; during the final five minutes, both antennas faced downward. The
corresponding $C/N_0$ values received are plotted against observation time in
the figure below.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.8\textwidth}
        \includegraphics[width=\textwidth]{Included_Images/RHCP_CNR.png}
        \caption{RHCP Antenna}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Included_Images/LHCP_CNR.png}
        \caption{LHCP Antenna}
    \end{subfigure}
    \caption{Temporal plot of $C/N_0$s received by both antenna.}
\end{figure}
The above observations are qualitatively summarized in the table below:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
     & \textbf{Direct Signal} & \textbf{Reflected Signal} \\
    \hline
    \textbf{RHCP Antenna} & High $C/N_0$ & Low $C/N_0$ \\
    \hline
    \textbf{LHCP Antenna} & Low $C/N_0$ & High $C/N_0$ \\
    \hline
    \end{tabular}
    \caption{Summary of $C/N_0$ behaviors for different antenna under different
    signal reception scenarios.}
\end{table}
Since both direct and reflected signals can be received by both the zenith RHCP
antenna and the nadir LHCP antenna, and their signal properties differ based on
signal polarization, it is critical to classify the received signals for
distinguishing reflected signals from direct signals in a robust GNSS-R system.
Machine learning approaches offer an effective solution, which will be detailed
in the following section. 

\subsection{Machine Learning}
As classification of the reflected and direct signals is required for robust GNSS-R
implementation, the LightGBM model is trained to classify the signals based on  
the features extracted from both antennas. The data collection, dataset preparation and model
selection has been explained in the methodology section. The performance of the model is
evaluated based on various metrics including accuracy, precision, recall and F1-score. The subsections 
below present the evaluation results and discussion.
\subsubsection{F1-score}
The F1-score is a mean of precision and recall of a classification model. Precision of a model indicates
the ability of a model to not label a negative sample as positive and recall indicates the ability of a model to 
find all the positive samples. The F1-score is calculated as follows from precision and recall:
\begin{equation}
    F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
\end{equation}
The F1-score for the LightGBM model on the test dataset is shown in the figure below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/F1.png}
    \caption{F1-score for LightGBM model on test dataset}
\end{figure}
From the firgure above, it is observed that the LightGBM model achieves a F1 score of approximately 
0.9 on the test dataset. This indicates that the model has a proper balance between precision and recall and
is effective in classifying different classes of signals. 
\subsubsection{Confusion matrix}
Confusion matrix is a table that is used to describe the performance of a classification model on a set of test data
for which the true values are known. The diagonal elements of a confusion matrix the correctly classified samples 
while the off-diagonal values indicate misclassified ones. The confusion matrix for our signal classification 
LightGBM model is shown in the figure below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/Confusion-matrix.png}
    \caption{Confusion matrix for LightGBM model on test dataset}
\end{figure}
From the confusion matrix above, it is observed that the LightGBM model has correctly classified a majority 
of the samples in the test dataset. Primary confusion for the model occurs for direct and ground reflected signals being classified as 
tree reflection. As can be observed from the confusion matrix, 1110 and 1008 samples of direct and ground reflected
signals respectively are misclassified as tree reflected signals. This could be due to the similarity in the features of these
signals. However, the model has performed well in classifying tree reflected signals with only 3 samples being misclassified. 
This results are acceptable as the model will later be used to identify vegetation reflected signals mostly.
\subsubsection{Receiver Operating Characteristic (ROC) Curve}
A Receiver Operating Characteristic (ROC) curve is a graphical representation that illustrates the performance of a 
binary classification model at various threshold settings. It plots the True Positive Rate (TPR) against the False Positive
Rate (FPR) and helps to visualize the trade-off between sensitivity and specificity of the model. If the ROC values is 
closer to 1, it basically indicates that the model has good ability to distinguish between classes. However, a value near
0.5 indicates that the model has no discrimination ability. For the LightGBM model trained for classification of signals, 
the ROC curve is shown in the figure below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/ROC.png}
    \caption{ROC curve for LightGBM model on test dataset}
\end{figure}
The figure indicates that the LightGBM model has a ROC value of over 0.95 for all three classes of signals, meaning 
that the model has high abiity to distinguish between signal classes.
\subsubsection{Feature Importance}
Feature importance is a techniques in Machine Learning to identify and rank the most relevant features 
in a model. A feature importance analysis was performed on the LightGBM model to identify the most significant features for 
our signal classification task. The feature importance plot is shown in the figure below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Included_Images/feature_importance.png}
    \caption{Feature importance plot for LightGBM model}
\end{figure}
The figure indicates that the top three most important features for the LightGBM model are:
\begin{itemize}
    \item Pseudorange 
    \item Doppler measurement
    \item Elevation Angle
\end{itemize}
These features have the highest importance scores, indicating that they contribute significantly to the model's ability to classify
the signals accurately. This insight can be useful for future feature engineering and model improvement efforts.
\subsection{LiDAR SLAM}
\subsubsection{Stage 1: Modular SLAM Implementation without GPS}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg1_fastlio.png}
    \caption{Fast-LIO LiDAR Odometry}
    \label{fig:stg1_fastlio}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg1_pgo.png}
    \caption{PGO with FPFH and ICP}
    \label{fig:stg1_pgo}
\end{figure}
The implementation of modular SLAM is first done without the integration of GPS
to make sure the architecture works. When comparing \autoref{fig:stg1_fastlio}
and \autoref{fig:stg1_pgo}, it is observed that there is less drifting in the
point clouds in the PGO SLAM. This is because the PGO SLAM uses Fast Point
Feature Histograms (FPFH) method to have an initial guess to align the point
clouds and uses Iterative Closest Point (ICP) for further refinement. FPFH can
effectively represent the local geometry around key points in a point cloud which
makes it robust against changes in viewpoints. Since FPFH is better at aligning
point clouds that are far away for global robustness, it is then passed to ICP
for further precise refinement with millimeter-level precision
\cite{song2025robotic}. 

On the other hand, Fast-LIO only uses Iterated Extended Kalman Filter (IEKF) to
align the point clouds. In this case, we can see that the dual layer of point
cloud alignment in PGO further increase the accuracy in SLAM.

\subsubsection{Stage 2: Modular SLAM Implementation with GPS}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg2_pgomis.png}
    \caption{PGO SLAM after integration of GPS. Green trajectory indicates LiDAR odometry, 3 axis trajectory indicates GPS trajectory}
    \label{fig:stg2_pgo}
\end{figure}
Once GPS is being integrated to the PGO SLAM, we can see in \autoref{fig:stg2_pgo} that there
are two different trajectories and the point clouds are very misaligned. First
of all, both trajectories are not aligned as the position of the LiDAR and the
GPS on the drone itself are different, hence having two different frames. Second of all, the point clouds are misaligned as GPS
has a high gain in the SLAM and some issues related to the transformation of the
GPS to LiDAR odometry. 

To solve the misalignments of both trajectories, extrinsic calibration is
carried out between IMU from Pixhawk that is connected to the GPS and the LiDAR
point clouds to align both frames together. In this case, HKU Mars calibration
system is used for the extrinsic calibration:
\url{https://github.com/hku-mars/LiDAR_IMU_Init/issues} 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg3_fastlio.png}
    \caption{Fast-LIO LiDAR Odometry}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg3_pgo.png}
    \caption{PGO SLAM with GPS and odometry trajectory aligned. Green trajectory indicates LiDAR odometry, 3 axis trajectory indicates GPS trajectory}
    \label{fig:stg3_pgo}
\end{figure}

Based on \autoref{fig:stg3_pgo}, despite both odometry and GPS trajectory and aligned, we can
still observe that the point clouds in PGO are still misaligned. Aside from having high
gain for GPS in the SLAM, the misalignment of point clouds is due to the issues regarding the coordinate
frame rotation between GPS and LiDAR odometry, and GPS transformation logic which
needs to be solved in later stage. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/withgps.png}
    \caption{PGO with GPS integration}
    \label{fig:withgps}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/nogps.png}
    \caption{PGO without GPS integration}
    \label{fig:nogps}
\end{figure}

\autoref{fig:withgps} and \autoref{fig:nogps} shows the difference between
the PGO SLAM with and without GPS integration when using the same architecture. In
this case we can confirm that GPS is the main issue that causes the point cloud
distortion.

% --- Conclusion ---
\section{Conclusion}
Placeholder text


% --- Future Works ---
\section{Future Works}
In this section, we present the limitations of each component at its current
stage, followed by proposed solutions. The frameworks discussed include path
planning, GNSS-R, machine learning, and LiDAR SLAM. The discussed solutions will
be operated in the next stage of our project. 

\subsection{Path Planning}
For the path planning framework, so far, most results have been conducted in a
local mapping environment without involving a UAV. Although the deployed
algorithms have demonstrated accurate results, it is crucial to obtain dynamic
results in UAV simulation and the real world. Therefore, the following
limitations arise in the path planning workflow:
\begin{itemize}
    \item Simulation in dynamic UAV path planning.
    \item Autonomous Navigation in the real-world UAV.
    \item Simultaneous mapping and autonomous navigation
\end{itemize}
\paragraph{1. Simulation in dynamic UAV path planning} ~\\
Simulating the flight trajectories of UAVs is crucial for understanding and
evaluating the dynamics and kinematics of the UAV during autonomous navigation.
With these simulations, we can verify our locally developed algorithms
and incorporate different frameworks (i.e., ROS) to implement software that is
compatible with hardware. 

To address this limitation, we will use uavpackage \cite{MathWorks_UAV_Delivery}, developed by MATLAB, for
small quadrotors, and Microsoft AirSim \cite{shah2017airsimhighfidelityvisualphysical} to conduct advanced simulations. These
simulation software packages are built specifically for unmanned aerial
vehicles; therefore, they provide the necessary aerodynamics and real-world
physics packages to minimize the gap between real-world and simulation
environments. 
\paragraph{2. Autonomous Navigation in the real-world UAV} ~\\
Currently, we do not deploy our algorithms in the real-world UAV; instead, we
use the control and path planning system in QGround Control to communicate with
the UAV. As a result, it is necessary to conduct real-world testing after we
obtain validated simulation results. Conducting real-world autonomous navigation
is necessary to automate the Fresnel Zone detection and to collect GNSS-R data. 

As a solution, we will
conduct indoor experiments in the university labs, incorporating obstacle
avoidance. These experiments are designed to validate the autonomous navigation
of the real-world drone and to address any potential hardware and software
issues. Figure \ref{fig:indoor_experiment} shows an example of a designed indoor autonomous experiment. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/indoor_experiment.png}
    \caption{Indoor autonomous navigation experiment design}
    \label{fig:indoor_experiment}
\end{figure}
\paragraph{3. Simultaneous mapping and autonomous navigation} ~\\
Simultaneous mapping and navigation enable the drone to construct its own map
for unknown environments while in the air. This is crucial for navigating
unknown environments and flying at different altitudes. Moreover, it provides a
fully autonomous system where the user does not need to input a map of given
potential vegetation areas. 

We plan to integrate LiDAR SLAM data into our path planning pipeline to achieve
a simultaneous mapping system. The data collected by LiDAR can be used to
support the mapping and localization of the UAV. Currently, the ROS2 Nav2
package incorporates LiDAR odometry data into autonomous navigation. Although
most of the results from this system are reported for ground robots, we plan to
adapt a similar approach in our system in the future. 

In summary, we have identified three key limitations in the path planning
framework and have analyzed potential solutions for each limitation. The current
limitations, such as verifying simulation results and conducting real-world UAV
testing, are crucial and will be addressed during the next stage of the project.
Combining LiDAR SLAM data for the mapping system can be achieved later,
depending on the given time limitation, by collaborating with the SLAM pipeline. 


\subsection{GNSS-R}
In the next half of the project, the GNSS-R framework will integrate with the 
machine learning classification results to effectively distinguish between reflected 
and directly propagated signals. Then, the sensitivity of GNSS carrier phase measurements
for FFZs covering the trees would be explored. Lastly, a model of identified correlations
between GNSS measurements and ground features would be summarized to achieve the
ultimate objective of this project.


\subsection{Machine Learning}
The next part of this project will focus on the developing a neural network based model 
to perform regression based task to predict vegetation parameters such as biomass, VWC, 
thee height etc. Literature shows that neural networks have been effective in performing
such regression tasks using features such as: Delay Doppler Maps (DDMs), signal power,
polarization parameters etc. \cite{hu2019detecting,pilikos2024biomass}. Simultaneously,
the dataset for LightGBM model will be further expansed by collecting more GNSS-R data in controlled 
environment to improve the classification performance.

\subsection{LiDAR SLAM}
The next part of this project aims to solve the point cloud alignment issues
caused by the incorrect coordinate frame rotation between GPS and LiDAR
odometry, as well as from the GPS transformation logic. In addition to that, the
modular SLAM framework will be fine-tuned to improve robustness in mitigating
the effects of canopy interference and sparse LiDAR features.


% --- Project Management ---
\section{Project Management}
The Final Year Project spans two semesters, with first semester focusing mainly 
on literature review, methodology development, finalizing connections between 
different parts of the project and conducting preliminary experiments for collecting data and testing 
developed methodologies. The second semester will focus on integrating developed systems for
each individual parts, developing final models, conducting final experiments to collect more data and 
validate methodologies and writing final project report. The Gantt Chart below summarizes the project timeline 
for each part of the project. 

\subsection{Gantt Chart}
\begin{center}
\begin{sideways}
\begin{ganttchart}[
  vgrid=true,
  hgrid=true,
  title/.append style={fill=OliveGreen!70},
  title label font=\small\color{white},
  group/.append style={fill=none},
  group label font=\bfseries\small,
  bar label font=\scriptsize,
  progress label text={},
  x unit=1.8cm,
  y unit title=0.6cm,
  y unit chart=0.4cm,
  time slot format=isodate,
  time slot unit=month,
  canvas/.append style={fill=none},
  bar incomplete/.append style={fill=ForestGreen!30},
  today=2026-01-01,
  today label=Interim,
  today offset=.5,
  today rule/.style={draw=blue, ultra thick, dash pattern=on 2pt off 2pt},
  bar height=0.6,
  bar top shift=0.15,
  group height=0.6,
  group top shift=0.15,
  group peaks tip position=0,
  group left shift=0,
  group right shift=0,
]{2025-09-01}{2026-05-30}
\gantttitle{2025}{4}
\gantttitle{2026}{5} \\
\gantttitlecalendar{month=shortname} \\
% Overall
\ganttgroup[group/.style={fill=overallcolor}]{Overall}{2025-09-01}{2026-05-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Planning \& Initialization}{2025-09-01}{2025-09-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Literature Review}{2025-09-01}{2025-10-31} \\
\ganttbar[bar/.style={fill=overallcolor}]{UAV Setup}{2025-09-01}{2025-10-31} \\
\ganttbar[bar/.style={fill=overallcolor}]{System Design}{2025-10-01}{2025-11-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Experiment 1}{2025-10-01}{2025-10-31} \\
\ganttbar[bar/.style={fill=overallcolor}]{Experiment 2}{2025-11-01}{2025-11-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Experiment 3}{2025-12-01}{2025-12-31} \\
\ganttbar[bar/.style={fill=overallcolor}]{Interim Report \& Presentation}{2025-12-01}{2026-01-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Experiment 4}{2026-02-01}{2026-02-28} \\
\ganttbar[bar/.style={fill=overallcolor}]{Experiment 5}{2026-03-01}{2026-03-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Final Demonstration}{2026-04-01}{2026-04-30} \\
\ganttbar[bar/.style={fill=overallcolor}]{Final Report \& Presentation}{2026-05-01}{2026-05-30} \\
% Path Planning
\ganttgroup[group/.style={fill=pathcolor}]{Path Planning}{2025-10-01}{2026-05-30} \\
\ganttbar[bar/.style={fill=pathcolor}]{Waypoint Nav}{2025-10-01}{2025-12-31} \\
\ganttbar[bar/.style={fill=pathcolor}]{Coverage Planning}{2025-11-01}{2026-01-31} \\
\ganttbar[bar/.style={fill=pathcolor}]{Dynamic Sim (MATLAB/AirSim)}{2026-02-01}{2026-03-31} \\
\ganttbar[bar/.style={fill=pathcolor}]{Real-world Nav}{2026-03-01}{2026-04-30} \\
\ganttbar[bar/.style={fill=pathcolor}]{Simul Mapping \& Nav (w/ SLAM)}{2026-04-01}{2026-05-30} \\
% GNSS-R
\ganttgroup[group/.style={fill=gnsscolor}]{GNSS-R}{2025-10-01}{2026-04-30} \\
\ganttbar[bar/.style={fill=gnsscolor}]{GNSS Measurement Extraction}{2025-10-01}{2025-12-31} \\
\ganttbar[bar/.style={fill=gnsscolor}]{FFZ Analysis}{2025-11-01}{2026-01-31} \\
\ganttbar[bar/.style={fill=gnsscolor}]{ML Integration}{2026-02-01}{2026-02-28} \\
\ganttbar[bar/.style={fill=gnsscolor}]{FFZ Sensitivity}{2026-03-01}{2026-03-31} \\
\ganttbar[bar/.style={fill=gnsscolor}]{Correlation Modeling}{2026-04-01}{2026-04-30} \\
% ML
\ganttgroup[group/.style={fill=mlcolor}]{Machine Learning}{2025-10-01}{2026-04-30} \\
\ganttbar[bar/.style={fill=mlcolor}]{Feature Analysis}{2025-10-01}{2025-11-31} \\
\ganttbar[bar/.style={fill=mlcolor}]{Dataset Construction}{2025-10-01}{2025-11-31} \\
\ganttbar[bar/.style={fill=mlcolor}]{Model Selection \& Training}{2025-11-01}{2025-12-31} \\
\ganttbar[bar/.style={fill=mlcolor}]{NN Dev for Regression}{2026-01-01}{2026-03-01} \\
\ganttbar[bar/.style={fill=mlcolor}]{Dataset Expansion}{2026-01-01}{2026-04-01} \\
% LiDAR SLAM
\ganttgroup[group/.style={fill=slamcolor}]{LiDAR SLAM}{2025-11-01}{2026-04-30} \\
\ganttbar[bar/.style={fill=slamcolor}]{LiDAR-Inertial Odom}{2025-11-01}{2025-12-31} \\
\ganttbar[bar/.style={fill=slamcolor}]{Graph-Based SLAM}{2025-12-01}{2026-01-31} \\
\ganttbar[bar/.style={fill=slamcolor}]{System Architecture \& GPS Integration}{2026-01-01}{2026-01-31} \\
\ganttbar[bar/.style={fill=slamcolor}]{Improvements}{2026-02-01}{2026-03-31} \\
\ganttbar[bar/.style={fill=slamcolor}]{Path Planning Integration}{2026-04-01}{2026-04-30} \\
\end{ganttchart}
\end{sideways}
\end{center}
\vspace*{\fill}

\subsection{Project Difficulties and Solutions}
Placeholder text

\subsubsection{Path Planning}
In terms of project management, the issues that arose during the path-planning
process were mainly related to hardware and communication. During on-site
experiments, the UAV's telemetry disconnected from the ground station, even
though it was within the specified communication range. Since the mission
planner in QGround Control does not rely on real-time communication, the
experiments were not disrupted. However, to solve this problem, we have analyzed
strong long-distance communication telemetry types and will be replacing the
current hardware in the next stage. 

\subsubsection{GNSS-R}
Through processing the data collected during the first half of this project, one of the 
most important issue that arose was the fact that some results did not indicate obvious
conclusions. For example, as demonstrated in Figure \ref{GNSS-R Confusing Result}, the
FFZs of $C/N_0$ ratios do not display clear and conclusive results, as opposed to the
observations shown in the Results and Discussion section of the report. This is partially
caused by the fact that not all the FFZs shown here actually exist due to non-reflected
signals received by the bottom antenna assumed to be reflected from the ground. To solve 
this issue, machine learning approaches will be employed to extract only the reflected
signals from the nadir antenna observations.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/GNSS_R_FFZ_Confusing_Result.png}
    \caption{Example of inconclusive results.}
    \label{GNSS-R Confusing Result}
\end{figure}
Another issue that occurred was the fact that some dataset was too large to be able to be
plotted onto the satellite image map. Therefore, for such datasets, key moments will be 
selected out for analysis individually, while the code for processing those datasets will
also be optimized with the help of GenAI tools.

\subsubsection{Machine Learning}
One of the main difficulties currently in this project is not being able to collect true 
labelled data for training the classification model. Although the data collection was 
performed in a controlled environment, there are still chances of having some direct signals
being recorded by the nadir antenna and some reflected signals being recorded by the zenith antenna.
This could be leading to some mislabelling of the training data and thus affecting the model performance 
as observed in the confusion matrix section. To solve this issue, more controlled data collection will be 
performed to ensure the quality of the training dataset. Advanced data cleaning techniques will also be explored to 
improve the labelling accuracy of the dataset. 

\subsubsection{LiDAR SLAM}
One of the problems faced during this project is the point clouds produced by
Pose Graph Optimization (PGO) SLAM. Based on Figure \ref{debug_pc}, we can see that there are
two types of point cloud clusters. One of which is the purple point cloud
clusters and the other is the yellow point cloud clusters. To solve this issue,
the source code was debugged and revealed that the yellow point cloud clusters
are a loop-closure debug visualization in which the algorithm compares the
current scan against local history to verify a loop closure. Because the point
clouds are distorted due to GPS integration, these debug point clouds are
temporarily commented out to allow focus on the overall alignment of point
clouds in PGO SLAM.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/debug_pc.png}
    \caption{PGO SLAM with two types of point clouds}
    \label{debug_pc}
\end{figure}


% --- Appendix ---
\newpage
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}

\subsubsection*{GitHub Repositories}
The code scripts of this project are organized in various GitHub repositories, which
aligns with different frameworks of this project. Specifically: 
\begin{itemize}
    \item GNSS-R: \url{https://github.com/zhlouise/UAV_GNSS-R}
    \item Machine Learning: \url{https://github.com/gd-Sahat/FYP-ML/tree/main}
    \item Path Planning: \url{https://github.com/sashenkagamage/canopix.ai} 
    \item LiDAR SLAM: \url{https://github.com/A4paper2003/scpgo-workspace/tree/main}
\end{itemize}


% --- References ---
\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{IEEEtran}
\bibliography{References.bib} 


\end{document}
