\documentclass[12pt, a4paper]{article}


% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times} % Sets font to Times New Roman
\usepackage[margin=2.5cm]{geometry} % Sets 2.5cm margins
\usepackage{setspace} % Required for double spacing
\usepackage{titlesec}
\usepackage{sectsty}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{color}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}



% --- Settings ---
\doublespacing % Applies double spacing to the document
\sectionfont{\normalsize}
\subsectionfont{\normalsize\it\bfseries}
\subsubsectionfont{\normalsize\it}


\begin{document}


% --- Title Page ---
\begin{titlepage}
    \centering
    \includegraphics[width=0.5\textwidth]{Included_Images/PolyU_Logo.png}
    \hfill
    \includegraphics[width=0.25\textwidth]{Included_Images/FENG_Logo.png}
    \vfill
    {\Large 2025/26 Interdepartmental Final Year Project Interim Report\par}
    {\huge \textbf{Intelligent UAV Systems for GNSS-Based Remote Sensing on
    Vegetation} \par}
    \vfill
    {\large [AAE] ZHOU Jiayi (22099961D) \par} {\large [AAE] MAHMUD Md Sahat
    (22097159D) \par} {\large [EEE] GAMAGE Sashenka (22097129D) \par} {\large
    [ME] TAN Qing Lin (22101126D) \par}
    \vfill
    {\large \textbf{Chief Supervisor: } [AAE] Prof. Guohao ZHANG \par} {\large
    \textbf{Co-Supervisor: } [AAE] Prof. Li-Ta HSU \par}
    \vspace{1cm}
    {\large \textbf{Date of Submission: } 2026 January 18 \par}
\end{titlepage}


% --- Table of Contents ---
\pagenumbering{roman}
\tableofcontents
\newpage


% --- Main Content ---
\pagenumbering{arabic}


% --- Abstract ---
\section{Abstract}
Placeholder text


% --- Introduction ---
\section{Introduction}
Vegetation is a critical asset to the environment and the human civilization.
Not only does vegetation produce essential societal resources, but its
distribution and productivity also greatly impacts the terrestrial ecosystems
and the global climate \cite{Richardson2013}. Therefore, the continuous and
accurate monitoring of vegetation is essential for sustainable resource
management \cite{Wallace2006}, ecosystem preservation \cite{Zeng2022}, and
climate change modeling \cite{Richardson2013}. 

Traditionally, methods of vegetation monitoring involved manual field
assessments of site characteristics, extracting vegetation condition indicators
such as species composition, geometrical structure, and biochemical activities
\cite{Gibbons2006}. However, the accuracy and efficiency of such monitoring
methods are highly dependent on the available expertise and resources, as the
site assessments often required the assessor to possess reasonable levels of
field knowledge prior to surveying \cite{Parkes2003}. To address the limitations
of manual assessments and to accommodate for the increasing large-scale
monitoring demands, remote sensing systems emerged as an essential tool for
ecological monitoring \cite{Li2023}. 

Remote sensing refers to the acquisition of an object's information through
measurements obtained without coming into direct contact with said object,
effectively minimizing the need for manual involvement on-site
\cite{campbell2011introduction}. Specifically, remote sensing relies on
information derived from different measurements of energy reflected from the
object of interest \cite{campbell2011introduction}. In the context of surveying
terrestrial vegetation, the remote sensing methods could be classified into two
categories based on the distance between the target object and the measurement
sensor: 1) space-borne and 2) airborne remote sensing. Space-borne remote
sensing involves the use of instruments onboard orbiting satellites, including
spectral and hyperspectral cameras \cite{Qian2022}, synthetic aperture radar
(SAR) \cite{Hu2025}, and space-borne Light Detection And Ranging (LiDAR) sensors
\cite{Bergen2009}. Due to the wide coverage and availability of space-borne
data, large-scale terrestrial changes over time could be captured, enabling the
continuous monitoring of macro-scale ecosystems \cite{Khan2024}. Compared to
space-borne systems, airborne remote sensing can provide significantly improved
spatial resolution and assessment flexibility through the integration of sensors
onboard manned aircraft or unmanned aerial vehicles (UAVs) \cite{Khan2024}.
Although limited by coverage area, airborne remote sensing can provide timely
information for addressing regional emergencies such as pest \cite{Wulder2006}
or wildfire \cite{Arroyo2008} outspreads since the systems could be deployed
on-demand. In UAV-based remote sensing particularly, this temporal flexibility
is further complemented with the benefit of low operational cost, rendering it
an ideal platform for monitoring regional and urban vegetation \cite{Tang2015}. 

Conventionally, UAV remote sensing platforms carry similar instruments to that
of other remote sensing systems, including radar, LiDAR, and multi-spectral or
hyperspectral imagery sensors \cite{Tang2015}. However, while imaging
instruments are susceptible to the influence of lighting and weather
\cite{Wu2024}, LiDAR devices tend to face difficulties penetrating through dense
canopy \cite{Su2007}. Therefore, it is critical to explore a robust sensing
technique that is suitable for UAVs in terms of payload and power consumption to
complement the existing instruments. Recently, the technique of Global
Navigation Satellite System Reflectometry (GNSS-R) is receiving increasing
interest in the field of remote sensing. GNSS-R exploits the L-band signals
transmitted from Global Navigation Satellite Systems (GNSS) that are then
scattered on different terrain surfaces of the Earth \cite{Jin2024}. Then, the
reception of reflected GNSS signals can provide information regarding the
properties of the signal reflector on land \cite{Jin2024}. As signals of
opportunity conventionally dedicated to Positioning, Navigation, and Timing
(PNT) applications, GNSS is capable of providing real-time measurements
regardless of time, location, and weather \cite{Jin2024}. Additionally, as a
bi-static system where signal transmitters are separated from receivers, GNSS-R
is exempt from the need of dedicated transmitter-receiver instruments that are
crucial to mono-static radar systems \cite{Jin2010}. Instead, any consumer-grade
receivers capable of receiving reflected GNSS signals could be used for GNSS-R
remote sensing, further demonstrating GNSS-R's applicability onboard low-cost
remote sensing systems such as an UAV. 

\hl{May need some intro for path planning and lidar here. The above is from
Louise's project proposal, so need further editing. (Also need to mention 
the full name of SLAM somewhere)}

In this project, the technique of GNSS-R will be integrated onto an UAV platform
to achieve an intelligent and structured remote sensing system for vegetation
monitoring. The main objectives of this project are as follows:
\begin{enumerate}
\item To introduce an autonomous path planning framework onboard the UAV platform
for optimized GNSS-R remote sensing.
\item To analyze and model the correlations between signal propagation parameters
retrieved through GNSS-R and ground vegetation conditions.
\item To classify signals reflected by vegetation and predict vegetation parameters
based on raw GNSS data using machine learning-based approach.
\item To establish a detailed 3D canopy map using LiDAR-based SLAM, providing a
spatial validation reference for the 2D vegetation features detected by GNSS-R.
\end{enumerate}

This report is structured as follows: Section 3 reviews contemporary research in
UAV path planning, GNSS-R, machine learning, and LiDAR SLAM, while Section 4
outlines the project's methodologies. The subsequent sections cover the
experiments conducted (Section 5), a discussion of current results (Section 6),
and the project conclusions (Section 7). Finally, Section 8 explores future
work, and Section 9 summarizes project management details. 


% --- Literature Review ---
\section{Literature Review}
To achieve the project objectives, this literature review comprehensively examines
the contemporary research in four domains, each associated with one project objective.
\hl{First, existing path planning methodologies are reviewed to \dots}
Subsequently, the existing techniques of GNSS-R are explored, with a focus on its
prior applications in vegetation parameter retrieval. 
\hl{Then, machine learning \dots}
\hl{Finally, LiDAR-based SLAM techniques \dots}
Consequently, this review identifies the existing gaps in each domain for the 
objective of vegetation monitoring, which will inform the integrated project 
methodologies.

\subsection{Path Planning}
Placeholder text

\subsection{GNSS-R}
The concept of using GNSS-R for remote sensing was first proposed in 1993, where
the correlation between direct and scattered GNSS signals were proven promising
for the application of ocean altimetry \cite{MartnNeira1993APR}. Today, apart
from the originally proposed application, GNSS-R techniques has been extensively
used for Earth observations and land information retrieval, including ocean
salinity, soil moisture, and ice thickness \cite{RodriguezAlvarez2011}. However,
research on vegetation parameter retrieval remains limited to preliminary
analyses \cite{Jia2018}. 

In a previous study, the capabilities of monitoring vegetation through GNSS-R
were demonstrated through a theoretical simulation of GNSS scattering
characteristics \cite{Ferrazzoli2011}. Furthermore, this study revealed that the
sensitivity of GNSS-R signals can be influenced by incidence angle, soil
parameters, and tree size, while signals with lower elevation angles and RL
polarization (transmission of right-hand circular polarization (RHCP) signal,
reception of left-hand circular polarization (LHCP) signal) appeared to be ideal
for forest monitoring \cite{Ferrazzoli2011}. In another study, the polarization
scattering properties of GNSS-R signals in forest canopies were modeled, where
simulations revealed that tree trunk scattering effects would dominate total
scattering response, while satellite azimuth angles are significantly correlated
to the signal polarization \cite{Wu2014}. 

Aside from simulated approaches, empirical or semi-empirical studies regarding GNSS-R
remote sensing of vegetation were also reviewed. In a study by Yueh et al.,
GNSS-R data onboard the Cyclone GNSS (CYGNSS) satellite were analyzed against
the vegetation water content estimated through the satellite-derived Normalized
Difference Vegetation Index (NDVI) \cite{Yueh2022}. The results demonstrated
near-linear relationship between vegetation water content and GNSS-R signal
attenuation, while the CYGNSS data also suggested the existence of volume
scattering within complex forest components \cite{Yueh2022}. Similarly, another
study analyzing GNSS-R data from TechDemoSat-1 demonstrated reduced sensitivity
to soil moisture retrieval due to vegetation attenuation, which could be
effectively compensated using NDVI data, indicating a correlation between signal
attenuation and vegetation water content \cite{Camps2016}. Additionally, the
interference pattern between direct and reflected GNSS signals from the Earth's
surface was used in estimating vegetation height and land topography
\cite{RodriguezAlvarez2011}. Through using a Soil Moisture Interference-pattern
GNSS Observations at L-band (SMIGOL) reflectometer, the instantaneous power of
the direct and reflected signals were coherently added, in which the resulting
power oscillations present notches that are correlated to vegetation layer
thickness and the reflection geometry \cite{RodriguezAlvarez2011}. As a result,
an RMSE of 3-5 cm in estimating the height of vegetation with simple geometrical
structures (barley and wheat) could be achieved \cite{RodriguezAlvarez2011}.
Yet, despite the effort in prior studies, the use of GNSS-R techniques in
vegetation remote sensing and monitoring is far from developed. Most
importantly, existing research relies heavily on space-borne or static
ground-based platforms, while the use of high spatial resolution and flexibility
platforms - such as an UAV - remains underexplored. Therefore, it is critical to
develop a systematic framework for an UAV-based GNSS-R system, in which
correlations between GNSS-R signal features and vegetation condition parameters
are comprehensively assessed and modeled. Additionally, with adequate dataset size,
the GNSS-R framework could be complemented by machine learning-based modelling
approaches, further enhancing the system robustness. \hl{Connection to ML part}

\subsection{Machine Learning}
Placeholder text

\subsection{LiDAR SLAM}
In recent years, autonomous robots have been widely implemented to carry out
repetitive tasks or take part in site surveys to ensure human safety or even
reduce operational cost. To carry out remote sensing via UAV, mapping is
essential to allow robots to create a spatial understanding of the site in order
to create efficient flight paths as well as avoid obstacles. 




% --- Methodology ---
\section{Methodology}
Placeholder text

\subsection{UAV Platform}
Placeholder text

\subsection{System Architecture}
Placeholder text

\subsection{Path Planning}
Placeholder text

\subsection{GNSS-R}
The GNSS-R framework takes in raw GNSS data collected from the onboard receivers
and runs the data through the steps of GNSS measurement extraction, Fresnel zone
analysis, and GNSS measurement level modelling. 

\subsubsection{GNSS Measurement Extraction}
Two major GNSS measurements are extracted from the collected data: carrier to
noise ratio ($C/N_0$) and pseudorange error. 

\paragraph{Carrier to Noise Ratio} ~\\
Carrier to noise ratio refers to the power of received carrier signal relative
to the noise power per unit bandwidth, usually expressed in decibel-Hertz (dB-Hz)
\cite{joseph2010gnss}. $C/N_0$ can be readily obtained from the receiver, and it 
is calculated using the following equation:
\begin{equation}
    C/N_0 = C - (N - BW)
\end{equation}
where $C$ refers to the carrier power in dBm or dBW; $N$ refers to the noise power
in dBm or dBW; and $BW$ refers to the bandwidth of the observation in Hz.

\paragraph{Pseudorange Error} ~\\
In GNSS, pseudorange refers to the "apparent" distance between a satellite and a
receiver, which includes the true geometric range plus various biases and delays.
For the $i$th satellite, its pseudorange $\rho^i$ could be expressed as the following 
\cite{kaplan2017understanding}:
\begin{equation}
    \rho^i = R^i + \Delta t_r + \Delta t_s^i + I^i + T^i + \epsilon^i
\end{equation}
where $R^i$ refers to the true geometric distance between the satellite and the receiver;
$\Delta t_r$ refers to the receiver clock bias; $\Delta t_s^i$ refers to the satellite
clock bias; $I^i$ refers to the ionospheric delay; $T^i$ refers to the tropospheric 
delay; and $\epsilon^i$ refers to the pseudorange error, which is mainly
introduced through signal reflections. Among all the signals received at a given
instant, a satellite $m$ with the highest elevation angle is selected as the
master satellite, where the assumption that the master satellite is free of reflections
is applied \cite{hsu2018analysis}:
\begin{equation}
    \rho^m = R^m + \Delta t_r + \Delta t_s^m + I^m + T^m.
\end{equation}
Assuming $\Delta t_s^i$ and $\Delta t_s^m$ can be effectively removed by 
satellite-broadcasted parameters, $I^i$ and $I^m$ can be effectively removed by the
Klobuchar model, and $T^i$ and $T^m$ can be effectively removed by the Saastamoinen model,
the pseudorange equations can be approximated as \cite{kaplan2017understanding}:
\begin{equation}
    \rho^i = R^i + \Delta t_r + \epsilon^i,
\label{ith satellite pseudorange}
\end{equation}
\begin{equation}
    \rho^m = R^m + \Delta t_r.
\label{master satellite pseudorange}
\end{equation}
Finally, taking the difference between \eqref{ith satellite pseudorange} and 
\eqref{master satellite pseudorange} and rearranging the equation will result in the
pseudorange error:
\begin{equation}
    \epsilon^i = \rho^i - \rho^m - R^i + R^m.
\end{equation}

\subsubsection{Fresnel Zone Analysis}
Placeholder text

\subsubsection{GNSS Measurement Level Model}
Placeholder text

\subsection{Machine Learning}
Placeholder text

\subsection{LiDAR SLAM}
\subsubsection{LiDAR-Inertial Odometry}
For the front end of the modular SLAM system, LiDAR-Inertial Odometry (Fast-LIO)
is used to build a map effectively within a short amount of time. To do this,
Iterative Extended Kalman Filter (IEKF) is used to fuse the data between IMU and LiDAR
point clouds to create an odometry of the environment. 

Iterative Extended Kalman Filter (IEKF) follows the core of Bayesian Recursion
to estimate the state of the robot through the prediction and measurement update
step. In the prediction step, IEKF assumes that the state of the system at time
$k$ evolved from the prior state at time $k-1$ is shown as follows:
\begin{equation}
    x_k = f(x_{k-1},u_k) + w_{k} 
\end{equation}

Where $x_k$ is the state vector containing the terms of interest for the system
at time $k$. The $f(.)$ represents a non-linear state function that is used to
forecast current state data from prior state data $x_{k-1}$ and $u_k$ is a control vector. We can approximate $w_{k-1}$ as
$N(0,Q_k)$ where it has a zero-mean Gaussian distribution with covariance matrix
$Q_k$.

To know the uncertainty of the prediction model, error covariance matrix $P^f_k$
is defined as shown:
\begin{align}
    P_k^f &= E\left(e_k^f(e_k^f)^T\right) \nonumber \\
    &\approx F_k E(e_{k-1}e_{k-1}^T)F_k^T + E(w_{k-1}w_{k-1}^T) \nonumber \\
    &= F_kP_{k-1}F_k^T + Q_{k-1}
\end{align}

Where $e$ is the prediction error, $F$ is the Jacobian matrix of the linearized
state transition function $f(.)$ evaluated at the previous estimate
$x_{k-1}$, and $Q$ is the process noise covariance matrix representing the
uncertainty of the physical model.

Even so, the predicted state alone is prone to errors due to various
factors such as inaccurate prediction function and further source of noise. This
issue can be solved by taking sensor measurements to improve the predicted state
accuracy. Hence the observation model is defined as shown:
\begin{equation}
    z_k = h(x_k) + v_k
\end{equation} 

Where $z_k$ represents the expected sensor measurements based on the predicted
state $x_k$ expressed by a non-linear function $h(.)$, and the observation noise
$v_k$ is assumed as $N(0,R_k)$ where it has zero mean with covariance matrix $R_k$.

To further improve the accuracy for the nonlinear system, an iteration index
$K$ is added to repeat the measurement update at time step $k$ as shown:
\begin{equation}
    x^a_{k,K=0} = x^f_k
\end{equation}

Where $x^a$ is the state estimate after corrected by sensor measurement and
$x^f$ is the predictive distribution before looking at the lidar data. In this
case, the iteration $K$ is repeated until the change in the state estimate
$|x^a_{k,K+1} - x^a_{k,K}|$ falls below threshold $\epsilon$. 

As actual measurements from sensors will typically differ from this prediction
due to sensor noise and model inaccuracies, it is incorporated into the state
estimate by computing the difference between the actual measurements and the
predicted measurement based on the current iterative estimate $K$:
\begin{equation}
    z_{k, K} = z_k - h(x^a_{k, K})
\end{equation}
Where $h(x^a_{k, K})$ is the non-linear observation function evaluated at the
most recent estimate. To balance the uncertainty of the prediction against the
new sensor data, the Kalman Gain $K_{k,K}$ is calculated using the state-space
dimension formula \cite{xu2021fast}.
\begin{equation} 
    K_{k, K} = (H_{k, K}^T R_k^{-1} H_{k, K} + (P_k^f)^{-1})^{-1} H_{k, K}^T R_k^{-1} 
\end{equation}

Where $H_{k, K}$ is the Jacobian of the observation function re-evaluated
at the current iterative estimate $x^a_{k, K}$. This gain is then used
to compute the next refined state estimate:
\begin{equation}
    x^a_{k,K+1} = x^a_{k, K} + K_{k, K} (z_{k, K}) - (I - K_{k, K} H_{k, K})(x^a_{k, K} - x_k^f)
\end{equation}

After updating the state estimate, the updated covariance of the state
distribution is given as:
\begin{equation} 
    P_k = (I - K_{k, K} H_{k, K}) P_k^f 
\end{equation}

While the IEKF provides a high-frequency estimate of the current state, the
overall SLAM problem can be generalized as a Maximum A Posteriori (MAP)
estimation. Bayes Filter is then used to recursively perform the state
prediction and measurement update steps to minimizes linearization errors for
robust localization. In this case, MAP can be expressed as:
\begin{equation}
    P(X|Z, U) \propto P(x_0) \prod_{k} P(z_k|x_k) \prod_{k} P(x_k|x_{k-1}, u_{k-1})
\end{equation}

Where $P(X|Z, U)$ is the refined state estimate, $P(x_0)$ is the prior,
$P(z_k|x_k)$ is the sensor measurement and $P(x_k|x_{k-1}, u_{k-1})$ is the
predicted state.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/Kalman Filter.png}
    \caption{The red distribution in the figure represents the predicted step $P(x_k|x_{k-1}, u_{k-1})$  which results in predictive distribution  $x^f_k$ .The blue distribution represents likelihood $P(z_k|x_k)$ derived from the noisy sensor measurement $z_k$. The green distribution illustrates the refined state estimate $x^a_k$, showing the reduction in the uncertainty covariance $P_k$ after fusing both data sources \cite{faragher2012understanding}.}
\end{figure}

\subsubsection{Graph-Based SLAM}
As LiDAR odometry is used in the front end to provide high-rate local pose
estimates without global constrains, iSAM2 pose graph optimization architecture
is used for the back end to further optimize the point cloud map for higher
accuracy. This technique is rooted from the factor graph optimization algorithm
where Maximum A Posteriori (MAP) is also used to factorize the joint posterior
probability of the robot trajectory X as shown  \cite{wen2021factor}:

\begin{equation} 
    P(X|Z, U) \propto P(x_0) \prod_{k} P(z_k|x_k) \prod_{k} P(x_k|x_{k-1}, u_{k-1}) 
\end{equation}

Where $P(x_0)$ is the prior, $P(z_k|x_k)$ represents the Update Factors from
LiDAR measurements, and $P(x_k|x_{k-1}, u_{k-1})$ represents the Propagation
Factors derived from the IEKF prediction. The purpose of this is to decompose
the complex problem of estimating a full trajectory into smaller, simpler pieces
called 'factors'. By breaking down the trajectory into these discrete
components, this allows iSAM2 to optimize specific parts of the trajectory
affected by new information or loop closures instead of resolving the entire
global map every time the robot moves. This ensures that global consistency is
maintained without the computational burden of a full-batch optimization.

To do this, the Gaussian noise is factorized into exponential functions. Unlike
a standard filter that only estimates the current state $x_k$, the factorized
graph maintains the entire trajectory history $X$ = ($x_0, x_1, \dots, x_k$) to
minimize the sum of squared errors. The Gaussian noise from the IEKF motion
prediction is expressed as:
\begin{equation}
    P(x_k|x_{k-1},u_{k-1}) = \exp(-||f_k(x_{k-1}, u_k)-x_k||^2_{\Sigma_{u,k}})
\end{equation}

Similarly, the Gaussian noise from lidar measurements is factorized into the following equation:
\begin{equation}
    P(z_k | x_k) = \exp(-||h_k(x_k) - z_k||^2_{\Sigma_{z,k}})
\end{equation}

The factorization is crucial to transform MAP estimation from a product
of probability distributions into a Non-linear Least Squares (NLS) optimization
problem. By taking the negative logarithm of the posterior, we convert the task
of finding the maximum probability into finding the minimum of an error function
$e(\mathcal{X})$ as shown:
\begin{equation}
    \hat{\chi} = \arg\max_{\chi} \prod_{k} F_k(x_k) = \arg\min_{\chi} e(\chi)
\end{equation}
\begin{equation}
    e(\chi) \doteq \sum_{k} ||h_k(x_k) - z_k||^2_{\Sigma_k} + \sum_{k} ||f_k(x_{k-1}, u_k) - x_k||^2_{\Sigma_k}
\end{equation}

To solve the non-linear least squares problem defined by $e(\mathcal{X})$, the
system iteratively refines the trajectory estimate to find the optimal solution
$\hat{\mathcal{X}}$. The error function is linearized using a first-order Taylor
expansion around the current estimate $\mathcal{X}^{(i)}$ where the state is
then uploaded incrementally:
\begin{equation}
    \mathcal{X}^{(i+1)} = \mathcal{X}^{(i)} + \Delta \mathcal{X}
\end{equation}

\hl{Where $\Delta \mathcal{X}$ is the optimal step calculated to move the current estimate toward the minimum of the "error
valley".This optimization process relies on the Jacobian matrix $\mathbf{J}_R$,
which represents the first-order partial derivatives of the residuals with
respect to the state variables. In the context of LiDAR-Inertial integration,
the Jacobian is composed of two primary components: the partial derivatives of
the LiDAR measurement residuals and the propagation model
residuals: (EQUATIONS)

By setting the gradient of the error function to zero, the system solves for
$\Delta \mathcal{X}$ through the normal equations, typically using the
Gauss-Newton method. Unlike the IEKF, which only iterates on the current state
at time $k$, this graph-based optimization allows the system to re-linearize the
entire trajectory history—or a sliding window of it—ensuring that local sensor
noise does not lead to permanent global map distortion }

\begin{equation}
    \mathbf{J}_R = \frac{\partial(\mathbf{H}(\mathcal{X}) - \mathbf{Z})}{\partial \mathcal{X}} + \frac{\partial(\mathbf{F}(\mathcal{X}, \mathbf{U}) - \mathcal{X})}{\partial \mathcal{X}}
\end{equation}

\subsubsection{System Architecture}
In this project, a modular SLAM system is used with the front end having a
Fast-LIO lidar odometry to carry out mapping in real time, while Pose Graph
Optimization (PGO) coupled with GPS is used for the back end. During
implementation, both Fast-Lio and PGO are initialized. The point cloud data will
first be processed by Fast-LIO to create an odometry, and the odometry will then
be passed to the PGO to further optimize the point clouds to increase accuracy.
In the meantime, GPS data is passed into PGO to provide constraints to the SLAM
if large drifts occur. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Included_Images/SLAM flowchart.png}
    \caption{Flowchart of the modular SLAM system}
\end{figure}

This project builds upon an open-source implementation
that integrates Fast-LIO with PGO for GPS-aided SLAM:
\url{https://github.com/yxw027/FAST_LIO_GPS} 

The pseudocode of the front-end and back-end of the modular SLAM can be shown below:

{
    \singlespacing
    \begin{algorithm}[H]
    \caption{Front-End: FAST-LIO with MAVLink TimeSync}
    \begin{algorithmic}[1]
    \State \textbf{Initialize:} IEKF state $\mathbf{x}$ and covariance $\mathbf{P}$
    \State \textbf{Initialize:} ikd-Tree for incremental mapping
    \While{ROS is active}
        \State Sync LiDAR and IMU buffers based on GPS-aligned timestamps
        \If{IMU not initialized}
            \State Perform \textit{IMU\_init} (Gravity/Bias estimation)
            \State \textbf{continue}
        \EndIf
        \State \textbf{State Prediction:} $\hat{\mathbf{x}} \leftarrow$ IMU forward propagation
        \State \textbf{Undistortion:} Compensate point motion using $\hat{\mathbf{x}}$
        \For{$iter = 1$ \textbf{to} $NUM\_MAX\_ITERATIONS$}
            \State Find point-to-plane correspondences in ikd-Tree
            \State Compute measurement residual $\mathbf{z}$ and Jacobian $\mathbf{H}$
            \State \textbf{Update:} Correct state $\mathbf{x}$ using IEKF update rule
            \If{converged} \State \textbf{break} \EndIf
        \EndFor
        \State \textbf{ikd-Tree Update:} Add new features to the spatial map
        \State Publish Odometry to Back-End
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
}

{
    \singlespacing
    \begin{algorithm}[H]
    \caption{Back-End Pose Graph Optimization (PGO)}
    \begin{algorithmic}[1]
    \State \textbf{Initialize:} GTSAM Factor Graph and ISAM2 optimizer
    \State \textbf{Initialize:} Noise models (Odometry, GPS, Loop Closure)
    \While{ROS is active}
        \If{New Front-End Odometry received}
            \State Calculate delta movement since last Keyframe
            \If{movement $>$ $KeyframeThreshold$}
                \State Create new Node $n_t$ in Pose Graph
                \State \textbf{Propagation Factor:} Add \textit{BetweenFactor}($x_{k-1}, x_k, \Delta Odom$)
                \If{GPS matched for $n_t$}
                    \State Transform GPS Lat/Lon to UTM coordinates
                    \State \textbf{GPS Factor:} Add \textit{GPSFactor}($x_k, UTM_{xyz}$)
                \EndIf
                \State \textbf{Loop Closure:} Search for matches using Scan Context
                \If{Loop detected with $n_{\text{old}}$}
                    \State Calculate relative transform via ICP
                    \State \textbf{Loop Factor:} Add \textit{BetweenFactor}($x_{\text{old}}, x_t, T_{\text{ICP}}$)
                \EndIf
                \State Perform ISAM2 global optimization
                \State Update trajectory with optimized poses
            \EndIf
        \EndIf
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Included_Images/rqt_graph.png}
    \caption{Interactions of different ROS topics in the SLAM system}
\end{figure}




% --- Experiments ---
\section{Experiments}
Placeholder text

\subsection{Experiment Workflow}
Placeholder text

\subsection{Summary of Experiments Conducted}
Placeholder text

\subsection{Key Experiment 1}
Placeholder text

\subsection{Key Experiment 2}
Placeholder text


% --- Results and Discussion ---
\section{Results and Discussion}
Placeholder text

\subsection{Path Planning}
Placeholder text

\subsection{GNSS-R}
Placeholder text

\subsection{Machine Learning}
Placeholder text

\subsection{LiDAR SLAM}
\subsubsection{Stage 1: Modular SLAM Implementation without GPS}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg1_fastlio.png}
    \caption{Fast-LIO LiDAR Odometry}
    \label{fig:stg1_fastlio}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg1_pgo.png}
    \caption{PGO with FPFH and ICP}
    \label{fig:stg1_pgo}
\end{figure}
The implementation of modular SLAM is first done without the integration of GPS
to make sure the architecture works. When comparing \autoref{fig:stg1_fastlio}
and \autoref{fig:stg1_pgo}, it is observed that there is less drifting in the
point clouds in the PGO SLAM. This is because the PGO SLAM uses Fast Point
Feature Histograms (FPFH) method to have an initial guess to align the point
clouds and uses Iterative Closest Point (ICP) for further refinement. FPFH can
effectively represent the local geometry around key points in a point cloud which
makes it robust against changes in viewpoints. Since FPFH is better at aligning
point clouds that are far away for global robustness, it is then passed to ICP
for further precise refinement with millimeter-level precision
\cite{song2025robotic}. 

On the other hand, Fast-LIO only uses Iterated Extended Kalman Filter (IEKF) to
align the point clouds. In this case, we can see that the dual layer of point
cloud alignment in PGO further increase the accuracy in SLAM.

\subsubsection{Stage 2: Modular SLAM Implementation with GPS}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg2_pgomis.png}
    \caption{PGO SLAM after integration of GPS. Green trajectory indicates LiDAR odometry, 3 axis trajectory indicates GPS trajectory}
    \label{fig:stg2_pgo}
\end{figure}
Once gps is being integrated to the PGO SLAM, we can see in \autoref{fig:stg2_pgo} that there
are two different trajectories and the point clouds are very misaligned. First
of all, both trajectories are not aligned as the position of the LiDAR and the
GPS on the drone itself are different, hence having two different frames. Second of all, the point clouds are misaligned as GPS
has a high gain in the SLAM and some issues related to the transformation of the
GPS to LiDAR odometry. 

To solve the misalignments of both trajectories, extrinsic calibration is
carried out between IMU from Pixhawk that is connected to the GPS and the LiDAR
point clouds to align both frames together. In this case, HKU Mars calibration
system is used for the extrinsic calibration:
\url{https://github.com/hku-mars/LiDAR_IMU_Init/issues} 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg3_fastlio.png}
    \caption{Fast-LIO LiDAR Odometry}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/stg3_pgo.png}
    \caption{PGO SLAM with GPS and odometry trajectory aligned. Green trajectory indicates LiDAR odometry, 3 axis trajectory indicates GPS trajectory}
    \label{fig:stg3_pgo}
\end{figure}

Based on \autoref{fig:stg3_pgo}, despite both odometry and GPS trajectory and aligned, we can
still observe that the point clouds in PGO are still misaligned. Aside from having high
gain for GPS in the SLAM, the misalignment is due to the issues regarding the coordinate
frame rotation between GPS and LiDAR Odometry, and GPS transformation logic which
needs to be solved in later stage. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/withgps.png}
    \caption{PGO with GPS integration}
    \label{fig:withgps}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Included_Images/nogps.png}
    \caption{PGO without GPS integration}
    \label{fig:nogps}
\end{figure}

\autoref{fig:withgps} and \autoref{fig:nogps} shows the difference between
the PGO SLAM with and without GPS integration when using the same architecture. In
this case we can confirm that GPS is the main issue that causes the point cloud
distortion.

% --- Conclusion ---
\section{Conclusion}
Placeholder text


% --- Future Works ---
\section{Future Works}
Placeholder text

\subsection{Path Planning}
Placeholder text

\subsection{GNSS-R}
Placeholder text

\subsection{Machine Learning}
Placeholder text

\subsection{LiDAR SLAM}
Placeholder text


% --- Project Management ---
\section{Project Management}
Placeholder text

\subsection{Gantt Chart}
Placeholder text

\subsection{Project Difficulties and Solutions}
Placeholder text

\subsubsection{Path Planning}
Placeholder text

\subsubsection{GNSS-R}
Placeholder text

\subsubsection{Machine Learning}
Placeholder text

\subsubsection{LiDAR SLAM}
Placeholder text


% --- Appendix ---
\newpage
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}
Placeholder text


% --- References ---
\newpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{IEEEtran}
\bibliography{References.bib} 


\end{document}